_target_: src.models.evoformer_lite_simple.ProteinLitModule

# Model architecture parameters
d_model: 768           # Base model dimension (ESM-C embeddings size)
d_msa: 768              # MSA embedding dimension (ESM-MSA output)
n_blocks: 6             # Number of Evoformer-Lite blocks
n_heads: 8              # Attention heads for all attention layers
dropout: 0.1            # General dropout rate for model components
in_dropout: 0.1         # Dropout applied to inputs before Evoformer-Lite

# Task and training parameters
task_type: ${data.task_type}   # Task type: "mf", "bp", or "cc" (inferred from data config)
warmup_ratio: 0.05             # Warm-up ratio for cosine LR schedule 

# Optimizer configuration
optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 1e-4          # Learning rate â€“ tune per task if necessary
  weight_decay: 1e-2  # L2 regularisation
  betas: [0.9, 0.999]
  eps: 1e-8

# Learning-rate scheduler configuration
scheduler:
  _target_: src.utils.lr_schedulers.get_cosine_schedule_with_warmup
  _partial_: true      # Hydra will inject (optimizer, num_training_steps, num_warmup_steps)
  num_cycles: 0.5      # Single half-cosine cycle
  last_epoch: -1       # Start fresh (-1) 