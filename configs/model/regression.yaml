_target_: src.models.regression_module.ProteinLitModule

# Task type: "stability" or "fluorescence" - automatically from data config
task_type: ${data.task_type}

# Model architecture parameters
d_esm: 1152          # ESM-C embedding dimension
d_latent: 896        # Latent dimension for cross-modal fusion
d_prot: 1024         # ProtT5 XL embedding dimension
d_ankh: 2560         # Ankh3-XLarge embedding dimension
d_pglm: 2560         # XTrimoPGLM embedding dimension

# Architecture hyperparameters
n_cross_layers: 3    # Cross-modal attention layers between streams
n_heads: 8           # Attention heads

# Independent dropout hyperparameters
attention_dropout: 0.10    # Attention dropout
ffn_dropout: 0.18          # FFN dropout (dominates parameters)
fusion_mlp_dropout: 0.10   # Gate/fusion MLP dropout
head_dropout: 0.25         # Final head dropout

# Training parameters
warmup_ratio: 0.05         # Warmup ratio for cosine schedule (5% of total training steps)
learning_rate: 1e-4        # Learning rate for all parameters

# Optimizer configuration
optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: ${model.learning_rate}
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1e-8

# Learning rate scheduler configuration
scheduler:
  _target_: src.utils.lr_schedulers.get_cosine_schedule_with_warmup
  _partial_: true
  num_cycles: 0.5      # Single half-cosine
  last_epoch: -1       # Start from beginning

