_target_: src.models.protein_module.ProteinLitModule

# Model architecture parameters
d_model: 1152          # Base model dimension (ESM-C actual embedding size)
n_cross_layers: 3      # Cross-modal attention layers between sequence and auxiliary streams
n_heads: 8             # Attention heads for all transformer layers
dropout: 0.15          # General dropout rate for model components
task_type: ${data.task_type}         # Task type: "mf", "bp", or "cc" - automatically from data config
d_prot: 1024                    # ProtT5-BFD embedding dimension
d_ankh: 1024                    # Ankh3-Large embedding dimension (matches ankh_emb.pt)
d_pglm: 1536                    # XTrimoPGLM embedding dimension (matches pglm_emb.pt)

# Training parameters  
warmup_ratio: 0.01     # Warmup ratio for cosine schedule (1% of total training steps)

# Optimizer configuration
optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 1e-4             # IMPORTANT: Adjust learning rate based on task type
  weight_decay: 1e-2   # L2 regularization 
  betas: [0.9, 0.999]  # AdamW momentum parameters
  eps: 1e-8            # Numerical stability epsilon

# Learning rate scheduler configuration  
scheduler:
  _target_: src.utils.lr_schedulers.get_cosine_schedule_with_warmup
  _partial_: true      # Hydra will provide missing arguments (optimizer, num_training_steps, num_warmup_steps)
  num_cycles: 0.5      # Single half-cosine (default and most common choice)
  last_epoch: -1       # Start from beginning (-1 means start fresh)
