_target_: src.models.protein_module.ProteinLitModule

# Model architecture parameters
d_model: 1152          # Base model dimension (ESM-C actual embedding size)
d_latent: 896          # Latent dimension for cross-modal fusion and final representation
n_cross_layers: 3      # Cross-modal attention layers between sequence and auxiliary streams
n_heads: 8             # Consider 12 heads 

# Independent dropout hyperparameters
# Attention dropout: 0.12–0.15
attention_dropout: 0.10
# FFN dropout: 0.18–0.22 (FFN dominates params)
ffn_dropout: 0.18
# Gate/fusion MLP dropout: ~0.05
fusion_mlp_dropout: 0.10
# Head dropout: 0.30 (biggest bang for buck)
head_dropout: 0.25
task_type: ${data.task_type}         # Task type: "mf", "bp", or "cc" - automatically from data config
d_prot: 1024                    # ProtT5-BFD embedding dimension
d_ankh: 2560                    # Ankh3-Large embedding dimension (matches ankh_emb.pt)
d_pglm: 2560                    # XTrimoPGLM embedding dimension (matches pglm_emb.pt)

# Training parameters  
warmup_ratio: 0.01     # Warmup ratio for cosine schedule (1% of total training steps)
learning_rate: 1e-4    # Learning rate for all parameters

# SupCon contrastive learning parameters
supcon_on: true                    # Enable/disable SupCon contrastive learning
supcon_lambda: 0.2                # Percentage of BCE loss for SupCon (0.2 = 20% of BCE loss)
supcon_tau: 0.12                   # Temperature for contrastive learning (try 0.05–0.12)
supcon_proj_dim: 256               # Contrastive embedding dimension
supcon_proj_dropout: 0.10          # Dropout for SupCon projection head
supcon_queue_size: 4096            # Memory queue size (0 disables queue)

# Optimizer configuration
optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: ${model.learning_rate}  # Pass learning rate from config
  betas: [0.9, 0.999]   # AdamW momentum parameters
  eps: 1e-8             # Numerical stability epsilon
#CONSIDER REDUCE LR ON PLATEAU
# Learning rate scheduler configuration  
scheduler:
  _target_: src.utils.lr_schedulers.get_cosine_schedule_with_warmup
  _partial_: true      # Hydra will provide missing arguments (optimizer, num_training_steps, num_warmup_steps)
  num_cycles: 0.5      # Single half-cosine (default and most common choice)
  last_epoch: -1       # Start from beginning (-1 means start fresh)
