_target_: src.models.protein_module.ProteinLitModule

# Model architecture parameters
d_esm: 1152          # Base model dimension (ESM-C actual embedding size)
d_latent: 896          # Latent dimension for cross-modal fusion and final representation
n_cross_layers: 3      # Cross-modal attention layers between sequence and auxiliary streams
n_heads: 8             # Number of attention heads

# Independent dropout hyperparameters
attention_dropout: 0.10
ffn_dropout: 0.18
fusion_mlp_dropout: 0.10
head_dropout: 0.25
task_type: ${data.task_type}         # Task type: "mf", "bp", or "cc" - automatically from data config
d_prot: 1024                    # ProtT5 embedding dimension
d_ankh: 2560                    # Ankh3-XLarge embedding dimension (matches ankh_emb.pt)
d_pglm: 2560                    # XTrimoPGLM embedding dimension (matches pglm_emb.pt)

# Training parameters  
warmup_ratio: 0.05     # Warmup ratio for cosine schedule
learning_rate: 1e-4    # Learning rate for all parameters
# Optimizer configuration
optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: ${model.learning_rate}  # Pass learning rate from config
  weight_decay: 0.01
  betas: [0.9, 0.999]   # AdamW momentum parameters
  eps: 1e-8             # Numerical stability epsilon
# Learning rate scheduler configuration  
scheduler:
  _target_: src.utils.lr_schedulers.get_cosine_schedule_with_warmup
  _partial_: true      # Hydra will provide missing arguments (optimizer, num_training_steps, num_warmup_steps)
  num_cycles: 0.5      # Single half-cosine (default and most common choice)
  last_epoch: -1       # Start from beginning (-1 means start fresh)
