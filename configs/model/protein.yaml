_target_: src.models.protein_module.ProteinLitModule

# Model architecture parameters
d_model: 1152          # Base model dimension (ESM-C actual embedding size)
d_msa: 768             # MSA embedding dimension (MSA-Transformer output)
n_seq_layers: 2        # Additional transformer layers on top of ESM-C
n_cross_layers: 2      # Cross-modal attention layers between ESM and MSA
n_heads: 8             # Attention heads for all transformer layers
dropout: 0.15           # General dropout rate for model components
task_type: ${data.task_type}         # Task type: "mf", "bp", or "cc" - automatically from data config
# Training parameters  
warmup_ratio: 0.075     # Warmup ratio for cosine schedule (5% of total training steps)

# MSA Encoder specific dropout parameters (now configurable!)
p_chan: 0.10           # Channel dropout probability (AlphaFold-style embedding dropout)
p_feat: 0.15           # Feature dropout after MSA projection

# Optimizer configuration
optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 1.1e-4             # IMPORTANT: Adjust learning rate based on task type
  weight_decay: 1e-2   # L2 regularization 
  betas: [0.9, 0.999]  # AdamW momentum parameters
  eps: 1e-8            # Numerical stability epsilon

# # Learning rate scheduler configuration  
scheduler:
  _target_: src.utils.lr_schedulers.get_cosine_schedule_with_warmup
  _partial_: true      # Hydra will provide missing arguments (optimizer, num_training_steps, num_warmup_steps)
  num_cycles: 0.5      # Single half-cosine (default and most common choice)
  last_epoch: -1       # Start from beginning (-1 means start fresh)

# Alternative scheduler options (uncomment to use instead):
# scheduler:
#   _target_: torch.optim.lr_scheduler.CosineAnnealingLR
#   _partial_: true
#   T_max: 100           # Max epochs (override in experiments to match trainer.max_epochs)
#   eta_min: 1e-6        # Minimum learning rate at the end of annealing
#   last_epoch: -1       # Start from beginning (-1 means start fresh)

# scheduler:
#   _target_: torch.optim.lr_scheduler.ReduceLROnPlateau  
#   _partial_: true
#   mode: "min"          # Reduce when validation loss stops decreasing
#   factor: 0.5          # Multiply LR by this factor when reducing
#   patience: 10         # Wait this many epochs without improvement
#   threshold: 1e-4      # Minimum change to qualify as improvement
#   min_lr: 1e-7         # Minimum learning rate
#   verbose: true        # Print when LR is reduced 