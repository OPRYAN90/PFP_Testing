_target_: src.models.protein_module.ProteinLitModule

# Model architecture parameters
d_model: 1152          # Base model dimension (ESM-C actual embedding size)
n_cross_layers: 3      # Cross-modal attention layers between sequence and auxiliary streams
n_heads: 8             # Consider 12 heads 
dropout: 0.15          # General dropout rate for model components
task_type: ${data.task_type}         # Task type: "mf", "bp", or "cc" - automatically from data config
d_prot: 1024                    # ProtT5-BFD embedding dimension
d_ankh: 2560                    # Ankh3-Large embedding dimension (matches ankh_emb.pt)
d_pglm: 2560                    # XTrimoPGLM embedding dimension (matches pglm_emb.pt)

# Training parameters  
warmup_ratio: 0.01     # Warmup ratio for cosine schedule (1% of total training steps)
# Learning rates for parameter groups (used inside module's optimizer setup)
lr_main: 1e-4          # LR for fusion/head/cross-attn/etc.
lr_plm: 5e-5           # LR for LoRA params inside PLMs

# Optimizer configuration
optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  betas: [0.9, 0.999]  # AdamW momentum parameters
  eps: 1e-8            # Numerical stability epsilon

# Learning rate scheduler configuration  
scheduler:
  _target_: src.utils.lr_schedulers.get_cosine_schedule_with_warmup
  _partial_: true      # Hydra will provide missing arguments (optimizer, num_training_steps, num_warmup_steps)
  num_cycles: 0.5      # Single half-cosine (default and most common choice)
  last_epoch: -1       # Start from beginning (-1 means start fresh)

# LoRA configuration (matches ProteinLitModule defaults)
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_include_out: true     # adapt attention out-proj in ESM++
lora_include_ffn: true     # adapt FFN up/down in ESM++
