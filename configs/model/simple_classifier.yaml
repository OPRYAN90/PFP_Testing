_target_: src.models.simple_classifier.ProteinLitModule

# Model architecture parameters
d_in: 1152              # Input feature dimension (ESM-C embeddings size)
d_hidden: 768           # Hidden dimension for Transformer layers
num_layers: 8           # Number of Transformer layers (depth)
dropout: 0.1            # General dropout rate for model components

# Task and training parameters
task_type: ${data.task_type}   # Task type: "mf", "bp", or "cc" (inferred from data config)
warmup_ratio: 0.05             # Warm-up ratio for cosine LR schedule (5% of total training steps)

# Optimizer configuration
optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 1e-4              # Learning rate â€“ tune per task if necessary
  weight_decay: 1e-2    # L2 regularisation
  betas: [0.9, 0.999]
  eps: 1e-8

# Learning-rate scheduler configuration
scheduler:
  _target_: src.utils.lr_schedulers.get_cosine_schedule_with_warmup
  _partial_: true      # Hydra will inject (optimizer, num_training_steps, num_warmup_steps)
  num_cycles: 0.5      # Single half-cosine cycle
  last_epoch: -1       # Start fresh (-1) 