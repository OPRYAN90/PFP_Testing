{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e257b8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing equivalence …\n",
      "B= 2 N= 16 L= 32 D=128  maxΔ=0  meanΔ=0\n",
      "B= 1 N=  8 L= 16 D= 64  maxΔ=0  meanΔ=0\n",
      "B= 3 N= 32 L= 64 D=256  maxΔ=0  meanΔ=0\n",
      "\n",
      "FINAL RESULT : ✅ SUCCESS\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Row-self-attention equivalence test:\n",
    "OpenFold vs a “flatten-and-permute” PyTorch port\n",
    "\"\"\"\n",
    "\n",
    "import math, torch, numpy as np\n",
    "import torch.nn as nn\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "#  OpenFold helper layers (trimmed to what the test needs)\n",
    "# ----------------------------------------------------------------------\n",
    "def _prod(nums):          # utility\n",
    "    out = 1\n",
    "    for n in nums: out *= n\n",
    "    return out\n",
    "\n",
    "def _fan(shape, fan=\"fan_in\"):\n",
    "    fan_out, fan_in = shape\n",
    "    if fan == \"fan_in\":  return fan_in\n",
    "    if fan == \"fan_out\": return fan_out\n",
    "    return (fan_in + fan_out) / 2\n",
    "\n",
    "def lecun_normal_(w):\n",
    "    from scipy.stats import truncnorm\n",
    "    f = _fan(w.shape, \"fan_in\")\n",
    "    scale = 1. / max(1, f)\n",
    "    std = math.sqrt(scale) / truncnorm.std(a=-2, b=2, loc=0, scale=1)\n",
    "    vals = truncnorm.rvs(-2, 2, loc=0, scale=std, size=_prod(w.shape))\n",
    "    with torch.no_grad(): w.copy_(torch.tensor(vals.reshape(w.shape)))\n",
    "\n",
    "class OFLinear(nn.Linear):\n",
    "    def __init__(self, inp, outp, bias=False):\n",
    "        super().__init__(inp, outp, bias=bias)\n",
    "        lecun_normal_(self.weight)\n",
    "        if bias: nn.init.zeros_(self.bias)\n",
    "\n",
    "class OFLayerNorm(nn.Module):\n",
    "    def __init__(self, d, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(d))\n",
    "        self.bias   = nn.Parameter(torch.zeros(d))\n",
    "        self.eps    = eps\n",
    "    def forward(self,x): return nn.functional.layer_norm(\n",
    "        x, x.shape[-1:], self.weight, self.bias, self.eps)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "#  OpenFold Attention (row, no pair-bias)\n",
    "# ----------------------------------------------------------------------\n",
    "class OFAttention(nn.Module):\n",
    "    def __init__(self, d, heads):\n",
    "        super().__init__()\n",
    "        dh = d // heads\n",
    "        self.h = heads\n",
    "        self.q = OFLinear(d, d, bias=False)\n",
    "        self.k = OFLinear(d, d, bias=False)\n",
    "        self.v = OFLinear(d, d, bias=False)\n",
    "        self.o = OFLinear(d, d, bias=True)     # Wᵒ\n",
    "        self.g = OFLinear(d, d, bias=True)     # Wᵍ\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.dh = dh\n",
    "    def _split(self,t):        # [..., L, H*dh] -> [..., H, L, dh]\n",
    "        t = t.view(*t.shape[:-1], self.h, self.dh)\n",
    "        return t.transpose(-3,-2)\n",
    "    def forward(self, x, mask=None):\n",
    "        q,k,v = map(self._split,(self.q(x), self.k(x), self.v(x)))\n",
    "        q = q / math.sqrt(self.dh)\n",
    "        a = torch.matmul(q, k.transpose(-2,-1))   # [..., H, L, L]\n",
    "        if mask is not None: a = a + mask         # additive (-inf) bias\n",
    "        a = torch.softmax(a, dim=-1)\n",
    "        o = torch.matmul(a, v)                    # [..., H, L, dh]\n",
    "        o = o.transpose(-3,-2).reshape_as(x)\n",
    "        o = self.o(o) * self.sig(self.g(x))\n",
    "        return o\n",
    "\n",
    "class OFRowAttention(nn.Module):\n",
    "    def __init__(self, d, heads):\n",
    "        super().__init__()\n",
    "        self.norm = OFLayerNorm(d)\n",
    "        self.att  = OFAttention(d, heads)\n",
    "    def forward(self, m, mask=None):\n",
    "        m_norm = self.norm(m)\n",
    "        update = self.att(m_norm, mask)           # no pair-bias -> mask maybe None\n",
    "        return m + update                         # residual inside\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "#  Your (flatten-and-permute) implementation\n",
    "# ----------------------------------------------------------------------\n",
    "class RowwiseDropout(nn.Module):\n",
    "    def __init__(self,p): super().__init__(); self.p=p\n",
    "    def forward(self,x):\n",
    "        if (not self.training) or self.p==0: return x\n",
    "        B,N,*rest = x.shape\n",
    "        mask = (torch.rand(B,N,1,1, device=x.device) > self.p).float()\n",
    "        return x * mask / (1-self.p)\n",
    "\n",
    "class IdentityLinear(nn.Linear):\n",
    "    \"\"\"acts like nn.Identity but still has .weight/.bias attrs\"\"\"\n",
    "    def __init__(self, d):\n",
    "        super().__init__(d,d, bias=False)\n",
    "        nn.init.eye_(self.weight)\n",
    "        for p in self.parameters(): p.requires_grad=False\n",
    "    def forward(self,x): return x\n",
    "\n",
    "class FlatRowAttention(nn.Module):\n",
    "    def __init__(self, d, heads, p_drop=0.):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d)\n",
    "        self.mha  = nn.MultiheadAttention(\n",
    "            d, heads, dropout=p_drop, batch_first=True, bias=False)\n",
    "        self.mha.out_proj = IdentityLinear(d)     # <-   fixed!\n",
    "        self.Wo   = nn.Linear(d,d, bias=True)\n",
    "        self.Wg   = nn.Linear(d,d, bias=True)\n",
    "        nn.init.zeros_(self.Wo.weight); nn.init.zeros_(self.Wo.bias)\n",
    "        nn.init.zeros_(self.Wg.weight); nn.init.ones_(self.Wg.bias)\n",
    "        self.drop = RowwiseDropout(p_drop)\n",
    "    def forward(self,x):                          # x:[B,N,L,D]\n",
    "        B,N,L,D = x.shape\n",
    "        x_n = self.norm(x)\n",
    "        qkv = x_n.permute(0,2,1,3).reshape(B*L, N, D)\n",
    "        y,_ = self.mha(qkv,qkv,qkv, need_weights=False)\n",
    "        y = y.reshape(B,L,N,D).permute(0,2,1,3)\n",
    "        y = self.Wo(y) * torch.sigmoid(self.Wg(x_n))\n",
    "        return x + self.drop(y)                   # residual inside\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "#  Helpers\n",
    "# ----------------------------------------------------------------------\n",
    "def make_data(B=2,N=16,L=32,D=128):\n",
    "    msa  = torch.randn(B,N,L,D)\n",
    "    mask = torch.ones(B,N,L)                      # could randomise later\n",
    "    return msa, mask\n",
    "\n",
    "def align_weights(of, flat):\n",
    "    \"\"\"copy weights so the two nets start identical\"\"\"\n",
    "    flat.norm.weight.data.copy_(of.norm.weight)\n",
    "    flat.norm.bias .data.copy_(of.norm.bias)\n",
    "    # QKV\n",
    "    qkv = flat.mha.in_proj_weight.chunk(3,0)\n",
    "    of.att.q.weight.data.copy_(qkv[0])\n",
    "    of.att.k.weight.data.copy_(qkv[1])\n",
    "    of.att.v.weight.data.copy_(qkv[2])\n",
    "    # Wo / Wg\n",
    "    of.att.o.weight.data.copy_(flat.Wo.weight)\n",
    "    of.att.o.bias .data.copy_(flat.Wo.bias)\n",
    "    of.att.g.weight.data.copy_(flat.Wg.weight)\n",
    "    of.att.g.bias .data.copy_(flat.Wg.bias)\n",
    "\n",
    "def run_pair(B,N,L,D,H):\n",
    "    msa,_ = make_data(B,N,L,D)\n",
    "    of    = OFRowAttention(D,H).eval()\n",
    "    flat  = FlatRowAttention(D,H).eval()\n",
    "    align_weights(of, flat)\n",
    "    with torch.no_grad():\n",
    "        out_ref  = of(msa)             # [B,N,L,D]\n",
    "        out_flat = flat(msa)           # same\n",
    "    diff = (out_ref - out_flat).abs()\n",
    "    print(f\"B={B:2d} N={N:3d} L={L:3d} D={D:3d}  maxΔ={diff.max():.6g}  meanΔ={diff.mean():.6g}\")\n",
    "    return diff.max().item() < 1e-5\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "#  Tests\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"Testing equivalence …\")\n",
    "passed = True\n",
    "for cfg in [(2,16,32,128,8),\n",
    "            (1, 8,16, 64,4),\n",
    "            (3,32,64,256,8)]:          # enlarge cautiously if RAM limited\n",
    "    passed &= run_pair(*cfg)\n",
    "print(\"\\nFINAL RESULT :\", \"✅ SUCCESS\" if passed else \"❌ MISMATCH\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
