{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09630c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.3895313892515355e+38"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.finfo(torch.bfloat16).min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e257b8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing equivalence …\n",
      "B= 2 N= 16 L= 32 D=128  maxΔ=0  meanΔ=0\n",
      "B= 1 N=  8 L= 16 D= 64  maxΔ=0  meanΔ=0\n",
      "B= 3 N= 32 L= 64 D=256  maxΔ=0  meanΔ=0\n",
      "\n",
      "FINAL RESULT : ✅ SUCCESS\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Row-self-attention equivalence test:\n",
    "OpenFold vs a “flatten-and-permute” PyTorch port\n",
    "\"\"\"\n",
    "\n",
    "import math, torch, numpy as np\n",
    "import torch.nn as nn\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "#  OpenFold helper layers (trimmed to what the test needs)\n",
    "# ----------------------------------------------------------------------\n",
    "def _prod(nums):          # utility\n",
    "    out = 1\n",
    "    for n in nums: out *= n\n",
    "    return out\n",
    "\n",
    "def _fan(shape, fan=\"fan_in\"):\n",
    "    fan_out, fan_in = shape\n",
    "    if fan == \"fan_in\":  return fan_in\n",
    "    if fan == \"fan_out\": return fan_out\n",
    "    return (fan_in + fan_out) / 2\n",
    "\n",
    "def lecun_normal_(w):\n",
    "    from scipy.stats import truncnorm\n",
    "    f = _fan(w.shape, \"fan_in\")\n",
    "    scale = 1. / max(1, f)\n",
    "    std = math.sqrt(scale) / truncnorm.std(a=-2, b=2, loc=0, scale=1)\n",
    "    vals = truncnorm.rvs(-2, 2, loc=0, scale=std, size=_prod(w.shape))\n",
    "    with torch.no_grad(): w.copy_(torch.tensor(vals.reshape(w.shape)))\n",
    "\n",
    "class OFLinear(nn.Linear):\n",
    "    def __init__(self, inp, outp, bias=False):\n",
    "        super().__init__(inp, outp, bias=bias)\n",
    "        lecun_normal_(self.weight)\n",
    "        if bias: nn.init.zeros_(self.bias)\n",
    "\n",
    "class OFLayerNorm(nn.Module):\n",
    "    def __init__(self, d, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(d))\n",
    "        self.bias   = nn.Parameter(torch.zeros(d))\n",
    "        self.eps    = eps\n",
    "    def forward(self,x): return nn.functional.layer_norm(\n",
    "        x, x.shape[-1:], self.weight, self.bias, self.eps)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "#  OpenFold Attention (row, no pair-bias)\n",
    "# ----------------------------------------------------------------------\n",
    "class OFAttention(nn.Module):\n",
    "    def __init__(self, d, heads):\n",
    "        super().__init__()\n",
    "        dh = d // heads\n",
    "        self.h = heads\n",
    "        self.q = OFLinear(d, d, bias=False)\n",
    "        self.k = OFLinear(d, d, bias=False)\n",
    "        self.v = OFLinear(d, d, bias=False)\n",
    "        self.o = OFLinear(d, d, bias=True)     # Wᵒ\n",
    "        self.g = OFLinear(d, d, bias=True)     # Wᵍ\n",
    "        self.sig = nn.Sigmoid()\n",
    "        self.dh = dh\n",
    "    def _split(self,t):        # [..., L, H*dh] -> [..., H, L, dh]\n",
    "        t = t.view(*t.shape[:-1], self.h, self.dh)\n",
    "        return t.transpose(-3,-2)\n",
    "    def forward(self, x, mask=None):\n",
    "        q,k,v = map(self._split,(self.q(x), self.k(x), self.v(x)))\n",
    "        q = q / math.sqrt(self.dh)\n",
    "        a = torch.matmul(q, k.transpose(-2,-1))   # [..., H, L, L]\n",
    "        if mask is not None: a = a + mask         # additive (-inf) bias\n",
    "        a = torch.softmax(a, dim=-1)\n",
    "        o = torch.matmul(a, v)                    # [..., H, L, dh]\n",
    "        o = o.transpose(-3,-2).reshape_as(x)\n",
    "        o = self.o(o) * self.sig(self.g(x))\n",
    "        return o\n",
    "\n",
    "class OFRowAttention(nn.Module):\n",
    "    def __init__(self, d, heads):\n",
    "        super().__init__()\n",
    "        self.norm = OFLayerNorm(d)\n",
    "        self.att  = OFAttention(d, heads)\n",
    "    def forward(self, m, mask=None):\n",
    "        m_norm = self.norm(m)\n",
    "        update = self.att(m_norm, mask)           # no pair-bias -> mask maybe None\n",
    "        return m + update                         # residual inside\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "#  Your (flatten-and-permute) implementation\n",
    "# ----------------------------------------------------------------------\n",
    "class RowwiseDropout(nn.Module):\n",
    "    def __init__(self,p): super().__init__(); self.p=p\n",
    "    def forward(self,x):\n",
    "        if (not self.training) or self.p==0: return x\n",
    "        B,N,*rest = x.shape\n",
    "        mask = (torch.rand(B,N,1,1, device=x.device) > self.p).float()\n",
    "        return x * mask / (1-self.p)\n",
    "\n",
    "class IdentityLinear(nn.Linear):\n",
    "    \"\"\"acts like nn.Identity but still has .weight/.bias attrs\"\"\"\n",
    "    def __init__(self, d):\n",
    "        super().__init__(d,d, bias=False)\n",
    "        nn.init.eye_(self.weight)\n",
    "        for p in self.parameters(): p.requires_grad=False\n",
    "    def forward(self,x): return x\n",
    "\n",
    "class FlatRowAttention(nn.Module):\n",
    "    def __init__(self, d, heads, p_drop=0.):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(d)\n",
    "        self.mha  = nn.MultiheadAttention(\n",
    "            d, heads, dropout=p_drop, batch_first=True, bias=False)\n",
    "        self.mha.out_proj = IdentityLinear(d)     # <-   fixed!\n",
    "        self.Wo   = nn.Linear(d,d, bias=True)\n",
    "        self.Wg   = nn.Linear(d,d, bias=True)\n",
    "        nn.init.zeros_(self.Wo.weight); nn.init.zeros_(self.Wo.bias)\n",
    "        nn.init.zeros_(self.Wg.weight); nn.init.ones_(self.Wg.bias)\n",
    "        self.drop = RowwiseDropout(p_drop)\n",
    "    def forward(self,x):                          # x:[B,N,L,D]\n",
    "        B,N,L,D = x.shape\n",
    "        x_n = self.norm(x)\n",
    "        qkv = x_n.permute(0,2,1,3).reshape(B*L, N, D)\n",
    "        y,_ = self.mha(qkv,qkv,qkv, need_weights=False)\n",
    "        y = y.reshape(B,L,N,D).permute(0,2,1,3)\n",
    "        y = self.Wo(y) * torch.sigmoid(self.Wg(x_n))\n",
    "        return x + self.drop(y)                   # residual inside\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "#  Helpers\n",
    "# ----------------------------------------------------------------------\n",
    "def make_data(B=2,N=16,L=32,D=128):\n",
    "    msa  = torch.randn(B,N,L,D)\n",
    "    mask = torch.ones(B,N,L)                      # could randomise later\n",
    "    return msa, mask\n",
    "\n",
    "def align_weights(of, flat):\n",
    "    \"\"\"copy weights so the two nets start identical\"\"\"\n",
    "    flat.norm.weight.data.copy_(of.norm.weight)\n",
    "    flat.norm.bias .data.copy_(of.norm.bias)\n",
    "    # QKV\n",
    "    qkv = flat.mha.in_proj_weight.chunk(3,0)\n",
    "    of.att.q.weight.data.copy_(qkv[0])\n",
    "    of.att.k.weight.data.copy_(qkv[1])\n",
    "    of.att.v.weight.data.copy_(qkv[2])\n",
    "    # Wo / Wg\n",
    "    of.att.o.weight.data.copy_(flat.Wo.weight)\n",
    "    of.att.o.bias .data.copy_(flat.Wo.bias)\n",
    "    of.att.g.weight.data.copy_(flat.Wg.weight)\n",
    "    of.att.g.bias .data.copy_(flat.Wg.bias)\n",
    "\n",
    "def run_pair(B,N,L,D,H):\n",
    "    msa,_ = make_data(B,N,L,D)\n",
    "    of    = OFRowAttention(D,H).eval()\n",
    "    flat  = FlatRowAttention(D,H).eval()\n",
    "    align_weights(of, flat)\n",
    "    with torch.no_grad():\n",
    "        out_ref  = of(msa)             # [B,N,L,D]\n",
    "        out_flat = flat(msa)           # same\n",
    "    diff = (out_ref - out_flat).abs()\n",
    "    print(f\"B={B:2d} N={N:3d} L={L:3d} D={D:3d}  maxΔ={diff.max():.6g}  meanΔ={diff.mean():.6g}\")\n",
    "    return diff.max().item() < 1e-5\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "#  Tests\n",
    "# ----------------------------------------------------------------------\n",
    "print(\"Testing equivalence …\")\n",
    "passed = True\n",
    "for cfg in [(2,16,32,128,8),\n",
    "            (1, 8,16, 64,4),\n",
    "            (3,32,64,256,8)]:          # enlarge cautiously if RAM limited\n",
    "    passed &= run_pair(*cfg)\n",
    "print(\"\\nFINAL RESULT :\", \"✅ SUCCESS\" if passed else \"❌ MISMATCH\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edc6b589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, subprocess, sys, importlib\n",
    "\n",
    "# ── 1.  Clone Cutlass once if it isn't there ───────────────────────────────\n",
    "cutlass_dir = pathlib.Path.home() / \"cutlass\"\n",
    "if not cutlass_dir.exists():\n",
    "    subprocess.check_call([\"git\", \"clone\", \"--depth\", \"1\",\n",
    "                           \"https://github.com/NVIDIA/cutlass\", str(cutlass_dir)])\n",
    "\n",
    "# ── 2.  Export CUTLASS_PATH for *this* Python process ──────────────────────\n",
    "os.environ[\"CUTLASS_PATH\"] = str(cutlass_dir)\n",
    "\n",
    "# ── 3.  (Optional) ensure our change is permanent for future kernels ───────\n",
    "home_bashrc = pathlib.Path.home() / \".bashrc\"\n",
    "line = f'export CUTLASS_PATH=\"{cutlass_dir}\"\\n'\n",
    "if line not in home_bashrc.read_text():\n",
    "    home_bashrc.open(\"a\").write(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bf9d0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success → torch.Size([1, 32, 128, 4, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from deepspeed.ops.deepspeed4science import DS4Sci_EvoformerAttention as evo\n",
    "\n",
    "B, N_seq, N_res, H, D = 1, 32, 128, 4, 32\n",
    "Q = torch.randn(B, N_seq, N_res, H, D, dtype=torch.float16, device=\"cuda\")\n",
    "K = torch.randn_like(Q);  V = torch.randn_like(Q)\n",
    "# build a -1e9 additive bias *in the same dtype as Q*\n",
    "pad_bias = torch.full((B, N_seq, 1, 1, N_res),\n",
    "                      fill_value=-1e4,\n",
    "                      dtype=Q.dtype,          # float16 or bfloat16\n",
    "                      device=\"cuda\")\n",
    "pair_bias = torch.zeros(B, 1, H, N_res, N_res, dtype=torch.float16, device=\"cuda\")\n",
    "\n",
    "out = evo(Q, K, V, [pad_bias, pair_bias])\n",
    "print(\"Success →\", out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d643d47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+cu121\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/deepspeed/ops/deepspeed4science/evoformer_attn.py\n"
     ]
    }
   ],
   "source": [
    "import torch, deepspeed\n",
    "print(torch.__version__)        # 2.1.2+cu121\n",
    "import deepspeed.ops.deepspeed4science.evoformer_attn as ds_evo\n",
    "print(ds_evo.__file__)          # confirm you’re importing the patched file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "371dc662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-12 01:50:59,703] [WARNING] [real_accelerator.py:209:get_accelerator] Setting accelerator to CPU. If you have GPU or other accelerator, we were unable to detect it.\n",
      "[2025-07-12 01:50:59,708] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cpu (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-12 01:51:06,574] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n",
      "torch  ⟶ 2.1.2+cu121\n",
      "deepspeed ⟶ 0.17.2\n",
      "python path ⟶ /home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/deepspeed/__init__.py\n",
      "evoformer_attn CUDA so ⟶ NOT LOADED\n"
     ]
    }
   ],
   "source": [
    "import importlib, os, subprocess, sys, re, json, torch\n",
    "\n",
    "import deepspeed\n",
    "from deepspeed.ops.deepspeed4science import evoformer_attn\n",
    "\n",
    "print(f\"torch  ⟶ {torch.__version__}\")\n",
    "print(f\"deepspeed ⟶ {deepspeed.__version__}\")\n",
    "print(f\"python path ⟶ {deepspeed.__file__}\")\n",
    "\n",
    "so_path = evoformer_attn.kernel_.__file__ if getattr(evoformer_attn, \"kernel_\", None) else \"NOT LOADED\"\n",
    "print(f\"evoformer_attn CUDA so ⟶ {so_path}\")\n",
    "\n",
    "if os.path.exists(so_path):\n",
    "    try:\n",
    "        # read the first 2k bytes to sniff for view/reshape strings\n",
    "        with open(so_path, \"rb\") as f: header = f.read(2048)\n",
    "        has_reshape = b\".reshape(\" in header or b\"reshape(\" in header\n",
    "        print(\"patched reshape present?  \", has_reshape)\n",
    "    except Exception as e:  \n",
    "        print(\"could not inspect binary:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05050117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu128\n",
      "Requirement already satisfied: torch in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.1.2+cu121)\n",
      "Requirement already satisfied: torchvision in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.16.2+cu121)\n",
      "Requirement already satisfied: torchaudio in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.1.2+cu121)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: triton==2.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: numpy in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchvision) (1.25.2)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->torchvision) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d4628bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "\n",
    "class AFNOMix2D(nn.Module):\n",
    "    \"\"\"\n",
    "    Adaptive Fourier Neural Operator token mixer (2-D) with padding support.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    d_model : int   # channel dimension D\n",
    "    num_blocks : int = 8      # block-diagonal chunks along D\n",
    "    sparsity_thresh : float = 1e-2   # λ for soft-shrink\n",
    "    hard_thresh_frac : float = 1.0   # keep lowest-freq fraction (≤ 1.0)\n",
    "    hidden_factor : int = 1          # expansion in inner GELU\n",
    "    dropout : float = 0.1\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_blocks: int = 8,\n",
    "        sparsity_thresh: float = 1e-2,\n",
    "        hard_thresh_frac: float = 1.0,\n",
    "        hidden_factor: int = 1,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert d_model % num_blocks == 0, \"d_model must divide num_blocks\"\n",
    "        blk = d_model // num_blocks\n",
    "\n",
    "        self.norm   = nn.LayerNorm(d_model)\n",
    "        self.drop   = nn.Dropout(dropout)\n",
    "        self.gamma  = nn.Parameter(torch.full((d_model,), 0.1))\n",
    "\n",
    "        scale = 0.02\n",
    "        # real&imag weights packed in dim-0 (2, …)\n",
    "        self.w1 = nn.Parameter(scale * torch.randn(2, num_blocks, blk,\n",
    "                                                   blk * hidden_factor))\n",
    "        self.b1 = nn.Parameter(scale * torch.randn(2, num_blocks,\n",
    "                                                   blk * hidden_factor))\n",
    "        self.w2 = nn.Parameter(scale * torch.randn(2, num_blocks,\n",
    "                                                   blk * hidden_factor, blk))\n",
    "        self.b2 = nn.Parameter(scale * torch.randn(2, num_blocks, blk))\n",
    "\n",
    "        self.num_blocks      = num_blocks\n",
    "        self.block_size      = blk\n",
    "        self.sparsity_thresh = sparsity_thresh\n",
    "        self.hard_frac       = hard_thresh_frac\n",
    "        self.hidden_factor   = hidden_factor\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # helpers\n",
    "    # ----------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _soft_shrink_complex(z: torch.Tensor, lambd: float):\n",
    "        \"\"\"Complex soft-shrink with safe div.\"\"\"\n",
    "        mag = z.abs()\n",
    "        return z * F.relu(mag - lambd) / (mag + 1e-9)\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # forward\n",
    "    # ----------------------------------------------------------\n",
    "    def forward(self, x: torch.Tensor, pad: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x   : [B, S, L, D]  – real tensor\n",
    "        pad : [B, S, L]     – True ⇒ padded token\n",
    "        \"\"\"\n",
    "        B, S, L, D = x.shape\n",
    "        assert D == self.block_size * self.num_blocks, \"dim mismatch\"\n",
    "\n",
    "        # ------------------------------------------------------\n",
    "        # 0) mask pads & layer-norm\n",
    "        # ------------------------------------------------------\n",
    "        x_masked = x.masked_fill(pad.unsqueeze(-1), 0.)\n",
    "        x_n      = self.norm(x_masked)\n",
    "\n",
    "        # ------------------------------------------------------\n",
    "        # 1) 2-D real FFT  →  [B, S_hat, L_hat, D] (complex)\n",
    "        # ------------------------------------------------------\n",
    "        z = torch.fft.rfft2(x_n, dim=(-3, -2), norm=\"ortho\")   # complex\n",
    "        S_hat, L_hat = z.shape[-3:-1]          # S stays full, L halves\n",
    "\n",
    "        # ------------------------------------------------------\n",
    "        # 2) optional hard truncation of high-freq modes\n",
    "        # ------------------------------------------------------\n",
    "        if self.hard_frac < 1.0:\n",
    "            k_S = int(S_hat * self.hard_frac)\n",
    "            k_L = int(L_hat * self.hard_frac)\n",
    "            z[..., k_S:, :, :] = 0\n",
    "            z[..., :, k_L:, :] = 0\n",
    "\n",
    "        # ------------------------------------------------------\n",
    "        # 3) block-diagonal mixing in frequency domain\n",
    "        # ------------------------------------------------------\n",
    "        # reshape channel dim into (B, Ŝ, L̂, nb, blk)\n",
    "        z = z.view(B, S_hat, L_hat, self.num_blocks, self.block_size)\n",
    "\n",
    "        # split real/imag weights like the official code\n",
    "        w1_r, w1_i = self.w1[0], self.w1[1]\n",
    "        b1_r, b1_i = self.b1[0], self.b1[1]\n",
    "        w2_r, w2_i = self.w2[0], self.w2[1]\n",
    "        b2_r, b2_i = self.b2[0], self.b2[1]\n",
    "\n",
    "        # first linear + GELU in complex domain\n",
    "        o1_r = (torch.einsum('...bi,bij->...bj',  z.real, w1_r) -\n",
    "                torch.einsum('...bi,bij->...bj',  z.imag, w1_i) + b1_r)\n",
    "        o1_i = (torch.einsum('...bi,bij->...bj',  z.imag, w1_r) +\n",
    "                torch.einsum('...bi,bij->...bj',  z.real, w1_i) + b1_i)\n",
    "        o1   = torch.complex(o1_r, o1_i)\n",
    "        o1   = torch.complex(F.gelu(o1.real), F.gelu(o1.imag))\n",
    "\n",
    "        # second linear\n",
    "        o2_r = (torch.einsum('...bi,bij->...bj',  o1.real, w2_r) -\n",
    "                torch.einsum('...bi,bij->...bj',  o1.imag, w2_i) + b2_r)\n",
    "        o2_i = (torch.einsum('...bi,bij->...bj',  o1.imag, w2_r) +\n",
    "                torch.einsum('...bi,bij->...bj',  o1.real, w2_i) + b2_i)\n",
    "        z    = torch.complex(o2_r, o2_i)\n",
    "\n",
    "        # ------------------------------------------------------\n",
    "        # 4) soft-shrink sparsity, reshape back\n",
    "        # ------------------------------------------------------\n",
    "        z = self._soft_shrink_complex(z, self.sparsity_thresh)\n",
    "        z = z.view(B, S_hat, L_hat, D)\n",
    "\n",
    "        # ------------------------------------------------------\n",
    "        # 5) inverse FFT → residual + dropout, re-apply pad\n",
    "        # ------------------------------------------------------\n",
    "        y = torch.fft.irfft2(z, s=(S, L), dim=(-3, -2), norm=\"ortho\")\n",
    "        y = y.masked_fill(pad.unsqueeze(-1), 0.)\n",
    "\n",
    "        return x + self.drop(self.gamma * y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e02d0eb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x   = torch.randn(2, 256, 1024, 768)\n",
    "pad = torch.zeros (2, 256, 1024, dtype=torch.bool)\n",
    "mix = AFNOMix2D(768)\n",
    "y   = mix(x, pad)\n",
    "assert y.shape == x.shape and y.dtype == x.dtype\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
