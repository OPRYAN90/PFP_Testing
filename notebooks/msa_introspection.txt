
Environment
===========

{
  "python": "3.10.18 (main, Jun  5 2025, 13:14:17) [GCC 11.2.0]",
  "platform": "Linux-6.8.0-1033-gcp-x86_64-with-glibc2.31",
  "torch": "2.7.1+cu126",
  "esm": "unknown",
  "cuda_available": false,
  "cuda_device_count": 0
}

Model Class
===========

<class 'esm.model.msa_transformer.MSATransformer'>

Named Modules (tree; truncated)
===============================

<root> :: MSATransformer
embed_tokens :: Embedding
dropout_module :: Dropout
layers :: ModuleList
layers.0 :: AxialTransformerLayer
layers.0.row_self_attention :: NormalizedResidualBlock
layers.0.row_self_attention.layer :: RowSelfAttention
layers.0.row_self_attention.layer.k_proj :: Linear
layers.0.row_self_attention.layer.v_proj :: Linear
layers.0.row_self_attention.layer.q_proj :: Linear
layers.0.row_self_attention.layer.out_proj :: Linear
layers.0.row_self_attention.layer.dropout_module :: Dropout
layers.0.row_self_attention.dropout_module :: Dropout
layers.0.row_self_attention.layer_norm :: LayerNorm
layers.0.column_self_attention :: NormalizedResidualBlock
layers.0.column_self_attention.layer :: ColumnSelfAttention
layers.0.column_self_attention.layer.k_proj :: Linear
layers.0.column_self_attention.layer.v_proj :: Linear
layers.0.column_self_attention.layer.q_proj :: Linear
layers.0.column_self_attention.layer.out_proj :: Linear
layers.0.column_self_attention.layer.dropout_module :: Dropout
layers.0.column_self_attention.dropout_module :: Dropout
layers.0.column_self_attention.layer_norm :: LayerNorm
layers.0.feed_forward_layer :: NormalizedResidualBlock
layers.0.feed_forward_layer.layer :: FeedForwardNetwork
layers.0.feed_forward_layer.layer.activation_fn :: GELU
layers.0.feed_forward_layer.layer.activation_dropout_module :: Dropout
layers.0.feed_forward_layer.layer.fc1 :: Linear
layers.0.feed_forward_layer.layer.fc2 :: Linear
layers.0.feed_forward_layer.dropout_module :: Dropout
layers.0.feed_forward_layer.layer_norm :: LayerNorm
layers.1 :: AxialTransformerLayer
layers.1.row_self_attention :: NormalizedResidualBlock
layers.1.row_self_attention.layer :: RowSelfAttention
layers.1.row_self_attention.layer.k_proj :: Linear
layers.1.row_self_attention.layer.v_proj :: Linear
layers.1.row_self_attention.layer.q_proj :: Linear
layers.1.row_self_attention.layer.out_proj :: Linear
layers.1.row_self_attention.layer.dropout_module :: Dropout
layers.1.row_self_attention.dropout_module :: Dropout
layers.1.row_self_attention.layer_norm :: LayerNorm
layers.1.column_self_attention :: NormalizedResidualBlock
layers.1.column_self_attention.layer :: ColumnSelfAttention
layers.1.column_self_attention.layer.k_proj :: Linear
layers.1.column_self_attention.layer.v_proj :: Linear
layers.1.column_self_attention.layer.q_proj :: Linear
layers.1.column_self_attention.layer.out_proj :: Linear
layers.1.column_self_attention.layer.dropout_module :: Dropout
layers.1.column_self_attention.dropout_module :: Dropout
layers.1.column_self_attention.layer_norm :: LayerNorm
layers.1.feed_forward_layer :: NormalizedResidualBlock
layers.1.feed_forward_layer.layer :: FeedForwardNetwork
layers.1.feed_forward_layer.layer.activation_fn :: GELU
layers.1.feed_forward_layer.layer.activation_dropout_module :: Dropout
layers.1.feed_forward_layer.layer.fc1 :: Linear
layers.1.feed_forward_layer.layer.fc2 :: Linear
layers.1.feed_forward_layer.dropout_module :: Dropout
layers.1.feed_forward_layer.layer_norm :: LayerNorm
layers.2 :: AxialTransformerLayer
layers.2.row_self_attention :: NormalizedResidualBlock
layers.2.row_self_attention.layer :: RowSelfAttention
layers.2.row_self_attention.layer.k_proj :: Linear
layers.2.row_self_attention.layer.v_proj :: Linear
layers.2.row_self_attention.layer.q_proj :: Linear
layers.2.row_self_attention.layer.out_proj :: Linear
layers.2.row_self_attention.layer.dropout_module :: Dropout
layers.2.row_self_attention.dropout_module :: Dropout
layers.2.row_self_attention.layer_norm :: LayerNorm
layers.2.column_self_attention :: NormalizedResidualBlock
layers.2.column_self_attention.layer :: ColumnSelfAttention
layers.2.column_self_attention.layer.k_proj :: Linear
layers.2.column_self_attention.layer.v_proj :: Linear
layers.2.column_self_attention.layer.q_proj :: Linear
layers.2.column_self_attention.layer.out_proj :: Linear
layers.2.column_self_attention.layer.dropout_module :: Dropout
layers.2.column_self_attention.dropout_module :: Dropout
layers.2.column_self_attention.layer_norm :: LayerNorm
layers.2.feed_forward_layer :: NormalizedResidualBlock
layers.2.feed_forward_layer.layer :: FeedForwardNetwork
layers.2.feed_forward_layer.layer.activation_fn :: GELU
layers.2.feed_forward_layer.layer.activation_dropout_module :: Dropout
layers.2.feed_forward_layer.layer.fc1 :: Linear
layers.2.feed_forward_layer.layer.fc2 :: Linear
layers.2.feed_forward_layer.dropout_module :: Dropout
layers.2.feed_forward_layer.layer_norm :: LayerNorm
layers.3 :: AxialTransformerLayer
layers.3.row_self_attention :: NormalizedResidualBlock
layers.3.row_self_attention.layer :: RowSelfAttention
layers.3.row_self_attention.layer.k_proj :: Linear
layers.3.row_self_attention.layer.v_proj :: Linear
layers.3.row_self_attention.layer.q_proj :: Linear
layers.3.row_self_attention.layer.out_proj :: Linear
layers.3.row_self_attention.layer.dropout_module :: Dropout
layers.3.row_self_attention.dropout_module :: Dropout
layers.3.row_self_attention.layer_norm :: LayerNorm
layers.3.column_self_attention :: NormalizedResidualBlock
layers.3.column_self_attention.layer :: ColumnSelfAttention
layers.3.column_self_attention.layer.k_proj :: Linear
layers.3.column_self_attention.layer.v_proj :: Linear
layers.3.column_self_attention.layer.q_proj :: Linear
layers.3.column_self_attention.layer.out_proj :: Linear
layers.3.column_self_attention.layer.dropout_module :: Dropout
layers.3.column_self_attention.dropout_module :: Dropout
layers.3.column_self_attention.layer_norm :: LayerNorm
layers.3.feed_forward_layer :: NormalizedResidualBlock
layers.3.feed_forward_layer.layer :: FeedForwardNetwork
layers.3.feed_forward_layer.layer.activation_fn :: GELU
layers.3.feed_forward_layer.layer.activation_dropout_module :: Dropout
layers.3.feed_forward_layer.layer.fc1 :: Linear
layers.3.feed_forward_layer.layer.fc2 :: Linear
layers.3.feed_forward_layer.dropout_module :: Dropout
layers.3.feed_forward_layer.layer_norm :: LayerNorm
layers.4 :: AxialTransformerLayer
layers.4.row_self_attention :: NormalizedResidualBlock
layers.4.row_self_attention.layer :: RowSelfAttention
layers.4.row_self_attention.layer.k_proj :: Linear
layers.4.row_self_attention.layer.v_proj :: Linear
layers.4.row_self_attention.layer.q_proj :: Linear
layers.4.row_self_attention.layer.out_proj :: Linear
layers.4.row_self_attention.layer.dropout_module :: Dropout
layers.4.row_self_attention.dropout_module :: Dropout
layers.4.row_self_attention.layer_norm :: LayerNorm
layers.4.column_self_attention :: NormalizedResidualBlock
layers.4.column_self_attention.layer :: ColumnSelfAttention
layers.4.column_self_attention.layer.k_proj :: Linear
layers.4.column_self_attention.layer.v_proj :: Linear
layers.4.column_self_attention.layer.q_proj :: Linear
layers.4.column_self_attention.layer.out_proj :: Linear
layers.4.column_self_attention.layer.dropout_module :: Dropout
layers.4.column_self_attention.dropout_module :: Dropout
layers.4.column_self_attention.layer_norm :: LayerNorm
layers.4.feed_forward_layer :: NormalizedResidualBlock
layers.4.feed_forward_layer.layer :: FeedForwardNetwork
layers.4.feed_forward_layer.layer.activation_fn :: GELU
layers.4.feed_forward_layer.layer.activation_dropout_module :: Dropout
layers.4.feed_forward_layer.layer.fc1 :: Linear
layers.4.feed_forward_layer.layer.fc2 :: Linear
layers.4.feed_forward_layer.dropout_module :: Dropout
layers.4.feed_forward_layer.layer_norm :: LayerNorm
layers.5 :: AxialTransformerLayer
layers.5.row_self_attention :: NormalizedResidualBlock
layers.5.row_self_attention.layer :: RowSelfAttention
layers.5.row_self_attention.layer.k_proj :: Linear
layers.5.row_self_attention.layer.v_proj :: Linear
layers.5.row_self_attention.layer.q_proj :: Linear
layers.5.row_self_attention.layer.out_proj :: Linear
layers.5.row_self_attention.layer.dropout_module :: Dropout
layers.5.row_self_attention.dropout_module :: Dropout
layers.5.row_self_attention.layer_norm :: LayerNorm
layers.5.column_self_attention :: NormalizedResidualBlock
layers.5.column_self_attention.layer :: ColumnSelfAttention
layers.5.column_self_attention.layer.k_proj :: Linear
layers.5.column_self_attention.layer.v_proj :: Linear
layers.5.column_self_attention.layer.q_proj :: Linear
layers.5.column_self_attention.layer.out_proj :: Linear
layers.5.column_self_attention.layer.dropout_module :: Dropout
layers.5.column_self_attention.dropout_module :: Dropout
layers.5.column_self_attention.layer_norm :: LayerNorm
layers.5.feed_forward_layer :: NormalizedResidualBlock
layers.5.feed_forward_layer.layer :: FeedForwardNetwork
layers.5.feed_forward_layer.layer.activation_fn :: GELU
layers.5.feed_forward_layer.layer.activation_dropout_module :: Dropout
layers.5.feed_forward_layer.layer.fc1 :: Linear
layers.5.feed_forward_layer.layer.fc2 :: Linear
layers.5.feed_forward_layer.dropout_module :: Dropout
layers.5.feed_forward_layer.layer_norm :: LayerNorm
layers.6 :: AxialTransformerLayer
layers.6.row_self_attention :: NormalizedResidualBlock
layers.6.row_self_attention.layer :: RowSelfAttention
layers.6.row_self_attention.layer.k_proj :: Linear
layers.6.row_self_attention.layer.v_proj :: Linear
layers.6.row_self_attention.layer.q_proj :: Linear
layers.6.row_self_attention.layer.out_proj :: Linear
layers.6.row_self_attention.layer.dropout_module :: Dropout
layers.6.row_self_attention.dropout_module :: Dropout
layers.6.row_self_attention.layer_norm :: LayerNorm
layers.6.column_self_attention :: NormalizedResidualBlock
layers.6.column_self_attention.layer :: ColumnSelfAttention
layers.6.column_self_attention.layer.k_proj :: Linear
layers.6.column_self_attention.layer.v_proj :: Linear
layers.6.column_self_attention.layer.q_proj :: Linear
layers.6.column_self_attention.layer.out_proj :: Linear
layers.6.column_self_attention.layer.dropout_module :: Dropout
layers.6.column_self_attention.dropout_module :: Dropout
layers.6.column_self_attention.layer_norm :: LayerNorm
layers.6.feed_forward_layer :: NormalizedResidualBlock
layers.6.feed_forward_layer.layer :: FeedForwardNetwork
layers.6.feed_forward_layer.layer.activation_fn :: GELU
layers.6.feed_forward_layer.layer.activation_dropout_module :: Dropout
layers.6.feed_forward_layer.layer.fc1 :: Linear
layers.6.feed_forward_layer.layer.fc2 :: Linear
layers.6.feed_forward_layer.dropout_module :: Dropout
layers.6.feed_forward_layer.layer_norm :: LayerNorm
layers.7 :: AxialTransformerLayer
layers.7.row_self_attention :: NormalizedResidualBlock
layers.7.row_self_attention.layer :: RowSelfAttention
layers.7.row_self_attention.layer.k_proj :: Linear
layers.7.row_self_attention.layer.v_proj :: Linear
layers.7.row_self_attention.layer.q_proj :: Linear
layers.7.row_self_attention.layer.out_proj :: Linear
layers.7.row_self_attention.layer.dropout_module :: Dropout
layers.7.row_self_attention.dropout_module :: Dropout
layers.7.row_self_attention.layer_norm :: LayerNorm
layers.7.column_self_attention :: NormalizedResidualBlock
layers.7.column_self_attention.layer :: ColumnSelfAttention
layers.7.column_self_attention.layer.k_proj :: Linear
layers.7.column_self_attention.layer.v_proj :: Linear
layers.7.column_self_attention.layer.q_proj :: Linear
layers.7.column_self_attention.layer.out_proj :: Linear
layers.7.column_self_attention.layer.dropout_module :: Dropout
layers.7.column_self_attention.dropout_module :: Dropout
layers.7.column_self_attention.layer_norm :: LayerNorm
layers.7.feed_forward_layer :: NormalizedResidualBlock
layers.7.feed_forward_layer.layer :: FeedForwardNetwork
layers.7.feed_forward_layer.layer.activation_fn :: GELU
layers.7.feed_forward_layer.layer.activation_dropout_module :: Dropout
layers.7.feed_forward_layer.layer.fc1 :: Linear
layers.7.feed_forward_layer.layer.fc2 :: Linear
layers.7.feed_forward_layer.dropout_module :: Dropout
layers.7.feed_forward_layer.layer_norm :: LayerNorm
layers.8 :: AxialTransformerLayer
layers.8.row_self_attention :: NormalizedResidualBlock
layers.8.row_self_attention.layer :: RowSelfAttention
layers.8.row_self_attention.layer.k_proj :: Linear
layers.8.row_self_attention.layer.v_proj :: Linear
layers.8.row_self_attention.layer.q_proj :: Linear
layers.8.row_self_attention.layer.out_proj :: Linear
layers.8.row_self_attention.layer.dropout_module :: Dropout
layers.8.row_self_attention.dropout_module :: Dropout
layers.8.row_self_attention.layer_norm :: LayerNorm
layers.8.column_self_attention :: NormalizedResidualBlock
layers.8.column_self_attention.layer :: ColumnSelfAttention
layers.8.column_self_attention.layer.k_proj :: Linear
layers.8.column_self_attention.layer.v_proj :: Linear
layers.8.column_self_attention.layer.q_proj :: Linear
layers.8.column_self_attention.layer.out_proj :: Linear
layers.8.column_self_attention.layer.dropout_module :: Dropout
layers.8.column_self_attention.dropout_module :: Dropout
layers.8.column_self_attention.layer_norm :: LayerNorm
layers.8.feed_forward_layer :: NormalizedResidualBlock
layers.8.feed_forward_layer.layer :: FeedForwardNetwork
layers.8.feed_forward_layer.layer.activation_fn :: GELU
layers.8.feed_forward_layer.layer.activation_dropout_module :: Dropout
layers.8.feed_forward_layer.layer.fc1 :: Linear
layers.8.feed_forward_layer.layer.fc2 :: Linear
layers.8.feed_forward_layer.dropout_module :: Dropout
layers.8.feed_forward_layer.layer_norm :: LayerNorm
layers.9 :: AxialTransformerLayer
layers.9.row_self_attention :: NormalizedResidualBlock
layers.9.row_self_attention.layer :: RowSelfAttention
layers.9.row_self_attention.layer.k_proj :: Linear
layers.9.row_self_attention.layer.v_proj :: Linear
layers.9.row_self_attention.layer.q_proj :: Linear
layers.9.row_self_attention.layer.out_proj :: Linear
layers.9.row_self_attention.layer.dropout_module :: Dropout
layers.9.row_self_attention.dropout_module :: Dropout
layers.9.row_self_attention.layer_norm :: LayerNorm
layers.9.column_self_attention :: NormalizedResidualBlock
layers.9.column_self_attention.layer :: ColumnSelfAttention
layers.9.column_self_attention.layer.k_proj :: Linear
layers.9.column_self_attention.layer.v_proj :: Linear
layers.9.column_self_attention.layer.q_proj :: Linear
layers.9.column_self_attention.layer.out_proj :: Linear
layers.9.column_self_attention.layer.dropout_module :: Dropout
layers.9.column_self_attention.dropout_module :: Dropout
layers.9.column_self_attention.layer_norm :: LayerNorm
layers.9.feed_forward_layer :: NormalizedResidualBlock
layers.9.feed_forward_layer.layer :: FeedForwardNetwork
layers.9.feed_forward_layer.layer.activation_fn :: GELU
layers.9.feed_forward_layer.layer.activation_dropout_module :: Dropout
layers.9.feed_forward_layer.layer.fc1 :: Linear
layers.9.feed_forward_layer.layer.fc2 :: Linear
layers.9.feed_forward_layer.dropout_module :: Dropout
layers.9.feed_forward_layer.layer_norm :: LayerNorm
layers.10 :: AxialTransformerLayer
layers.10.row_self_attention :: NormalizedResidualBlock
layers.10.row_self_attention.layer :: RowSelfAttention
layers.10.row_self_attention.layer.k_proj :: Linear
layers.10.row_self_attention.layer.v_proj :: Linear
layers.10.row_self_attention.layer.q_proj :: Linear
layers.10.row_self_attention.layer.out_proj :: Linear
layers.10.row_self_attention.layer.dropout_module :: Dropout
layers.10.row_self_attention.dropout_module :: Dropout
layers.10.row_self_attention.layer_norm :: LayerNorm
layers.10.column_self_attention :: NormalizedResidualBlock
layers.10.column_self_attention.layer :: ColumnSelfAttention
layers.10.column_self_attention.layer.k_proj :: Linear
layers.10.column_self_attention.layer.v_proj :: Linear
layers.10.column_self_attention.layer.q_proj :: Linear
layers.10.column_self_attention.layer.out_proj :: Linear
layers.10.column_self_attention.layer.dropout_module :: Dropout
layers.10.column_self_attention.dropout_module :: Dropout
layers.10.column_self_attention.layer_norm :: LayerNorm
layers.10.feed_forward_layer :: NormalizedResidualBlock
layers.10.feed_forward_layer.layer :: FeedForwardNetwork
layers.10.feed_forward_layer.layer.activation_fn :: GELU
layers.10.feed_forward_layer.layer.activation_dropout_module :: Dropout
layers.10.feed_forward_layer.layer.fc1 :: Linear
layers.10.feed_forward_layer.layer.fc2 :: Linear
layers.10.feed_forward_layer.dropout_module :: Dropout
layers.10.feed_forward_layer.layer_norm :: LayerNorm
layers.11 :: AxialTransformerLayer
layers.11.row_self_attention :: NormalizedResidualBlock
layers.11.row_self_attention.layer :: RowSelfAttention
layers.11.row_self_attention.layer.k_proj :: Linear
layers.11.row_self_attention.layer.v_proj :: Linear
layers.11.row_self_attention.layer.q_proj :: Linear
layers.11.row_self_attention.layer.out_proj :: Linear
layers.11.row_self_attention.layer.dropout_module :: Dropout
layers.11.row_self_attention.dropout_module :: Dropout
layers.11.row_self_attention.layer_norm :: LayerNorm
layers.11.column_self_attention :: NormalizedResidualBlock
layers.11.column_self_attention.layer :: ColumnSelfAttention
layers.11.column_self_attention.layer.k_proj :: Linear
layers.11.column_self_attention.layer.v_proj :: Linear
layers.11.column_self_attention.layer.q_proj :: Linear
layers.11.column_self_attention.layer.out_proj :: Linear
layers.11.column_self_attention.layer.dropout_module :: Dropout
layers.11.column_self_attention.dropout_module :: Dropout
layers.11.column_self_attention.layer_norm :: LayerNorm
layers.11.feed_forward_layer :: NormalizedResidualBlock
layers.11.feed_forward_layer.layer :: FeedForwardNetwork
layers.11.feed_forward_layer.layer.activation_fn :: GELU
layers.11.feed_forward_layer.layer.activation_dropout_module :: Dropout
layers.11.feed_forward_layer.layer.fc1 :: Linear
layers.11.feed_forward_layer.layer.fc2 :: Linear
layers.11.feed_forward_layer.dropout_module :: Dropout
layers.11.feed_forward_layer.layer_norm :: LayerNorm
contact_head :: ContactPredictionHead
contact_head.regression :: Linear
contact_head.activation :: Sigmoid
embed_positions :: LearnedPositionalEmbedding
emb_layer_norm_before :: LayerNorm
emb_layer_norm_after :: LayerNorm
lm_head :: RobertaLMHead
lm_head.dense :: Linear
lm_head.layer_norm :: LayerNorm

Attention Projection Linears (q/k/v/out) — ALL HITS
===================================================

layers.0.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.0.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.0.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.0.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.0.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.0.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.0.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.0.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.1.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.1.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.1.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.1.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.1.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.1.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.1.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.1.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.2.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.2.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.2.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.2.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.2.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.2.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.2.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.2.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.3.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.3.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.3.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.3.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.3.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.3.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.3.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.3.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.4.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.4.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.4.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.4.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.4.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.4.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.4.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.4.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.5.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.5.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.5.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.5.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.5.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.5.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.5.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.5.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.6.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.6.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.6.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.6.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.6.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.6.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.6.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.6.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.7.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.7.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.7.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.7.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.7.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.7.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.7.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.7.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.8.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.8.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.8.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.8.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.8.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.8.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.8.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.8.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.9.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.9.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.9.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.9.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.9.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.9.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.9.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.9.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.10.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.10.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.10.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.10.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.10.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.10.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.10.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.10.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.11.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.11.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.11.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.11.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.11.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.11.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.11.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.11.column_self_attention.layer.out_proj :: (768, 768) :: bias=True

Row Attention Projections
=========================

layers.0.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.0.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.0.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.0.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.1.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.1.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.1.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.1.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.2.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.2.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.2.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.2.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.3.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.3.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.3.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.3.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.4.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.4.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.4.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.4.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.5.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.5.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.5.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.5.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.6.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.6.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.6.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.6.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.7.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.7.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.7.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.7.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.8.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.8.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.8.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.8.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.9.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.9.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.9.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.9.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.10.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.10.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.10.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.10.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.11.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.11.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.11.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.11.row_self_attention.layer.out_proj :: (768, 768) :: bias=True

Column Attention Projections
============================

layers.0.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.0.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.0.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.0.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.1.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.1.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.1.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.1.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.2.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.2.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.2.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.2.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.3.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.3.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.3.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.3.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.4.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.4.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.4.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.4.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.5.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.5.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.5.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.5.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.6.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.6.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.6.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.6.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.7.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.7.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.7.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.7.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.8.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.8.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.8.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.8.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.9.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.9.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.9.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.9.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.10.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.10.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.10.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.10.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.11.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.11.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.11.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.11.column_self_attention.layer.out_proj :: (768, 768) :: bias=True

All nn.Linear layers (for optional LoRA targets; truncated to 500)
==================================================================

layers.0.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.0.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.0.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.0.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.0.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.0.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.0.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.0.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.0.feed_forward_layer.layer.fc1 :: (3072, 768) :: bias=True
layers.0.feed_forward_layer.layer.fc2 :: (768, 3072) :: bias=True
layers.1.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.1.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.1.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.1.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.1.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.1.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.1.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.1.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.1.feed_forward_layer.layer.fc1 :: (3072, 768) :: bias=True
layers.1.feed_forward_layer.layer.fc2 :: (768, 3072) :: bias=True
layers.2.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.2.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.2.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.2.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.2.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.2.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.2.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.2.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.2.feed_forward_layer.layer.fc1 :: (3072, 768) :: bias=True
layers.2.feed_forward_layer.layer.fc2 :: (768, 3072) :: bias=True
layers.3.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.3.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.3.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.3.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.3.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.3.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.3.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.3.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.3.feed_forward_layer.layer.fc1 :: (3072, 768) :: bias=True
layers.3.feed_forward_layer.layer.fc2 :: (768, 3072) :: bias=True
layers.4.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.4.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.4.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.4.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.4.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.4.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.4.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.4.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.4.feed_forward_layer.layer.fc1 :: (3072, 768) :: bias=True
layers.4.feed_forward_layer.layer.fc2 :: (768, 3072) :: bias=True
layers.5.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.5.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.5.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.5.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.5.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.5.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.5.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.5.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.5.feed_forward_layer.layer.fc1 :: (3072, 768) :: bias=True
layers.5.feed_forward_layer.layer.fc2 :: (768, 3072) :: bias=True
layers.6.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.6.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.6.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.6.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.6.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.6.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.6.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.6.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.6.feed_forward_layer.layer.fc1 :: (3072, 768) :: bias=True
layers.6.feed_forward_layer.layer.fc2 :: (768, 3072) :: bias=True
layers.7.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.7.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.7.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.7.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.7.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.7.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.7.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.7.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.7.feed_forward_layer.layer.fc1 :: (3072, 768) :: bias=True
layers.7.feed_forward_layer.layer.fc2 :: (768, 3072) :: bias=True
layers.8.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.8.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.8.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.8.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.8.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.8.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.8.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.8.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.8.feed_forward_layer.layer.fc1 :: (3072, 768) :: bias=True
layers.8.feed_forward_layer.layer.fc2 :: (768, 3072) :: bias=True
layers.9.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.9.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.9.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.9.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.9.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.9.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.9.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.9.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.9.feed_forward_layer.layer.fc1 :: (3072, 768) :: bias=True
layers.9.feed_forward_layer.layer.fc2 :: (768, 3072) :: bias=True
layers.10.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.10.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.10.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.10.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.10.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.10.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.10.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.10.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.10.feed_forward_layer.layer.fc1 :: (3072, 768) :: bias=True
layers.10.feed_forward_layer.layer.fc2 :: (768, 3072) :: bias=True
layers.11.row_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.11.row_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.11.row_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.11.row_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.11.column_self_attention.layer.k_proj :: (768, 768) :: bias=True
layers.11.column_self_attention.layer.v_proj :: (768, 768) :: bias=True
layers.11.column_self_attention.layer.q_proj :: (768, 768) :: bias=True
layers.11.column_self_attention.layer.out_proj :: (768, 768) :: bias=True
layers.11.feed_forward_layer.layer.fc1 :: (3072, 768) :: bias=True
layers.11.feed_forward_layer.layer.fc2 :: (768, 3072) :: bias=True
contact_head.regression :: (1, 144) :: bias=True
lm_head.dense :: (768, 768) :: bias=True

Named parameters glimpse (q/k/v/out only; truncated)
====================================================

layers.0.row_self_attention.layer.k_proj.weight :: (768, 768) :: requires_grad=False
layers.0.row_self_attention.layer.k_proj.bias :: (768,) :: requires_grad=False
layers.0.row_self_attention.layer.v_proj.weight :: (768, 768) :: requires_grad=False
layers.0.row_self_attention.layer.v_proj.bias :: (768,) :: requires_grad=False
layers.0.row_self_attention.layer.q_proj.weight :: (768, 768) :: requires_grad=False
layers.0.row_self_attention.layer.q_proj.bias :: (768,) :: requires_grad=False
layers.0.row_self_attention.layer.out_proj.weight :: (768, 768) :: requires_grad=False
layers.0.row_self_attention.layer.out_proj.bias :: (768,) :: requires_grad=False
layers.0.column_self_attention.layer.k_proj.weight :: (768, 768) :: requires_grad=False
layers.0.column_self_attention.layer.k_proj.bias :: (768,) :: requires_grad=False
layers.0.column_self_attention.layer.v_proj.weight :: (768, 768) :: requires_grad=False
layers.0.column_self_attention.layer.v_proj.bias :: (768,) :: requires_grad=False
layers.0.column_self_attention.layer.q_proj.weight :: (768, 768) :: requires_grad=False
layers.0.column_self_attention.layer.q_proj.bias :: (768,) :: requires_grad=False
layers.0.column_self_attention.layer.out_proj.weight :: (768, 768) :: requires_grad=False
layers.0.column_self_attention.layer.out_proj.bias :: (768,) :: requires_grad=False
layers.1.row_self_attention.layer.k_proj.weight :: (768, 768) :: requires_grad=False
layers.1.row_self_attention.layer.k_proj.bias :: (768,) :: requires_grad=False
layers.1.row_self_attention.layer.v_proj.weight :: (768, 768) :: requires_grad=False
layers.1.row_self_attention.layer.v_proj.bias :: (768,) :: requires_grad=False
layers.1.row_self_attention.layer.q_proj.weight :: (768, 768) :: requires_grad=False
layers.1.row_self_attention.layer.q_proj.bias :: (768,) :: requires_grad=False
layers.1.row_self_attention.layer.out_proj.weight :: (768, 768) :: requires_grad=False
layers.1.row_self_attention.layer.out_proj.bias :: (768,) :: requires_grad=False
layers.1.column_self_attention.layer.k_proj.weight :: (768, 768) :: requires_grad=False
layers.1.column_self_attention.layer.k_proj.bias :: (768,) :: requires_grad=False
layers.1.column_self_attention.layer.v_proj.weight :: (768, 768) :: requires_grad=False
layers.1.column_self_attention.layer.v_proj.bias :: (768,) :: requires_grad=False
layers.1.column_self_attention.layer.q_proj.weight :: (768, 768) :: requires_grad=False
layers.1.column_self_attention.layer.q_proj.bias :: (768,) :: requires_grad=False
layers.1.column_self_attention.layer.out_proj.weight :: (768, 768) :: requires_grad=False
layers.1.column_self_attention.layer.out_proj.bias :: (768,) :: requires_grad=False
layers.2.row_self_attention.layer.k_proj.weight :: (768, 768) :: requires_grad=False
layers.2.row_self_attention.layer.k_proj.bias :: (768,) :: requires_grad=False
layers.2.row_self_attention.layer.v_proj.weight :: (768, 768) :: requires_grad=False
layers.2.row_self_attention.layer.v_proj.bias :: (768,) :: requires_grad=False
layers.2.row_self_attention.layer.q_proj.weight :: (768, 768) :: requires_grad=False
layers.2.row_self_attention.layer.q_proj.bias :: (768,) :: requires_grad=False
layers.2.row_self_attention.layer.out_proj.weight :: (768, 768) :: requires_grad=False
layers.2.row_self_attention.layer.out_proj.bias :: (768,) :: requires_grad=False
layers.2.column_self_attention.layer.k_proj.weight :: (768, 768) :: requires_grad=False
layers.2.column_self_attention.layer.k_proj.bias :: (768,) :: requires_grad=False
layers.2.column_self_attention.layer.v_proj.weight :: (768, 768) :: requires_grad=False
layers.2.column_self_attention.layer.v_proj.bias :: (768,) :: requires_grad=False
layers.2.column_self_attention.layer.q_proj.weight :: (768, 768) :: requires_grad=False
layers.2.column_self_attention.layer.q_proj.bias :: (768,) :: requires_grad=False
layers.2.column_self_attention.layer.out_proj.weight :: (768, 768) :: requires_grad=False
layers.2.column_self_attention.layer.out_proj.bias :: (768,) :: requires_grad=False
layers.3.row_self_attention.layer.k_proj.weight :: (768, 768) :: requires_grad=False
layers.3.row_self_attention.layer.k_proj.bias :: (768,) :: requires_grad=False
layers.3.row_self_attention.layer.v_proj.weight :: (768, 768) :: requires_grad=False
layers.3.row_self_attention.layer.v_proj.bias :: (768,) :: requires_grad=False
layers.3.row_self_attention.layer.q_proj.weight :: (768, 768) :: requires_grad=False
layers.3.row_self_attention.layer.q_proj.bias :: (768,) :: requires_grad=False
layers.3.row_self_attention.layer.out_proj.weight :: (768, 768) :: requires_grad=False
layers.3.row_self_attention.layer.out_proj.bias :: (768,) :: requires_grad=False
layers.3.column_self_attention.layer.k_proj.weight :: (768, 768) :: requires_grad=False
layers.3.column_self_attention.layer.k_proj.bias :: (768,) :: requires_grad=False
layers.3.column_self_attention.layer.v_proj.weight :: (768, 768) :: requires_grad=False
layers.3.column_self_attention.layer.v_proj.bias :: (768,) :: requires_grad=False
layers.3.column_self_attention.layer.q_proj.weight :: (768, 768) :: requires_grad=False
layers.3.column_self_attention.layer.q_proj.bias :: (768,) :: requires_grad=False
layers.3.column_self_attention.layer.out_proj.weight :: (768, 768) :: requires_grad=False
layers.3.column_self_attention.layer.out_proj.bias :: (768,) :: requires_grad=False
layers.4.row_self_attention.layer.k_proj.weight :: (768, 768) :: requires_grad=False
layers.4.row_self_attention.layer.k_proj.bias :: (768,) :: requires_grad=False
layers.4.row_self_attention.layer.v_proj.weight :: (768, 768) :: requires_grad=False
layers.4.row_self_attention.layer.v_proj.bias :: (768,) :: requires_grad=False
layers.4.row_self_attention.layer.q_proj.weight :: (768, 768) :: requires_grad=False
layers.4.row_self_attention.layer.q_proj.bias :: (768,) :: requires_grad=False
layers.4.row_self_attention.layer.out_proj.weight :: (768, 768) :: requires_grad=False
layers.4.row_self_attention.layer.out_proj.bias :: (768,) :: requires_grad=False
layers.4.column_self_attention.layer.k_proj.weight :: (768, 768) :: requires_grad=False
layers.4.column_self_attention.layer.k_proj.bias :: (768,) :: requires_grad=False
layers.4.column_self_attention.layer.v_proj.weight :: (768, 768) :: requires_grad=False
layers.4.column_self_attention.layer.v_proj.bias :: (768,) :: requires_grad=False
layers.4.column_self_attention.layer.q_proj.weight :: (768, 768) :: requires_grad=False
layers.4.column_self_attention.layer.q_proj.bias :: (768,) :: requires_grad=False
layers.4.column_self_attention.layer.out_proj.weight :: (768, 768) :: requires_grad=False
layers.4.column_self_attention.layer.out_proj.bias :: (768,) :: requires_grad=False
layers.5.row_self_attention.layer.k_proj.weight :: (768, 768) :: requires_grad=False
layers.5.row_self_attention.layer.k_proj.bias :: (768,) :: requires_grad=False
layers.5.row_self_attention.layer.v_proj.weight :: (768, 768) :: requires_grad=False
layers.5.row_self_attention.layer.v_proj.bias :: (768,) :: requires_grad=False
layers.5.row_self_attention.layer.q_proj.weight :: (768, 768) :: requires_grad=False
layers.5.row_self_attention.layer.q_proj.bias :: (768,) :: requires_grad=False
layers.5.row_self_attention.layer.out_proj.weight :: (768, 768) :: requires_grad=False
layers.5.row_self_attention.layer.out_proj.bias :: (768,) :: requires_grad=False
layers.5.column_self_attention.layer.k_proj.weight :: (768, 768) :: requires_grad=False
layers.5.column_self_attention.layer.k_proj.bias :: (768,) :: requires_grad=False
layers.5.column_self_attention.layer.v_proj.weight :: (768, 768) :: requires_grad=False
layers.5.column_self_attention.layer.v_proj.bias :: (768,) :: requires_grad=False
layers.5.column_self_attention.layer.q_proj.weight :: (768, 768) :: requires_grad=False
layers.5.column_self_attention.layer.q_proj.bias :: (768,) :: requires_grad=False
layers.5.column_self_attention.layer.out_proj.weight :: (768, 768) :: requires_grad=False
layers.5.column_self_attention.layer.out_proj.bias :: (768,) :: requires_grad=False
layers.6.row_self_attention.layer.k_proj.weight :: (768, 768) :: requires_grad=False
layers.6.row_self_attention.layer.k_proj.bias :: (768,) :: requires_grad=False
layers.6.row_self_attention.layer.v_proj.weight :: (768, 768) :: requires_grad=False
layers.6.row_self_attention.layer.v_proj.bias :: (768,) :: requires_grad=False
layers.6.row_self_attention.layer.q_proj.weight :: (768, 768) :: requires_grad=False
layers.6.row_self_attention.layer.q_proj.bias :: (768,) :: requires_grad=False
layers.6.row_self_attention.layer.out_proj.weight :: (768, 768) :: requires_grad=False
layers.6.row_self_attention.layer.out_proj.bias :: (768,) :: requires_grad=False
layers.6.column_self_attention.layer.k_proj.weight :: (768, 768) :: requires_grad=False
layers.6.column_self_attention.layer.k_proj.bias :: (768,) :: requires_grad=False
layers.6.column_self_attention.layer.v_proj.weight :: (768, 768) :: requires_grad=False
layers.6.column_self_attention.layer.v_proj.bias :: (768,) :: requires_grad=False
layers.6.column_self_attention.layer.q_proj.weight :: (768, 768) :: requires_grad=False
layers.6.column_self_attention.layer.q_proj.bias :: (768,) :: requires_grad=False
layers.6.column_self_attention.layer.out_proj.weight :: (768, 768) :: requires_grad=False
layers.6.column_self_attention.layer.out_proj.bias :: (768,) :: requires_grad=False
layers.7.row_self_attention.layer.k_proj.weight :: (768, 768) :: requires_grad=False
layers.7.row_self_attention.layer.k_proj.bias :: (768,) :: requires_grad=False
layers.7.row_self_attention.layer.v_proj.weight :: (768, 768) :: requires_grad=False
layers.7.row_self_attention.layer.v_proj.bias :: (768,) :: requires_grad=False
layers.7.row_self_attention.layer.q_proj.weight :: (768, 768) :: requires_grad=False
layers.7.row_self_attention.layer.q_proj.bias :: (768,) :: requires_grad=False
layers.7.row_self_attention.layer.out_proj.weight :: (768, 768) :: requires_grad=False
layers.7.row_self_attention.layer.out_proj.bias :: (768,) :: requires_grad=False
layers.7.column_self_attention.layer.k_proj.weight :: (768, 768) :: requires_grad=False
layers.7.column_self_attention.layer.k_proj.bias :: (768,) :: requires_grad=False
layers.7.column_self_attention.layer.v_proj.weight :: (768, 768) :: requires_grad=False
layers.7.column_self_attention.layer.v_proj.bias :: (768,) :: requires_grad=False
layers.7.column_self_attention.layer.q_proj.weight :: (768, 768) :: requires_grad=False
layers.7.column_self_attention.layer.q_proj.bias :: (768,) :: requires_grad=False
layers.7.column_self_attention.layer.out_proj.weight :: (768, 768) :: requires_grad=False
layers.7.column_self_attention.layer.out_proj.bias :: (768,) :: requires_grad=False
layers.8.row_self_attention.layer.k_proj.weight :: (768, 768) :: requires_grad=False
layers.8.row_self_attention.layer.k_proj.bias :: (768,) :: requires_grad=False
layers.8.row_self_attention.layer.v_proj.weight :: (768, 768) :: requires_grad=False
layers.8.row_self_attention.layer.v_proj.bias :: (768,) :: requires_grad=False
layers.8.row_self_attention.layer.q_proj.weight :: (768, 768) :: requires_grad=False
layers.8.row_self_attention.layer.q_proj.bias :: (768,) :: requires_grad=False
layers.8.row_self_attention.layer.out_proj.weight :: (768, 768) :: requires_grad=False
layers.8.row_self_attention.layer.out_proj.bias :: (768,) :: requires_grad=False
layers.8.column_self_attention.layer.k_proj.weight :: (768, 768) :: requires_grad=False
layers.8.column_self_attention.layer.k_proj.bias :: (768,) :: requires_grad=False
layers.8.column_self_attention.layer.v_proj.weight :: (768, 768) :: requires_grad=False
layers.8.column_self_attention.layer.v_proj.bias :: (768,) :: requires_grad=False
layers.8.column_self_attention.layer.q_proj.weight :: (768, 768) :: requires_grad=False
layers.8.column_self_attention.layer.q_proj.bias :: (768,) :: requires_grad=False
layers.8.column_self_attention.layer.out_proj.weight :: (768, 768) :: requires_grad=False
layers.8.column_self_attention.layer.out_proj.bias :: (768,) :: requires_grad=False
layers.9.row_self_attention.layer.k_proj.weight :: (768, 768) :: requires_grad=False
layers.9.row_self_attention.layer.k_proj.bias :: (768,) :: requires_grad=False
layers.9.row_self_attention.layer.v_proj.weight :: (768, 768) :: requires_grad=False
layers.9.row_self_attention.layer.v_proj.bias :: (768,) :: requires_grad=False
layers.9.row_self_attention.layer.q_proj.weight :: (768, 768) :: requires_grad=False
layers.9.row_self_attention.layer.q_proj.bias :: (768,) :: requires_grad=False
layers.9.row_self_attention.layer.out_proj.weight :: (768, 768) :: requires_grad=False
layers.9.row_self_attention.layer.out_proj.bias :: (768,) :: requires_grad=False
layers.9.column_self_attention.layer.k_proj.weight :: (768, 768) :: requires_grad=False
layers.9.column_self_attention.layer.k_proj.bias :: (768,) :: requires_grad=False
layers.9.column_self_attention.layer.v_proj.weight :: (768, 768) :: requires_grad=False
layers.9.column_self_attention.layer.v_proj.bias :: (768,) :: requires_grad=False
layers.9.column_self_attention.layer.q_proj.weight :: (768, 768) :: requires_grad=False
layers.9.column_self_attention.layer.q_proj.bias :: (768,) :: requires_grad=False
layers.9.column_self_attention.layer.out_proj.weight :: (768, 768) :: requires_grad=False
layers.9.column_self_attention.layer.out_proj.bias :: (768,) :: requires_grad=False
layers.10.row_self_attention.layer.k_proj.weight :: (768, 768) :: requires_grad=False
layers.10.row_self_attention.layer.k_proj.bias :: (768,) :: requires_grad=False
layers.10.row_self_attention.layer.v_proj.weight :: (768, 768) :: requires_grad=False
layers.10.row_self_attention.layer.v_proj.bias :: (768,) :: requires_grad=False
layers.10.row_self_attention.layer.q_proj.weight :: (768, 768) :: requires_grad=False
layers.10.row_self_attention.layer.q_proj.bias :: (768,) :: requires_grad=False
layers.10.row_self_attention.layer.out_proj.weight :: (768, 768) :: requires_grad=False
layers.10.row_self_attention.layer.out_proj.bias :: (768,) :: requires_grad=False
layers.10.column_self_attention.layer.k_proj.weight :: (768, 768) :: requires_grad=False
layers.10.column_self_attention.layer.k_proj.bias :: (768,) :: requires_grad=False
layers.10.column_self_attention.layer.v_proj.weight :: (768, 768) :: requires_grad=False
layers.10.column_self_attention.layer.v_proj.bias :: (768,) :: requires_grad=False
layers.10.column_self_attention.layer.q_proj.weight :: (768, 768) :: requires_grad=False
layers.10.column_self_attention.layer.q_proj.bias :: (768,) :: requires_grad=False
layers.10.column_self_attention.layer.out_proj.weight :: (768, 768) :: requires_grad=False
layers.10.column_self_attention.layer.out_proj.bias :: (768,) :: requires_grad=False
layers.11.row_self_attention.layer.k_proj.weight :: (768, 768) :: requires_grad=False
layers.11.row_self_attention.layer.k_proj.bias :: (768,) :: requires_grad=False
layers.11.row_self_attention.layer.v_proj.weight :: (768, 768) :: requires_grad=False
layers.11.row_self_attention.layer.v_proj.bias :: (768,) :: requires_grad=False
layers.11.row_self_attention.layer.q_proj.weight :: (768, 768) :: requires_grad=False
layers.11.row_self_attention.layer.q_proj.bias :: (768,) :: requires_grad=False
layers.11.row_self_attention.layer.out_proj.weight :: (768, 768) :: requires_grad=False
layers.11.row_self_attention.layer.out_proj.bias :: (768,) :: requires_grad=False
layers.11.column_self_attention.layer.k_proj.weight :: (768, 768) :: requires_grad=False
layers.11.column_self_attention.layer.k_proj.bias :: (768,) :: requires_grad=False
layers.11.column_self_attention.layer.v_proj.weight :: (768, 768) :: requires_grad=False
layers.11.column_self_attention.layer.v_proj.bias :: (768,) :: requires_grad=False
layers.11.column_self_attention.layer.q_proj.weight :: (768, 768) :: requires_grad=False
layers.11.column_self_attention.layer.q_proj.bias :: (768,) :: requires_grad=False
layers.11.column_self_attention.layer.out_proj.weight :: (768, 768) :: requires_grad=False
layers.11.column_self_attention.layer.out_proj.bias :: (768,) :: requires_grad=False