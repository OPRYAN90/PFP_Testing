{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train → 29893 IDs  (sample keys: ['154L-A', '155C-A', '16PK-A'])\n",
      "val   →  3322 IDs  (sample keys: ['192L-A', '1A0A-A', '1A21-A'])\n",
      "test  →  3414 IDs  (sample keys: ['11AS-A', '18GS-A', '1A0P-A'])\n",
      "\n",
      "Total chains across splits: 36,629\n",
      "  (should be train + val + test)\n",
      "Case-insensitive duplicate IDs: 233\n",
      "IDs that differ only by case: 233\n",
      "Duplicate IDs across splits? 0\n",
      "\n",
      "First 10 test-chain IDs:\n",
      "  11AS-A\n",
      "  18GS-A\n",
      "  1A0P-A\n",
      "  1A22-A\n",
      "  1A4E-A\n",
      "  1A6F-A\n",
      "  1A6J-A\n",
      "  1A8Y-A\n",
      "  1A9C-A\n",
      "  1A9W-E\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "import itertools\n",
    "\n",
    "root = Path(r\"C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\")\n",
    "split_files = {\n",
    "    \"train\": root / \"train_pdbch.pt\",\n",
    "    \"val\"  : root / \"val_pdbch.pt\",\n",
    "    \"test\" : root / \"test_pdbch.pt\",\n",
    "}\n",
    "\n",
    "# 1) Load and unwrap\n",
    "splits = {}\n",
    "for name, path in split_files.items():\n",
    "    obj = torch.load(path, map_location=\"cpu\")\n",
    "    # if it’s a 1-key dict, pull out the sole value\n",
    "    if isinstance(obj, dict) and len(obj)==1:\n",
    "        key = next(iter(obj))\n",
    "        ids = obj[key]\n",
    "    else:\n",
    "        ids = obj\n",
    "    # ensure it’s a flat list of strings\n",
    "    assert isinstance(ids, (list, tuple))\n",
    "    splits[name] = list(ids)\n",
    "    print(f\"{name:5s} → {len(ids):5d} IDs  (sample keys: {list(ids)[:3]})\")\n",
    "\n",
    "# 2) Concatenate and basic stats\n",
    "all_ids = list(itertools.chain.from_iterable(splits.values()))\n",
    "print(f\"\\nTotal chains across splits: {len(all_ids):,}\")\n",
    "print(\"  (should be train + val + test)\")\n",
    "# This duplicate check is case-sensitive\n",
    "dupes = len(all_ids) - len(set(all_ids))\n",
    "# 3) Check for duplicates\n",
    "# Case-insensitive duplicate check\n",
    "all_ids_lower = [id.lower() for id in all_ids]\n",
    "case_insensitive_dupes = len(all_ids_lower) - len(set(all_ids_lower))\n",
    "print(f\"Case-insensitive duplicate IDs: {case_insensitive_dupes}\")\n",
    "\n",
    "# Also check for case variations of the same ID\n",
    "case_variations = len(set(all_ids)) - len(set(all_ids_lower))\n",
    "print(f\"IDs that differ only by case: {case_variations}\")\n",
    "print(f\"Duplicate IDs across splits? {dupes}\")\n",
    "\n",
    "# 4) Peek at first 10 test IDs\n",
    "print(\"\\nFirst 10 test-chain IDs:\")\n",
    "for cid in splits[\"test\"][:10]:\n",
    "    print(\" \", cid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧬 Creating PDBCH Folder Structure with Tilde Encoding\n",
      "============================================================\n",
      "📁 Created root directory: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\PDBCH\n",
      "\n",
      "🔄 Processing train split...\n",
      "   📋 Loaded 29,893 protein IDs\n",
      "   📁 Created split folder: train_pdbch\n",
      "   ✅ Created 29,893 unique folders\n",
      "   🔀 Applied tilde encoding to 675 lowercase IDs\n",
      "\n",
      "🔄 Processing val split...\n",
      "   📋 Loaded 3,322 protein IDs\n",
      "   📁 Created split folder: val_pdbch\n",
      "   ✅ Created 3,322 unique folders\n",
      "   🔀 Applied tilde encoding to 80 lowercase IDs\n",
      "\n",
      "🔄 Processing test split...\n",
      "   📋 Loaded 3,414 protein IDs\n",
      "   📁 Created split folder: test_pdbch\n",
      "   ✅ Created 3,414 unique folders\n",
      "   🔀 Applied tilde encoding to 137 lowercase IDs\n",
      "\n",
      "🔍 VERIFICATION RESULTS\n",
      "============================================================\n",
      "TRAIN: 29,893 folders | 29,893 proteins | ✅ SUCCESS\n",
      "      (Including 675 tilde-encoded for case conflicts)\n",
      "VAL  : 3,322 folders | 3,322 proteins | ✅ SUCCESS\n",
      "      (Including 80 tilde-encoded for case conflicts)\n",
      "TEST : 3,414 folders | 3,414 proteins | ✅ SUCCESS\n",
      "      (Including 137 tilde-encoded for case conflicts)\n",
      "\n",
      "📋 SAMPLE ENCODED FOLDER NAMES\n",
      "============================================================\n",
      "TRAIN:\n",
      "  154L-A       → 154L-A          (unchanged)\n",
      "  155C-A       → 155C-A          (unchanged)\n",
      "  16PK-A       → 16PK-A          (unchanged)\n",
      "  16VP-A       → 16VP-A          (unchanged)\n",
      "  1914-A       → 1914-A          (unchanged)\n",
      "VAL:\n",
      "  192L-A       → 192L-A          (unchanged)\n",
      "  1A0A-A       → 1A0A-A          (unchanged)\n",
      "  1A21-A       → 1A21-A          (unchanged)\n",
      "  1A27-A       → 1A27-A          (unchanged)\n",
      "  1A3Q-A       → 1A3Q-A          (unchanged)\n",
      "TEST:\n",
      "  11AS-A       → 11AS-A          (unchanged)\n",
      "  18GS-A       → 18GS-A          (unchanged)\n",
      "  1A0P-A       → 1A0P-A          (unchanged)\n",
      "  1A22-A       → 1A22-A          (unchanged)\n",
      "  1A4E-A       → 1A4E-A          (unchanged)\n",
      "\n",
      "🎉 PDBCH folder structure created successfully!\n",
      "📍 Location: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\PDBCH\n",
      "📁 Structure:\n",
      "   PDBCH/\n",
      "   ├── train_pdbch/ (29,893 protein folders)\n",
      "   ├── val_pdbch/ (3,322 protein folders)\n",
      "   ├── test_pdbch/ (3,414 protein folders)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "def encode_protein_id(protein_id):\n",
    "    \"\"\"Convert to Windows-safe: uppercase preserved, lowercase gets tilde after\"\"\"\n",
    "    return ''.join(c + '~' if c.islower() else c for c in protein_id)\n",
    "\n",
    "def decode_protein_id(encoded_name):\n",
    "    \"\"\"Restore original by removing tildes\"\"\"\n",
    "    return encoded_name.replace('~', '')\n",
    "\n",
    "# ── CONFIGURATION ──────────────────────────────────────────────────────────────\n",
    "# Source: HEAL_PDB split files\n",
    "source_root = Path(r\"C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\")\n",
    "split_files = {\n",
    "    \"train\": source_root / \"train_pdbch.pt\",\n",
    "    \"val\": source_root / \"val_pdbch.pt\", \n",
    "    \"test\": source_root / \"test_pdbch.pt\",\n",
    "}\n",
    "\n",
    "# Destination: New PDBCH folder structure\n",
    "dest_root = Path(r\"C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\PDBCH\")\n",
    "folder_names = {\n",
    "    \"train\": \"train_pdbch\",\n",
    "    \"val\": \"val_pdbch\",\n",
    "    \"test\": \"test_pdbch\"\n",
    "}\n",
    "\n",
    "print(\"🧬 Creating PDBCH Folder Structure with Tilde Encoding\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ── 1. CREATE ROOT DIRECTORY ──────────────────────────────────────────────────\n",
    "dest_root.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"📁 Created root directory: {dest_root}\")\n",
    "\n",
    "# ── 2. LOAD SPLITS & CREATE FOLDERS ───────────────────────────────────────────\n",
    "split_stats = {}\n",
    "\n",
    "for split_name, source_file in split_files.items():\n",
    "    print(f\"\\n🔄 Processing {split_name} split...\")\n",
    "    \n",
    "    # Load protein IDs from .pt file\n",
    "    obj = torch.load(source_file, map_location=\"cpu\")\n",
    "    if isinstance(obj, dict) and len(obj) == 1:\n",
    "        # Unwrap single-key dict\n",
    "        protein_ids = next(iter(obj.values()))\n",
    "    else:\n",
    "        protein_ids = obj\n",
    "    \n",
    "    protein_ids = list(protein_ids)\n",
    "    print(f\"   📋 Loaded {len(protein_ids):,} protein IDs\")\n",
    "    \n",
    "    # Create split folder\n",
    "    split_folder = dest_root / folder_names[split_name]\n",
    "    split_folder.mkdir(exist_ok=True)\n",
    "    print(f\"   📁 Created split folder: {split_folder.name}\")\n",
    "    \n",
    "    # Create individual protein folders with tilde encoding\n",
    "    created_count = 0\n",
    "    case_conflicts = 0\n",
    "    \n",
    "    for protein_id in protein_ids:\n",
    "        # Apply tilde encoding for Windows case-safety\n",
    "        safe_name = encode_protein_id(protein_id)\n",
    "        protein_folder = split_folder / safe_name\n",
    "        \n",
    "        # Track if this is a case-conflict resolution\n",
    "        if safe_name != protein_id:\n",
    "            case_conflicts += 1\n",
    "        \n",
    "        # Create empty folder\n",
    "        if not protein_folder.exists():\n",
    "            protein_folder.mkdir(exist_ok=True)\n",
    "            created_count += 1\n",
    "        \n",
    "        # Save original ID for reference\n",
    "        original_id_file = protein_folder / \"original_id.txt\"\n",
    "        with open(original_id_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(protein_id)\n",
    "    \n",
    "    print(f\"   ✅ Created {created_count:,} unique folders\")\n",
    "    if case_conflicts > 0:\n",
    "        print(f\"   🔀 Applied tilde encoding to {case_conflicts:,} lowercase IDs\")\n",
    "    \n",
    "    split_stats[split_name] = {\n",
    "        'protein_ids': len(protein_ids),\n",
    "        'folders_created': created_count,\n",
    "        'case_conflicts': case_conflicts\n",
    "    }\n",
    "\n",
    "# ── 3. VERIFICATION ────────────────────────────────────────────────────────────\n",
    "print(f\"\\n🔍 VERIFICATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for split_name in split_files.keys():\n",
    "    split_folder = dest_root / folder_names[split_name]\n",
    "    \n",
    "    # Count actual subfolders\n",
    "    actual_folders = len([d for d in split_folder.iterdir() if d.is_dir()])\n",
    "    expected_folders = split_stats[split_name]['protein_ids']\n",
    "    case_conflicts = split_stats[split_name]['case_conflicts']\n",
    "    \n",
    "    status = \"✅ SUCCESS\" if actual_folders == expected_folders else \"❌ MISMATCH\"\n",
    "    \n",
    "    print(f\"{split_name.upper():5s}: {actual_folders:,} folders | {expected_folders:,} proteins | {status}\")\n",
    "    if case_conflicts > 0:\n",
    "        print(f\"      (Including {case_conflicts:,} tilde-encoded for case conflicts)\")\n",
    "\n",
    "# ── 4. SAMPLE FOLDER NAMES ────────────────────────────────────────────────────\n",
    "print(f\"\\n📋 SAMPLE ENCODED FOLDER NAMES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for split_name in [\"train\", \"val\", \"test\"]:\n",
    "    split_folder = dest_root / folder_names[split_name]\n",
    "    sample_folders = sorted([d.name for d in split_folder.iterdir() if d.is_dir()])[:5]\n",
    "    \n",
    "    print(f\"{split_name.upper()}:\")\n",
    "    for folder_name in sample_folders:\n",
    "        original_id = decode_protein_id(folder_name)\n",
    "        if folder_name != original_id:\n",
    "            print(f\"  {original_id:12s} → {folder_name:15s} (tilde encoded)\")\n",
    "        else:\n",
    "            print(f\"  {original_id:12s} → {folder_name:15s} (unchanged)\")\n",
    "\n",
    "print(f\"\\n🎉 PDBCH folder structure created successfully!\")\n",
    "print(f\"📍 Location: {dest_root}\")\n",
    "print(f\"📁 Structure:\")\n",
    "print(f\"   {dest_root.name}/\")\n",
    "for folder_name in folder_names.values():\n",
    "    folder_path = dest_root / folder_name\n",
    "    folder_count = len([d for d in folder_path.iterdir() if d.is_dir()])\n",
    "    print(f\"   ├── {folder_name}/ ({folder_count:,} protein folders)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DUPLICATE ANALYSIS ===\n",
      "\n",
      "TRAIN:\n",
      "  Total IDs: 29893\n",
      "  Unique IDs: 29893\n",
      "  Duplicates: 0\n",
      "\n",
      "VAL:\n",
      "  Total IDs: 3322\n",
      "  Unique IDs: 3322\n",
      "  Duplicates: 0\n",
      "\n",
      "TEST:\n",
      "  Total IDs: 3414\n",
      "  Unique IDs: 3414\n",
      "  Duplicates: 0\n",
      "\n",
      "=== CASE SENSITIVITY ANALYSIS ===\n",
      "\n",
      "TRAIN:\n",
      "  Case variations: 155\n",
      "  IDs with case variations: 155\n",
      "    ['5IT7-pp', '5IT7-PP']\n",
      "    ['6GCS-j', '6GCS-J']\n",
      "    ['5AJ4-Ap', '5AJ4-AP']\n",
      "\n",
      "VAL:\n",
      "  Case variations: 4\n",
      "  IDs with case variations: 4\n",
      "    ['5IT7-jj', '5IT7-JJ']\n",
      "    ['4V8M-Bm', '4V8M-BM']\n",
      "    ['3J9M-l', '3J9M-L']\n",
      "\n",
      "TEST:\n",
      "  Case variations: 16\n",
      "  IDs with case variations: 16\n",
      "    ['6GIQ-D', '6GIQ-d']\n",
      "    ['4V6W-CO', '4V6W-Co']\n",
      "    ['4V6W-CB', '4V6W-Cb']\n",
      "\n",
      "=== INVALID FILENAME CHARACTER ANALYSIS ===\n",
      "\n",
      "TRAIN:\n",
      "  IDs with invalid filename chars: 0\n",
      "\n",
      "VAL:\n",
      "  IDs with invalid filename chars: 0\n",
      "\n",
      "TEST:\n",
      "  IDs with invalid filename chars: 0\n",
      "\n",
      "=== PREDICTED FOLDER COUNTS ===\n",
      "train: 29893 IDs → ~29738 predicted folders\n",
      "val: 3322 IDs → ~3318 predicted folders\n",
      "test: 3414 IDs → ~3398 predicted folders\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Load your splits\n",
    "root = Path(r\"C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\")\n",
    "split_files = {\n",
    "    \"train\": root / \"train_pdbch.pt\",\n",
    "    \"val\": root / \"val_pdbch.pt\", \n",
    "    \"test\": root / \"test_pdbch.pt\",\n",
    "}\n",
    "\n",
    "splits = {}\n",
    "for name, path in split_files.items():\n",
    "    obj = torch.load(path, map_location=\"cpu\")\n",
    "    if isinstance(obj, dict) and len(obj) == 1:\n",
    "        obj = next(iter(obj.values()))\n",
    "    splits[name] = list(obj)\n",
    "\n",
    "print(\"=== DUPLICATE ANALYSIS ===\")\n",
    "for split_name, ids in splits.items():\n",
    "    print(f\"\\n{split_name.upper()}:\")\n",
    "    print(f\"  Total IDs: {len(ids)}\")\n",
    "    print(f\"  Unique IDs: {len(set(ids))}\")\n",
    "    print(f\"  Duplicates: {len(ids) - len(set(ids))}\")\n",
    "    \n",
    "    # Find the actual duplicates\n",
    "    counts = Counter(ids)\n",
    "    duplicates = {k: v for k, v in counts.items() if v > 1}\n",
    "    if duplicates:\n",
    "        print(f\"  Duplicate IDs found: {len(duplicates)}\")\n",
    "        for dup_id, count in list(duplicates.items())[:5]:  # Show first 5\n",
    "            print(f\"    '{dup_id}': appears {count} times\")\n",
    "        if len(duplicates) > 5:\n",
    "            print(f\"    ... and {len(duplicates) - 5} more\")\n",
    "\n",
    "print(\"\\n=== CASE SENSITIVITY ANALYSIS ===\")\n",
    "for split_name, ids in splits.items():\n",
    "    print(f\"\\n{split_name.upper()}:\")\n",
    "    unique_original = len(set(ids))\n",
    "    unique_lowercase = len(set(id.lower() for id in ids))\n",
    "    case_variations = unique_original - unique_lowercase\n",
    "    print(f\"  Case variations: {case_variations}\")\n",
    "    \n",
    "    if case_variations > 0:\n",
    "        # Find examples of case variations\n",
    "        by_lower = {}\n",
    "        for id in set(ids):\n",
    "            key = id.lower()\n",
    "            if key not in by_lower:\n",
    "                by_lower[key] = []\n",
    "            by_lower[key].append(id)\n",
    "        \n",
    "        variations = {k: v for k, v in by_lower.items() if len(v) > 1}\n",
    "        print(f\"  IDs with case variations: {len(variations)}\")\n",
    "        for lower_key, variants in list(variations.items())[:3]:  # Show first 3\n",
    "            print(f\"    {variants}\")\n",
    "\n",
    "print(\"\\n=== INVALID FILENAME CHARACTER ANALYSIS ===\")\n",
    "# Windows invalid characters: < > : \" | ? * and control chars (0-31)\n",
    "invalid_chars = set('<>:\"|?*') | set(chr(i) for i in range(32))\n",
    "\n",
    "for split_name, ids in splits.items():\n",
    "    print(f\"\\n{split_name.upper()}:\")\n",
    "    invalid_ids = []\n",
    "    for id in set(ids):\n",
    "        if any(char in invalid_chars for char in id):\n",
    "            invalid_ids.append(id)\n",
    "        # Check for trailing dots or spaces (also invalid on Windows)\n",
    "        if id.endswith('.') or id.endswith(' '):\n",
    "            invalid_ids.append(id)\n",
    "    \n",
    "    print(f\"  IDs with invalid filename chars: {len(invalid_ids)}\")\n",
    "    if invalid_ids:\n",
    "        for invalid_id in invalid_ids[:5]:  # Show first 5\n",
    "            invalid_chars_found = [c for c in invalid_id if c in invalid_chars]\n",
    "            print(f\"    '{invalid_id}' (chars: {invalid_chars_found})\")\n",
    "\n",
    "print(\"\\n=== PREDICTED FOLDER COUNTS ===\")\n",
    "for split_name, ids in splits.items():\n",
    "    # Simulate Windows filesystem behavior\n",
    "    unique_folders = set()\n",
    "    for id in ids:\n",
    "        # Convert to lowercase (Windows case-insensitive)\n",
    "        folder_name = id.lower()\n",
    "        # Remove invalid characters (simplified - just replace with underscore)\n",
    "        folder_name = re.sub(r'[<>:\"|?*\\x00-\\x1f]', '_', folder_name)\n",
    "        # Handle trailing dots/spaces\n",
    "        folder_name = folder_name.rstrip('. ')\n",
    "        unique_folders.add(folder_name)\n",
    "    \n",
    "    print(f\"{split_name}: {len(ids)} IDs → ~{len(unique_folders)} predicted folders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 29738 subfolders\n",
      "test :  3398 subfolders\n",
      "val  :  3318 subfolders\n",
      "\n",
      "Total: 36454 subfolders\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Base path\n",
    "base_path = Path(r\"C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\")\n",
    "\n",
    "# Check each split\n",
    "splits = [\"train\", \"test\", \"val\"]\n",
    "\n",
    "total_subfolders = 0\n",
    "\n",
    "for split in splits:\n",
    "    split_folder = base_path / f\"protein_{split}_pdb\"\n",
    "    \n",
    "    if split_folder.exists():\n",
    "        # Count subfolders only\n",
    "        subfolders = [p for p in split_folder.iterdir() if p.is_dir()]\n",
    "        print(f\"{split:5s}: {len(subfolders):5d} subfolders\")\n",
    "        total_subfolders += len(subfolders)\n",
    "    else:\n",
    "        print(f\"{split:5s}: Folder not found\")\n",
    "\n",
    "print(f\"\\nTotal: {total_subfolders:5d} subfolders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train :  29893 IDs\n",
      "val   :   3322 IDs\n",
      "test  :   3414 IDs\n",
      "Loaded GO terms for 36,629 / 36,629 chains\n",
      "✓ Folder hierarchy populated\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Build per-protein GO-term folders for the PDBch split\n",
    "----------------------------------------------------\n",
    "\n",
    "Root/\n",
    "├── train_pdbch.pt          ← HEAL ID list (dict or list)\n",
    "├── val_pdbch.pt\n",
    "├── test_pdbch.pt\n",
    "├── pdbch_go.tsv            ← your TSV with three ontology columns\n",
    "└── (generated)\n",
    "    ├── protein_train_pdb/\n",
    "    │   └── 154L-A/\n",
    "    │       ├── mf_go.txt\n",
    "    │       ├── bp_go.txt\n",
    "    │       └── cc_go.txt\n",
    "    ├── protein_val_pdb/\n",
    "    └── protein_test_pdb/\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import torch, csv, os\n",
    "\n",
    "# ── CONFIG ───────────────────────────────────────────────────────────────────\n",
    "ROOT = Path(r\"C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\")  # adjust\n",
    "GO_TSV = ROOT / \"nrPDB-GO_2019.06.18_annot.tsv\"                                       # adjust\n",
    "SPLIT_PTS = {\n",
    "    \"train\": ROOT / \"train_pdbch.pt\", \n",
    "    \"val\"  : ROOT / \"val_pdbch.pt\",\n",
    "    \"test\" : ROOT / \"test_pdbch.pt\",\n",
    "}\n",
    "OUT_TPL = ROOT / \"protein_{split}_pdb\"                               # parent dirs\n",
    "TXT_NAMES = {\"mf\": \"mf_go.txt\", \"bp\": \"bp_go.txt\", \"cc\": \"cc_go.txt\"}\n",
    "\n",
    "# ── 1. LOAD SPLIT ID LISTS ───────────────────────────────────────────────────\n",
    "split_ids = {}\n",
    "for split, fpath in SPLIT_PTS.items():\n",
    "    obj = torch.load(fpath, map_location=\"cpu\")\n",
    "    if isinstance(obj, dict) and len(obj) == 1:      # unwrap 1-key dict\n",
    "        obj = next(iter(obj.values()))\n",
    "    split_ids[split] = list(obj)\n",
    "    print(f\"{split:5s} : {len(split_ids[split]):6d} IDs\")\n",
    "\n",
    "# ── 2. BUILD LOOK-UP FROM CHAIN → {mf:[], bp:[], cc:[]} ──────────────────────\n",
    "lookup = {cid: {\"mf\": [], \"bp\": [], \"cc\": []} for cid in\n",
    "          split_ids[\"train\"] + split_ids[\"val\"] + split_ids[\"test\"]}\n",
    "\n",
    "with GO_TSV.open(newline='', encoding=\"utf-8\") as fh:\n",
    "    tsv = csv.reader(fh, delimiter='\\t')\n",
    "    for row in tsv:\n",
    "        if not row or row[0].startswith('#'):\n",
    "            continue                         # skip comments / headers\n",
    "        chain = row[0].strip()\n",
    "        if chain not in lookup:\n",
    "            continue                         # GO line not relevant to PDBch\n",
    "        mf = [g.strip() for g in row[1].split(',') if g.strip()]\n",
    "        bp = [g.strip() for g in row[2].split(',') if g.strip()] if len(row) > 2 else []\n",
    "        cc = [g.strip() for g in row[3].split(',') if g.strip()] if len(row) > 3 else []\n",
    "        lookup[chain] = {\"mf\": mf, \"bp\": bp, \"cc\": cc}\n",
    "\n",
    "print(f\"Loaded GO terms for {sum(bool(v['mf'] or v['bp'] or v['cc'])\n",
    "                                for v in lookup.values()):,} / {len(lookup):,} chains\")\n",
    "\n",
    "# ── 3. CREATE FOLDERS & WRITE FILES ──────────────────────────────────────────\n",
    "for split, cids in split_ids.items():\n",
    "    split_dir = OUT_TPL.with_name(OUT_TPL.name.format(split=split))\n",
    "    split_dir.mkdir(parents=True, exist_ok=True)\n",
    "    for cid in cids:\n",
    "        cid_dir = split_dir / cid\n",
    "        cid_dir.mkdir(exist_ok=True)\n",
    "        terms = lookup[cid]\n",
    "        for ont, fname in TXT_NAMES.items():\n",
    "            with open(cid_dir / fname, 'w', encoding='utf-8') as fh:\n",
    "                fh.write('\\n'.join(terms[ont]))\n",
    "print(\"✓ Folder hierarchy populated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧬 Populating PDBCH Folders with GO Terms (Case-Sensitive)\n",
      "======================================================================\n",
      "📖 Loading GO annotations from TSV...\n",
      "✅ Loaded GO annotations for 36,647 proteins from TSV\n",
      "\n",
      "🔄 Processing train split...\n",
      "   📁 Found 29,893 protein folders\n",
      "   ✅ Processed: 29,893 proteins\n",
      "   📊 Found GO terms: 29,893\n",
      "   ❓ Missing GO terms: 0\n",
      "   📝 Files written: 89,679\n",
      "\n",
      "🔄 Processing val split...\n",
      "   📁 Found 3,322 protein folders\n",
      "   ✅ Processed: 3,322 proteins\n",
      "   📊 Found GO terms: 3,322\n",
      "   ❓ Missing GO terms: 0\n",
      "   📝 Files written: 9,966\n",
      "\n",
      "🔄 Processing test split...\n",
      "   📁 Found 3,414 protein folders\n",
      "   ✅ Processed: 3,414 proteins\n",
      "   📊 Found GO terms: 3,414\n",
      "   ❓ Missing GO terms: 0\n",
      "   📝 Files written: 10,242\n",
      "\n",
      "🎉 FINAL RESULTS\n",
      "======================================================================\n",
      "📊 Total proteins processed: 36,629\n",
      "✅ Proteins with GO terms: 36,629\n",
      "❓ Proteins without GO terms: 0\n",
      "📝 Total GO files written: 109,887\n",
      "📈 GO term coverage: 100.0%\n",
      "\n",
      "🔍 SAMPLE VERIFICATION\n",
      "======================================================================\n",
      "Sample from train:\n",
      "  154L-A       → 154L-A          | ✅ WITH GO\n",
      "  155C-A       → 155C-A          | ✅ WITH GO\n",
      "  16PK-A       → 16PK-A          | ✅ WITH GO\n",
      "\n",
      "🎯 CRITICAL CHANGES MADE:\n",
      "1. ✅ Decoded tilde-encoded folder names to get original protein IDs\n",
      "2. ✅ Used case-sensitive lookup in TSV file (no case conversion)\n",
      "3. ✅ Wrote GO term files back to tilde-encoded folders\n",
      "4. ✅ Created empty files for proteins without GO annotations\n",
      "5. ✅ Preserved exact case sensitivity throughout the process\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "def decode_protein_id(encoded_name):\n",
    "    \"\"\"Restore original protein ID by removing tildes\"\"\"\n",
    "    return encoded_name.replace('~', '')\n",
    "\n",
    "def encode_protein_id(protein_id):\n",
    "    \"\"\"Convert to Windows-safe: uppercase preserved, lowercase gets tilde after\"\"\"\n",
    "    return ''.join(c + '~' if c.islower() else c for c in protein_id)\n",
    "\n",
    "# ── CONFIGURATION ──────────────────────────────────────────────────────────────\n",
    "# Source: TSV file with GO annotations\n",
    "GO_TSV = Path(r\"C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\nrPDB-GO_2019.06.18_annot.tsv\")\n",
    "\n",
    "# Destination: New PDBCH folder structure\n",
    "PDBCH_ROOT = Path(r\"C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\PDBCH\")\n",
    "SPLIT_FOLDERS = {\n",
    "    \"train\": PDBCH_ROOT / \"train_pdbch\",\n",
    "    \"val\": PDBCH_ROOT / \"val_pdbch\", \n",
    "    \"test\": PDBCH_ROOT / \"test_pdbch\"\n",
    "}\n",
    "\n",
    "# GO term file names\n",
    "GO_FILES = {\"mf\": \"mf_go.txt\", \"bp\": \"bp_go.txt\", \"cc\": \"cc_go.txt\"}\n",
    "\n",
    "print(\"🧬 Populating PDBCH Folders with GO Terms (Case-Sensitive)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ── 1. BUILD CASE-SENSITIVE LOOKUP FROM TSV ───────────────────────────────────\n",
    "print(\"📖 Loading GO annotations from TSV...\")\n",
    "\n",
    "# Initialize lookup dictionary for all proteins we'll encounter\n",
    "go_lookup = {}\n",
    "\n",
    "with GO_TSV.open(newline='', encoding=\"utf-8\") as fh:\n",
    "    tsv_reader = csv.reader(fh, delimiter='\\t')\n",
    "    \n",
    "    for row_num, row in enumerate(tsv_reader, 1):\n",
    "        if not row or row[0].startswith('#'):\n",
    "            continue  # Skip empty rows and comments\n",
    "        \n",
    "        # Extract protein ID (case-sensitive)\n",
    "        protein_id = row[0].strip()\n",
    "        \n",
    "        # Extract GO terms from columns\n",
    "        mf_terms = [term.strip() for term in row[1].split(',') if term.strip()] if len(row) > 1 else []\n",
    "        bp_terms = [term.strip() for term in row[2].split(',') if term.strip()] if len(row) > 2 else []\n",
    "        cc_terms = [term.strip() for term in row[3].split(',') if term.strip()] if len(row) > 3 else []\n",
    "        \n",
    "        # Store in case-sensitive lookup\n",
    "        go_lookup[protein_id] = {\n",
    "            \"mf\": mf_terms,\n",
    "            \"bp\": bp_terms, \n",
    "            \"cc\": cc_terms\n",
    "        }\n",
    "\n",
    "print(f\"✅ Loaded GO annotations for {len(go_lookup):,} proteins from TSV\")\n",
    "\n",
    "# ── 2. PROCESS EACH SPLIT FOLDER ──────────────────────────────────────────────\n",
    "total_stats = {\"processed\": 0, \"found_go\": 0, \"missing_go\": 0, \"files_written\": 0}\n",
    "\n",
    "for split_name, split_folder in SPLIT_FOLDERS.items():\n",
    "    print(f\"\\n🔄 Processing {split_name} split...\")\n",
    "    \n",
    "    if not split_folder.exists():\n",
    "        print(f\"   ❌ Split folder not found: {split_folder}\")\n",
    "        continue\n",
    "    \n",
    "    # Get all protein subfolders (tilde-encoded names)\n",
    "    protein_folders = [d for d in split_folder.iterdir() if d.is_dir()]\n",
    "    print(f\"   📁 Found {len(protein_folders):,} protein folders\")\n",
    "    \n",
    "    split_stats = {\"processed\": 0, \"found_go\": 0, \"missing_go\": 0, \"files_written\": 0}\n",
    "    \n",
    "    for protein_folder in protein_folders:\n",
    "        # STEP 1: Decode tilde-encoded folder name to get original protein ID\n",
    "        encoded_folder_name = protein_folder.name\n",
    "        original_protein_id = decode_protein_id(encoded_folder_name)\n",
    "        \n",
    "        split_stats[\"processed\"] += 1\n",
    "        \n",
    "        # STEP 2: Case-sensitive lookup in TSV data\n",
    "        if original_protein_id in go_lookup:\n",
    "            go_terms = go_lookup[original_protein_id]\n",
    "            split_stats[\"found_go\"] += 1\n",
    "            \n",
    "            # STEP 3: Write GO term files back to tilde-encoded folder\n",
    "            for ontology, filename in GO_FILES.items():\n",
    "                file_path = protein_folder / filename\n",
    "                terms = go_terms[ontology]\n",
    "                \n",
    "                with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write('\\n'.join(terms))\n",
    "                \n",
    "                split_stats[\"files_written\"] += 1\n",
    "        else:\n",
    "            # No GO terms found for this protein\n",
    "            split_stats[\"missing_go\"] += 1\n",
    "            \n",
    "            # Create empty GO files\n",
    "            for ontology, filename in GO_FILES.items():\n",
    "                file_path = protein_folder / filename\n",
    "                with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write('')  # Empty file\n",
    "                \n",
    "                split_stats[\"files_written\"] += 1\n",
    "    \n",
    "    # Update total stats\n",
    "    for key in total_stats:\n",
    "        total_stats[key] += split_stats[key]\n",
    "    \n",
    "    print(f\"   ✅ Processed: {split_stats['processed']:,} proteins\")\n",
    "    print(f\"   📊 Found GO terms: {split_stats['found_go']:,}\")\n",
    "    print(f\"   ❓ Missing GO terms: {split_stats['missing_go']:,}\")\n",
    "    print(f\"   📝 Files written: {split_stats['files_written']:,}\")\n",
    "\n",
    "# ── 3. FINAL VERIFICATION ─────────────────────────────────────────────────────\n",
    "print(f\"\\n🎉 FINAL RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"📊 Total proteins processed: {total_stats['processed']:,}\")\n",
    "print(f\"✅ Proteins with GO terms: {total_stats['found_go']:,}\")\n",
    "print(f\"❓ Proteins without GO terms: {total_stats['missing_go']:,}\")\n",
    "print(f\"📝 Total GO files written: {total_stats['files_written']:,}\")\n",
    "\n",
    "coverage_pct = (total_stats['found_go'] / total_stats['processed'] * 100) if total_stats['processed'] > 0 else 0\n",
    "print(f\"📈 GO term coverage: {coverage_pct:.1f}%\")\n",
    "\n",
    "# ── 4. SAMPLE VERIFICATION ────────────────────────────────────────────────────\n",
    "print(f\"\\n🔍 SAMPLE VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check a few folders to verify the structure\n",
    "for split_name, split_folder in list(SPLIT_FOLDERS.items())[:1]:  # Just check train\n",
    "    sample_folders = sorted([d for d in split_folder.iterdir() if d.is_dir()])[:3]\n",
    "    \n",
    "    print(f\"Sample from {split_name}:\")\n",
    "    for folder in sample_folders:\n",
    "        encoded_name = folder.name\n",
    "        original_id = decode_protein_id(encoded_name)\n",
    "        \n",
    "        # Check if GO files exist\n",
    "        go_files_exist = all((folder / filename).exists() for filename in GO_FILES.values())\n",
    "        \n",
    "        # Check if any GO files have content\n",
    "        has_content = False\n",
    "        for filename in GO_FILES.values():\n",
    "            file_path = folder / filename\n",
    "            if file_path.exists() and file_path.stat().st_size > 0:\n",
    "                has_content = True\n",
    "                break\n",
    "        \n",
    "        status = \"✅ WITH GO\" if has_content else \"⭕ EMPTY GO\"\n",
    "        print(f\"  {original_id:12s} → {encoded_name:15s} | {status}\")\n",
    "\n",
    "print(f\"\\n🎯 CRITICAL CHANGES MADE:\")\n",
    "print(\"1. ✅ Decoded tilde-encoded folder names to get original protein IDs\")\n",
    "print(\"2. ✅ Used case-sensitive lookup in TSV file (no case conversion)\")\n",
    "print(\"3. ✅ Wrote GO term files back to tilde-encoded folders\")\n",
    "print(\"4. ✅ Created empty files for proteins without GO annotations\")\n",
    "print(\"5. ✅ Preserved exact case sensitivity throughout the process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧬 Populating PDBCH Sequences with Case-Sensitive FASTA Lookup\n",
      "================================================================================\n",
      "📁 Scanning existing PDBCH folders...\n",
      "   📂 train: 29,893 folders found\n",
      "   📂 val: 3,322 folders found\n",
      "   📂 test: 3,414 folders found\n",
      "🎯 Total folders to process: 36,629\n",
      "🔍 Unique protein IDs needed: 36,629\n",
      "\n",
      "📖 Parsing FASTA file: nrPDB-GO_2019.06.18_sequences.fasta\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc6e0e5a42914e658d3740942bed7a1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading FASTA: 0lines [00:00, ?lines/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sequences found: 36,629 / 36,629\n",
      "\n",
      "🚀 Processing 36,629 proteins with sequences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0cdebd5c9b348bbbdcb72a43e19537b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing sequence files:   0%|          | 0/36629 [00:00<?, ?protein/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎉 FINAL RESULTS\n",
      "================================================================================\n",
      "📊 Total proteins targeted: 36,629\n",
      "✅ Successful writes: 36,629\n",
      "❌ Failed writes: 0\n",
      "⏭️  Skipped (no sequence): 0\n",
      "\n",
      "📁 Files written per split:\n",
      "   train: 29,893 proteins\n",
      "   val: 3,322 proteins\n",
      "   test: 3,414 proteins\n",
      "\n",
      "🔍 VERIFICATION SAMPLE\n",
      "================================================================================\n",
      "Sample from train:\n",
      "  154L-A       → 154L-A          | ✅ SEQ_LEN=185\n",
      "  155C-A       → 155C-A          | ✅ SEQ_LEN=135\n",
      "  16PK-A       → 16PK-A          | ✅ SEQ_LEN=415\n",
      "\n",
      "🎯 CRITICAL WORKFLOW EXECUTED:\n",
      "1. ✅ Scanned tilde-encoded folders in PDBCH structure\n",
      "2. ✅ Decoded folder names to get original protein IDs\n",
      "3. ✅ Performed case-sensitive FASTA sequence lookup\n",
      "4. ✅ Wrote sequence files back to tilde-encoded folders\n",
      "5. ✅ Preserved exact case sensitivity throughout process\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Populate sequence.txt and L.csv for every PDBCH protein folder.\n",
    "Handles tilde encoding/decoding and case-sensitive FASTA lookup.\n",
    "\n",
    "Directory layout produced:\n",
    "PDBCH/\n",
    "    train_pdbch/<ENCODED_CID>/{sequence.txt, L.csv, *_go.txt}\n",
    "    val_pdbch/<ENCODED_CID>/...\n",
    "    test_pdbch/<ENCODED_CID>/...\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import csv, os, concurrent.futures as cf\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def decode_protein_id(encoded_name):\n",
    "    \"\"\"Restore original protein ID by removing tildes\"\"\"\n",
    "    return encoded_name.replace('~', '')\n",
    "\n",
    "def encode_protein_id(protein_id):\n",
    "    \"\"\"Convert to Windows-safe: uppercase preserved, lowercase gets tilde after\"\"\"\n",
    "    return ''.join(c + '~' if c.islower() else c for c in protein_id)\n",
    "\n",
    "# ── CONFIGURATION ──────────────────────────────────────────────────────────────\n",
    "ROOT = Path(r\"C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\")\n",
    "FASTA_PATH = ROOT / \"nrPDB-GO_2019.06.18_sequences.fasta\"\n",
    "\n",
    "# PDBCH folder structure (tilde-encoded folders)\n",
    "PDBCH_ROOT = ROOT / \"PDBCH\"\n",
    "SPLIT_FOLDERS = {\n",
    "    \"train\": PDBCH_ROOT / \"train_pdbch\",\n",
    "    \"val\": PDBCH_ROOT / \"val_pdbch\", \n",
    "    \"test\": PDBCH_ROOT / \"test_pdbch\"\n",
    "}\n",
    "\n",
    "MAX_WORKERS = os.cpu_count() or 8\n",
    "\n",
    "print(\"🧬 Populating PDBCH Sequences with Case-Sensitive FASTA Lookup\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ── 1. COLLECT ALL TILDE-ENCODED FOLDERS & DECODE TO GET TARGET IDs ───────────\n",
    "print(\"📁 Scanning existing PDBCH folders...\")\n",
    "\n",
    "tasks = []  # Will store (encoded_folder_path, original_protein_id, split_name)\n",
    "all_target_ids = set()  # Original protein IDs we need sequences for\n",
    "\n",
    "for split_name, split_folder in SPLIT_FOLDERS.items():\n",
    "    if not split_folder.exists():\n",
    "        print(f\"   ❌ Split folder not found: {split_folder}\")\n",
    "        continue\n",
    "    \n",
    "    # Get all tilde-encoded protein folders\n",
    "    protein_folders = [d for d in split_folder.iterdir() if d.is_dir()]\n",
    "    print(f\"   📂 {split_name}: {len(protein_folders):,} folders found\")\n",
    "    \n",
    "    for protein_folder in protein_folders:\n",
    "        # CRITICAL: Decode tilde-encoded folder name to get original protein ID\n",
    "        encoded_folder_name = protein_folder.name\n",
    "        original_protein_id = decode_protein_id(encoded_folder_name)\n",
    "        \n",
    "        # Store task info\n",
    "        tasks.append((protein_folder, original_protein_id, split_name))\n",
    "        all_target_ids.add(original_protein_id)\n",
    "\n",
    "print(f\"🎯 Total folders to process: {len(tasks):,}\")\n",
    "print(f\"🔍 Unique protein IDs needed: {len(all_target_ids):,}\")\n",
    "\n",
    "# ── 2. PARSE FASTA FILE (CASE-SENSITIVE LOOKUP) ───────────────────────────────\n",
    "print(f\"\\n📖 Parsing FASTA file: {FASTA_PATH.name}\")\n",
    "\n",
    "sequences = {}  # Case-sensitive lookup: original_protein_id -> sequence\n",
    "\n",
    "with FASTA_PATH.open() as fh:\n",
    "    current_id, sequence_lines = None, []\n",
    "    \n",
    "    for line in tqdm(fh, desc=\"Reading FASTA\", unit=\"lines\"):\n",
    "        if line.startswith('>'):\n",
    "            # Save previous sequence if it's one we need\n",
    "            if current_id and current_id in all_target_ids:\n",
    "                sequences[current_id] = ''.join(sequence_lines).upper()\n",
    "            \n",
    "            # Start new sequence\n",
    "            current_id = line[1:].split()[0].strip()  # Extract ID (case-sensitive)\n",
    "            sequence_lines = []\n",
    "        else:\n",
    "            sequence_lines.append(line.strip())\n",
    "    \n",
    "    # Don't forget the last sequence\n",
    "    if current_id and current_id in all_target_ids:\n",
    "        sequences[current_id] = ''.join(sequence_lines).upper()\n",
    "\n",
    "print(f\"✅ Sequences found: {len(sequences):,} / {len(all_target_ids):,}\")\n",
    "\n",
    "# Identify missing sequences\n",
    "missing_sequences = all_target_ids - sequences.keys()\n",
    "if missing_sequences:\n",
    "    print(f\"❌ Missing sequences: {len(missing_sequences):,}\")\n",
    "    # Show a few examples\n",
    "    for missing_id in sorted(missing_sequences)[:5]:\n",
    "        print(f\"   • {missing_id}\")\n",
    "    if len(missing_sequences) > 5:\n",
    "        print(f\"   • ... and {len(missing_sequences) - 5} more\")\n",
    "\n",
    "# ── 3. PARALLEL WRITER FUNCTION ───────────────────────────────────────────────\n",
    "def write_sequence_files(task):\n",
    "    \"\"\"Write sequence.txt and L.csv to tilde-encoded folder\"\"\"\n",
    "    protein_folder, original_protein_id, split_name = task\n",
    "    \n",
    "    try:\n",
    "        # Check if we have the sequence for this protein\n",
    "        if original_protein_id not in sequences:\n",
    "            return f\"MISSING: {original_protein_id} (folder: {protein_folder.name})\"\n",
    "        \n",
    "        sequence = sequences[original_protein_id]\n",
    "        \n",
    "        # Write sequence.txt to the TILDE-ENCODED folder\n",
    "        sequence_file = protein_folder / \"sequence.txt\"\n",
    "        sequence_file.write_text(sequence + \"\\n\", encoding=\"utf-8\")\n",
    "        \n",
    "        # Write L.csv (sequence length) to the TILDE-ENCODED folder\n",
    "        length_file = protein_folder / \"L.csv\"\n",
    "        with length_file.open('w', newline='', encoding=\"utf-8\") as f:\n",
    "            csv.writer(f).writerow([len(sequence)])\n",
    "        \n",
    "        return None  # Success\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {original_protein_id} -> {str(e)}\"\n",
    "\n",
    "# ── 4. FILTER TASKS (ONLY PROCESS THOSE WITH AVAILABLE SEQUENCES) ─────────────\n",
    "valid_tasks = []\n",
    "skipped_tasks = []\n",
    "\n",
    "for task in tasks:\n",
    "    protein_folder, original_protein_id, split_name = task\n",
    "    if original_protein_id in sequences:\n",
    "        valid_tasks.append(task)\n",
    "    else:\n",
    "        skipped_tasks.append(task)\n",
    "\n",
    "print(f\"\\n🚀 Processing {len(valid_tasks):,} proteins with sequences...\")\n",
    "if skipped_tasks:\n",
    "    print(f\"⏭️  Skipping {len(skipped_tasks):,} proteins without sequences\")\n",
    "\n",
    "# ── 5. PARALLEL EXECUTION WITH PROGRESS BAR ───────────────────────────────────\n",
    "errors = []\n",
    "\n",
    "with cf.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Use tqdm to track progress\n",
    "    results = list(tqdm(\n",
    "        executor.map(write_sequence_files, valid_tasks, chunksize=256),\n",
    "        total=len(valid_tasks),\n",
    "        desc=\"Writing sequence files\",\n",
    "        unit=\"protein\"\n",
    "    ))\n",
    "    \n",
    "    # Collect errors\n",
    "    errors = [result for result in results if result is not None]\n",
    "\n",
    "# ── 6. SUMMARY STATISTICS ─────────────────────────────────────────────────────\n",
    "print(f\"\\n🎉 FINAL RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "successful_writes = len(valid_tasks) - len(errors)\n",
    "print(f\"📊 Total proteins targeted: {len(tasks):,}\")\n",
    "print(f\"✅ Successful writes: {successful_writes:,}\")\n",
    "print(f\"❌ Failed writes: {len(errors):,}\")\n",
    "print(f\"⏭️  Skipped (no sequence): {len(skipped_tasks):,}\")\n",
    "\n",
    "# Break down by split\n",
    "split_stats = {\"train\": 0, \"val\": 0, \"test\": 0}\n",
    "for task in valid_tasks:\n",
    "    split_name = task[2]\n",
    "    split_stats[split_name] += 1\n",
    "\n",
    "print(f\"\\n📁 Files written per split:\")\n",
    "for split_name, count in split_stats.items():\n",
    "    print(f\"   {split_name}: {count:,} proteins\")\n",
    "\n",
    "# Show errors if any\n",
    "if errors:\n",
    "    print(f\"\\n❌ Errors encountered:\")\n",
    "    for error_msg in errors[:10]:  # Show first 10 errors\n",
    "        print(f\"   • {error_msg}\")\n",
    "    if len(errors) > 10:\n",
    "        print(f\"   • ... and {len(errors) - 10} more errors\")\n",
    "\n",
    "# ── 7. VERIFICATION SAMPLE ────────────────────────────────────────────────────\n",
    "print(f\"\\n🔍 VERIFICATION SAMPLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check a few folders to verify files were created correctly\n",
    "for split_name, split_folder in list(SPLIT_FOLDERS.items())[:1]:  # Just check train\n",
    "    sample_folders = sorted([d for d in split_folder.iterdir() if d.is_dir()])[:3]\n",
    "    \n",
    "    print(f\"Sample from {split_name}:\")\n",
    "    for folder in sample_folders:\n",
    "        encoded_name = folder.name\n",
    "        original_id = decode_protein_id(encoded_name)\n",
    "        \n",
    "        # Check if files exist\n",
    "        sequence_file = folder / \"sequence.txt\"\n",
    "        length_file = folder / \"L.csv\"\n",
    "        \n",
    "        seq_exists = sequence_file.exists()\n",
    "        len_exists = length_file.exists()\n",
    "        \n",
    "        if seq_exists and len_exists:\n",
    "            # Read sequence length for verification\n",
    "            try:\n",
    "                with length_file.open('r') as f:\n",
    "                    length = int(next(csv.reader(f))[0])\n",
    "                status = f\"✅ SEQ_LEN={length}\"\n",
    "            except:\n",
    "                status = \"⚠️  FILES_EXIST_BUT_UNREADABLE\"\n",
    "        else:\n",
    "            status = \"❌ MISSING_FILES\"\n",
    "        \n",
    "        print(f\"  {original_id:12s} → {encoded_name:15s} | {status}\")\n",
    "\n",
    "print(f\"\\n🎯 CRITICAL WORKFLOW EXECUTED:\")\n",
    "print(\"1. ✅ Scanned tilde-encoded folders in PDBCH structure\")\n",
    "print(\"2. ✅ Decoded folder names to get original protein IDs\")\n",
    "print(\"3. ✅ Performed case-sensitive FASTA sequence lookup\")\n",
    "print(\"4. ✅ Wrote sequence files back to tilde-encoded folders\")\n",
    "print(\"5. ✅ Preserved exact case sensitivity throughout process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Transferring A3M Files: HEAL_PDB → PDBCH (Case-Sensitive)\n",
      "================================================================================\n",
      "🔍 Scanning old HEAL_PDB structure for available A3M files...\n",
      "   📂 train: 29,733 proteins with final_filtered_256_stripped.a3m\n",
      "   📂 val: 3,316 proteins with final_filtered_256_stripped.a3m\n",
      "   📂 test: 3,398 proteins with final_filtered_256_stripped.a3m\n",
      "✅ Total A3M files available: 36,447\n",
      "\n",
      "🔄 Processing train split...\n",
      "   📁 Found 29,893 tilde-encoded folders\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af9672cb3d541e0956f63f9913cbd59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transferring train:   0%|          | 0/29893 [00:00<?, ?protein/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Copied: 29,733\n",
      "   ❓ Missing: 160\n",
      "   ❌ Errors: 0\n",
      "   📋 Copy examples:\n",
      "      154L-A       → 154L-A          ✅\n",
      "      155C-A       → 155C-A          ✅\n",
      "      16PK-A       → 16PK-A          ✅\n",
      "   📋 Missing examples:\n",
      "      1W8X-M       → 1W8X-M          ❓\n",
      "      2WWX-B       → 2WWX-B          ❓\n",
      "      3J79-f       → 3J79-f~         ❓\n",
      "\n",
      "🔄 Processing val split...\n",
      "   📁 Found 3,322 tilde-encoded folders\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8a2bcf092d4453c84900ae8700f0ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transferring val:   0%|          | 0/3322 [00:00<?, ?protein/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Copied: 3,316\n",
      "   ❓ Missing: 6\n",
      "   ❌ Errors: 0\n",
      "   📋 Copy examples:\n",
      "      192L-A       → 192L-A          ✅\n",
      "      1A0A-A       → 1A0A-A          ✅\n",
      "      1A21-A       → 1A21-A          ✅\n",
      "   📋 Missing examples:\n",
      "      1KVE-A       → 1KVE-A          ❓\n",
      "      3J9M-l       → 3J9M-l~         ❓\n",
      "      4BTP-A       → 4BTP-A          ❓\n",
      "\n",
      "🔄 Processing test split...\n",
      "   📁 Found 3,414 tilde-encoded folders\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2cf02aeaa44854a09d83d69e1d91c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Transferring test:   0%|          | 0/3414 [00:00<?, ?protein/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Copied: 3,398\n",
      "   ❓ Missing: 16\n",
      "   ❌ Errors: 0\n",
      "   📋 Copy examples:\n",
      "      11AS-A       → 11AS-A          ✅\n",
      "      18GS-A       → 18GS-A          ✅\n",
      "      1A0P-A       → 1A0P-A          ✅\n",
      "   📋 Missing examples:\n",
      "      3H4P-a       → 3H4P-a~         ❓\n",
      "      4V6W-Ab      → 4V6W-Ab~        ❓\n",
      "      4V6W-Ca      → 4V6W-Ca~        ❓\n",
      "\n",
      "🎉 TRANSFER COMPLETE\n",
      "================================================================================\n",
      "✅ Files successfully copied: 36,447\n",
      "❓ Files not found in source: 182\n",
      "❌ Copy errors: 0\n",
      "📈 Success rate: 99.5%\n",
      "\n",
      "🔍 VERIFICATION SAMPLE\n",
      "================================================================================\n",
      "Sample verification from train:\n",
      "  154L-A       → 154L-A          | ✅ A3M_SIZE=61,594B\n",
      "  155C-A       → 155C-A          | ✅ A3M_SIZE=45,033B\n",
      "  16PK-A       → 16PK-A          | ✅ A3M_SIZE=121,666B\n",
      "\n",
      "🎯 CRITICAL WORKFLOW EXECUTED:\n",
      "1. ✅ Scanned tilde-encoded PDBCH folders\n",
      "2. ✅ Decoded folder names to get original protein IDs\n",
      "3. ✅ Performed case-sensitive exact matching with old structure\n",
      "4. ✅ Copied A3M files to tilde-encoded destination folders\n",
      "5. ✅ Preserved split correspondence (train→train, val→val, test→test)\n",
      "6. ✅ Maintained exact case sensitivity throughout process\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Transfer final_filtered_256_stripped.a3m files from old HEAL_PDB structure \n",
    "to new PDBCH structure with proper tilde encoding and case-sensitive matching.\n",
    "\n",
    "Critical workflow:\n",
    "1. Scan tilde-encoded PDBCH folders\n",
    "2. Decode folder names to get original protein IDs  \n",
    "3. Find EXACT case-sensitive matches in old HEAL_PDB structure\n",
    "4. Copy .a3m files to tilde-encoded destination folders\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def decode_protein_id(encoded_name):\n",
    "    \"\"\"Restore original protein ID by removing tildes\"\"\"\n",
    "    return encoded_name.replace('~', '')\n",
    "\n",
    "def encode_protein_id(protein_id):\n",
    "    \"\"\"Convert to Windows-safe: uppercase preserved, lowercase gets tilde after\"\"\"\n",
    "    return ''.join(c + '~' if c.islower() else c for c in protein_id)\n",
    "\n",
    "# ── CONFIGURATION ──────────────────────────────────────────────────────────────\n",
    "HEAL_PDB_ROOT = Path(r\"C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\")\n",
    "PDBCH_ROOT = Path(r\"C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\PDBCH\")\n",
    "\n",
    "# Source: Old HEAL_PDB structure (non-tilde encoded)\n",
    "OLD_FOLDERS = {\n",
    "    \"train\": HEAL_PDB_ROOT / \"protein_train_pdb\",\n",
    "    \"val\": HEAL_PDB_ROOT / \"protein_val_pdb\", \n",
    "    \"test\": HEAL_PDB_ROOT / \"protein_test_pdb\"\n",
    "}\n",
    "\n",
    "# Destination: New PDBCH structure (tilde-encoded)\n",
    "NEW_FOLDERS = {\n",
    "    \"train\": PDBCH_ROOT / \"train_pdbch\",\n",
    "    \"val\": PDBCH_ROOT / \"val_pdbch\",\n",
    "    \"test\": PDBCH_ROOT / \"test_pdbch\"\n",
    "}\n",
    "\n",
    "TARGET_FILE = \"final_filtered_256_stripped.a3m\"\n",
    "\n",
    "print(\"🔄 Transferring A3M Files: HEAL_PDB → PDBCH (Case-Sensitive)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ── 1. BUILD CASE-SENSITIVE LOOKUP OF AVAILABLE A3M FILES ─────────────────────\n",
    "print(\"🔍 Scanning old HEAL_PDB structure for available A3M files...\")\n",
    "\n",
    "old_files_lookup = {}  # Structure: {split: {original_protein_id: file_path}}\n",
    "\n",
    "for split_name, old_folder in OLD_FOLDERS.items():\n",
    "    if not old_folder.exists():\n",
    "        print(f\"   ❌ Old folder not found: {old_folder}\")\n",
    "        continue\n",
    "    \n",
    "    old_files_lookup[split_name] = {}\n",
    "    \n",
    "    # Scan all protein folders in this split\n",
    "    protein_folders = [d for d in old_folder.iterdir() if d.is_dir()]\n",
    "    found_files = 0\n",
    "    \n",
    "    for protein_folder in protein_folders:\n",
    "        original_protein_id = protein_folder.name  # Case-sensitive protein ID\n",
    "        target_file_path = protein_folder / TARGET_FILE\n",
    "        \n",
    "        if target_file_path.exists():\n",
    "            old_files_lookup[split_name][original_protein_id] = target_file_path\n",
    "            found_files += 1\n",
    "    \n",
    "    print(f\"   📂 {split_name}: {found_files:,} proteins with {TARGET_FILE}\")\n",
    "\n",
    "total_available = sum(len(split_files) for split_files in old_files_lookup.values())\n",
    "print(f\"✅ Total A3M files available: {total_available:,}\")\n",
    "\n",
    "# ── 2. PROCESS EACH PDBCH SPLIT ───────────────────────────────────────────────\n",
    "overall_stats = {\"copied\": 0, \"missing\": 0, \"errors\": 0}\n",
    "\n",
    "for split_name, new_folder in NEW_FOLDERS.items():\n",
    "    if not new_folder.exists():\n",
    "        print(f\"\\n❌ New folder not found: {new_folder}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n🔄 Processing {split_name} split...\")\n",
    "    \n",
    "    # Get corresponding old folder lookup\n",
    "    if split_name not in old_files_lookup:\n",
    "        print(f\"   ❌ No old files found for {split_name} split\")\n",
    "        continue\n",
    "    \n",
    "    old_split_files = old_files_lookup[split_name]\n",
    "    \n",
    "    # Get all tilde-encoded protein folders in new structure\n",
    "    protein_folders = [d for d in new_folder.iterdir() if d.is_dir()]\n",
    "    print(f\"   📁 Found {len(protein_folders):,} tilde-encoded folders\")\n",
    "    \n",
    "    split_stats = {\"copied\": 0, \"missing\": 0, \"errors\": 0}\n",
    "    examples = {\"copied\": [], \"missing\": []}\n",
    "    \n",
    "    # Process each tilde-encoded folder\n",
    "    for protein_folder in tqdm(protein_folders, desc=f\"Transferring {split_name}\", unit=\"protein\"):\n",
    "        # STEP 1: Decode tilde-encoded folder name to get original protein ID\n",
    "        encoded_folder_name = protein_folder.name\n",
    "        original_protein_id = decode_protein_id(encoded_folder_name)\n",
    "        \n",
    "        # STEP 2: Look for EXACT case-sensitive match in old structure\n",
    "        if original_protein_id in old_split_files:\n",
    "            # Found exact match!\n",
    "            source_file_path = old_split_files[original_protein_id]\n",
    "            destination_file_path = protein_folder / TARGET_FILE\n",
    "            \n",
    "            try:\n",
    "                # STEP 3: Copy file to tilde-encoded destination folder\n",
    "                shutil.copy2(source_file_path, destination_file_path)\n",
    "                split_stats[\"copied\"] += 1\n",
    "                \n",
    "                # Store example for verification\n",
    "                if len(examples[\"copied\"]) < 3:\n",
    "                    examples[\"copied\"].append({\n",
    "                        \"original_id\": original_protein_id,\n",
    "                        \"encoded_folder\": encoded_folder_name,\n",
    "                        \"source\": source_file_path,\n",
    "                        \"dest\": destination_file_path\n",
    "                    })\n",
    "                \n",
    "            except Exception as e:\n",
    "                split_stats[\"errors\"] += 1\n",
    "                print(f\"   ❌ Error copying {original_protein_id}: {e}\")\n",
    "        else:\n",
    "            # No exact match found\n",
    "            split_stats[\"missing\"] += 1\n",
    "            \n",
    "            # Store example for debugging\n",
    "            if len(examples[\"missing\"]) < 3:\n",
    "                examples[\"missing\"].append({\n",
    "                    \"original_id\": original_protein_id,\n",
    "                    \"encoded_folder\": encoded_folder_name\n",
    "                })\n",
    "    \n",
    "    # Update overall stats\n",
    "    for key in overall_stats:\n",
    "        overall_stats[key] += split_stats[key]\n",
    "    \n",
    "    # Report split results\n",
    "    print(f\"   ✅ Copied: {split_stats['copied']:,}\")\n",
    "    print(f\"   ❓ Missing: {split_stats['missing']:,}\")\n",
    "    print(f\"   ❌ Errors: {split_stats['errors']:,}\")\n",
    "    \n",
    "    # Show examples\n",
    "    if examples[\"copied\"]:\n",
    "        print(f\"   📋 Copy examples:\")\n",
    "        for ex in examples[\"copied\"]:\n",
    "            print(f\"      {ex['original_id']:12s} → {ex['encoded_folder']:15s} ✅\")\n",
    "    \n",
    "    if examples[\"missing\"]:\n",
    "        print(f\"   📋 Missing examples:\")\n",
    "        for ex in examples[\"missing\"]:\n",
    "            print(f\"      {ex['original_id']:12s} → {ex['encoded_folder']:15s} ❓\")\n",
    "\n",
    "# ── 3. FINAL SUMMARY ───────────────────────────────────────────────────────────\n",
    "print(f\"\\n🎉 TRANSFER COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"✅ Files successfully copied: {overall_stats['copied']:,}\")\n",
    "print(f\"❓ Files not found in source: {overall_stats['missing']:,}\")\n",
    "print(f\"❌ Copy errors: {overall_stats['errors']:,}\")\n",
    "\n",
    "success_rate = (overall_stats['copied'] / (overall_stats['copied'] + overall_stats['missing']) * 100) if (overall_stats['copied'] + overall_stats['missing']) > 0 else 0\n",
    "print(f\"📈 Success rate: {success_rate:.1f}%\")\n",
    "\n",
    "# ── 4. VERIFICATION SAMPLE ────────────────────────────────────────────────────\n",
    "print(f\"\\n🔍 VERIFICATION SAMPLE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check a few transferred files\n",
    "for split_name, new_folder in list(NEW_FOLDERS.items())[:1]:  # Just check train\n",
    "    if not new_folder.exists():\n",
    "        continue\n",
    "    \n",
    "    sample_folders = sorted([d for d in new_folder.iterdir() if d.is_dir()])[:3]\n",
    "    \n",
    "    print(f\"Sample verification from {split_name}:\")\n",
    "    for folder in sample_folders:\n",
    "        encoded_name = folder.name\n",
    "        original_id = decode_protein_id(encoded_name)\n",
    "        a3m_file = folder / TARGET_FILE\n",
    "        \n",
    "        if a3m_file.exists():\n",
    "            file_size = a3m_file.stat().st_size\n",
    "            status = f\"✅ A3M_SIZE={file_size:,}B\"\n",
    "        else:\n",
    "            status = \"❌ A3M_MISSING\"\n",
    "        \n",
    "        print(f\"  {original_id:12s} → {encoded_name:15s} | {status}\")\n",
    "\n",
    "print(f\"\\n🎯 CRITICAL WORKFLOW EXECUTED:\")\n",
    "print(\"1. ✅ Scanned tilde-encoded PDBCH folders\")\n",
    "print(\"2. ✅ Decoded folder names to get original protein IDs\")\n",
    "print(\"3. ✅ Performed case-sensitive exact matching with old structure\")\n",
    "print(\"4. ✅ Copied A3M files to tilde-encoded destination folders\")\n",
    "print(\"5. ✅ Preserved split correspondence (train→train, val→val, test→test)\")\n",
    "print(\"6. ✅ Maintained exact case sensitivity throughout process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧬 Mapping Problematic Proteins to OpenProtein Set\n",
      "======================================================================\n",
      "📖 Loading OpenProtein Set FASTA sequences...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0487d4a4bf495c95b6bee5fbeb66ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Reading FASTA: 0lines [00:00, ?lines/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 131,487 OpenProtein Set sequences\n",
      "\n",
      "🔍 Scanning PDBCH folders for 233 problematic proteins...\n",
      "   📂 train: Found 208 proteins\n",
      "   📂 val: Found 5 proteins\n",
      "   📂 test: Found 20 proteins\n",
      "✅ Total proteins to map: 233\n",
      "\n",
      "🔄 Performing hierarchical mapping...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b66fbdffc44413871dbf48b73fb661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Mapping proteins:   0%|          | 0/233 [00:00<?, ?protein/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Writing results to TSV...\n",
      "\n",
      "🎉 MAPPING COMPLETE\n",
      "======================================================================\n",
      "📊 Mapping Statistics:\n",
      "   Stage 1 (Exact ID)      : 189 ( 81.1%)\n",
      "   Stage 2 (Exact Sequence):  43 ( 18.5%)\n",
      "   Stage 3 (Similarity)    :   1 (  0.4%)\n",
      "   No Sequence Available   :   0 (  0.0%)\n",
      "   No Match Found          :   0 (  0.0%)\n",
      "\n",
      "✅ Total successful matches: 233 / 233 (100.0%)\n",
      "⏱️  Processing time: 7.4 seconds\n",
      "📁 Results saved to: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\problematic_proteins_openfold_mapping.tsv\n",
      "\n",
      "📋 SAMPLE MAPPING RESULTS:\n",
      "----------------------------------------------------------------------\n",
      "5IT7-EE  → 5IT7-EE    → 5it7_EE  → 5it7_EE         | Stage 1 - Exact ID\n",
      "5XY3-m   → 5XY3-m~    → 5xy3_m   → 5xy3_m          | Stage 1 - Exact ID\n",
      "6GAZ-Ap  → 6GAZ-Ap~   → 6gaz_Ap  → 5aj3_p          | Stage 2 - Exact Sequence\n",
      "4V8M-Bp  → 4V8M-Bp~   → 4v8m_Bp  → 4v8m_Bp         | Stage 1 - Exact ID\n",
      "5XYI-b   → 5XYI-b~    → 5xyi_b   → 5xyi_b          | Stage 1 - Exact ID\n",
      "3JB9-i   → 3JB9-i~    → 3jb9_i   → 3jb9_i          | Stage 1 - Exact ID\n",
      "5XYI-e   → 5XYI-e~    → 5xyi_e   → 5xyi_e          | Stage 1 - Exact ID\n",
      "4V6U-Be  → 4V6U-Be~   → 4v6u_Be  → 4v4n_Ae         | Stage 2 - Exact Sequence\n",
      "5OQL-l   → 5OQL-l~    → 5oql_l   → 5oql_l          | Stage 1 - Exact ID\n",
      "4V7E-Cq  → 4V7E-Cq~   → 4v7e_Cq  → 4v7e_Cq         | Stage 1 - Exact ID\n",
      "... and 223 more results in TSV file\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Hierarchical mapping of problematic PDBCH proteins to OpenProtein Set sequences.\n",
    "\n",
    "Maps proteins using:\n",
    "1. Stage 1: Exact ID match (format conversion)\n",
    "2. Stage 2: Exact sequence match  \n",
    "3. Stage 3: Similarity match (±15 AA tolerance)\n",
    "\n",
    "Critical: Preserves exact case sensitivity and tilde encoding throughout.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import csv, time\n",
    "from collections import defaultdict, Counter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def decode_protein_id(encoded_name):\n",
    "    \"\"\"Restore original protein ID by removing tildes\"\"\"\n",
    "    return encoded_name.replace('~', '')\n",
    "\n",
    "def format_protein_id_for_openfold(original_id):\n",
    "    \"\"\"\n",
    "    Format protein ID for OpenProtein Set lookup:\n",
    "    - Make first 4 characters lowercase\n",
    "    - Convert - to _\n",
    "    - Leave chain part exactly as-is (no case change)\n",
    "    \n",
    "    Examples:\n",
    "        3H4P-a  → 3h4p_a\n",
    "        5IT7-aa → 5it7_aa\n",
    "        4V6W-Co → 4v6w_Co\n",
    "    \"\"\"\n",
    "    if '-' in original_id:\n",
    "        pdb_part, chain_part = original_id.split('-', 1)\n",
    "        return f\"{pdb_part[:4].lower()}_{chain_part}\"\n",
    "    elif '_' in original_id:\n",
    "        pdb_part, chain_part = original_id.split('_', 1)\n",
    "        return f\"{pdb_part[:4].lower()}_{chain_part}\"\n",
    "    else:\n",
    "        return original_id[:4].lower()\n",
    "\n",
    "# ── CONFIGURATION ──────────────────────────────────────────────────────────────\n",
    "ROOT = Path(r\"C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\")\n",
    "FASTA_FILE = ROOT / \"data\" / \"openfold_pdb_query_sequences.fasta\"\n",
    "PDBCH_ROOT = ROOT / \"data\" / \"PDBCH\"\n",
    "\n",
    "SPLIT_FOLDERS = {\n",
    "    \"train\": PDBCH_ROOT / \"train_pdbch\",\n",
    "    \"val\": PDBCH_ROOT / \"val_pdbch\",\n",
    "    \"test\": PDBCH_ROOT / \"test_pdbch\"\n",
    "}\n",
    "\n",
    "OUTPUT_TSV = ROOT / \"problematic_proteins_openfold_mapping.tsv\"\n",
    "\n",
    "# Specific problematic proteins (tilde-encoded folder names + additional from percentage list)\n",
    "PROBLEMATIC_PROTEINS = {\n",
    "    # Original tilde-encoded list\n",
    "    \"3H4P-a~\", \"3J79-f~\", \"3J79-i~\", \"3J7Y-f~\", \"3J92-f~\", \"3J9M-l~\", \"3JB9-g~\",\n",
    "    \"3JB9-i~\", \"3JB9-r~\", \"3JCS-d~\", \"3JCS-e~\", \"3JD5-f~\", \"3JD5-j~\", \"3JD5-n~\",\n",
    "    \"4CE4-u~\", \"4V3P-La~\", \"4V3P-Lb~\", \"4V3P-Le~\", \"4V3P-Lj~\", \"4V4N-Ad~\", \"4V4N-Aj~\",\n",
    "    \"4V6U-Be~\", \"4V6U-Bl~\", \"4V6W-Ab~\", \"4V6W-Af~\", \"4V6W-Ca~\", \"4V6W-Cb~\", \"4V6W-Cc~\",\n",
    "    \"4V6W-Cg~\", \"4V6W-Cj~\", \"4V6W-Ck~\", \"4V6W-Co~\", \"4V6W-Cr~\", \"4V7E-Ba~\", \"4V7E-Bb~\",\n",
    "    \"4V7E-Bf~\", \"4V7E-Bg~\", \"4V7E-Ce~\", \"4V7E-Ci~\", \"4V7E-Co~\", \"4V7E-Cq~\", \"4V7E-Cu~\",\n",
    "    \"4V8M-Bi~\", \"4V8M-Bj~\", \"4V8M-Bk~\", \"4V8M-Bl~\", \"4V8M-Bm~\", \"4V8M-Bp~\", \"4V8M-Br~\",\n",
    "    \"4V8M-Bs~\", \"4V8M-Bt~\", \"4V8M-Bu~\", \"4V8M-Bv~\", \"4V8M-By~\", \"5AJ4-Ak~\", \"5AJ4-Ap~\",\n",
    "    \"5GUP-l~\", \"5GUP-m~\", \"5GUP-v~\", \"5IT7-a~a~\", \"5IT7-c~c~\", \"5IT7-e~e~\", \"5IT7-f~f~\",\n",
    "    \"5IT7-h~h~\", \"5IT7-i~i~\", \"5IT7-j~j~\", \"5IT7-o~o~\", \"5IT7-p~p~\", \"5IT7-r~r~\", \"5KZ5-a~\",\n",
    "    \"5L9W-b~\", \"5LJ5-s~\", \"5LNK-h~\", \"5LNK-l~\", \"5LNK-m~\", \"5LNK-n~\", \"5MMM-i~\", \"5NGM-Ao~\",\n",
    "    \"5O31-m~\", \"5OOL-w~\", \"5OPT-l~\", \"5OPT-p~\", \"5OPT-r~\", \"5OPT-t~\", \"5OQL-e~\", \"5OQL-h~\",\n",
    "    \"5OQL-l~\", \"5OQL-p~\", \"5OQL-u~\", \"5OQL-v~\", \"5OQL-x~\", \"5OQL-y~\", \"5T2A-n~\", \"5T2A-v~\",\n",
    "    \"5T5H-i~\", \"5T5H-l~\", \"5T5H-n~\", \"5T5H-t~\", \"5T5H-v~\", \"5T5H-w~\", \"5V93-e~\", \"5V93-f~\",\n",
    "    \"5V93-n~\", \"5V93-p~\", \"5V93-q~\", \"5VK2-a~\", \"5XXB-b~\", \"5XXB-c~\", \"5XXB-e~\", \"5XXB-f~\",\n",
    "    \"5XXB-g~\", \"5XXB-h~\", \"5XXB-i~\", \"5XXB-n~\", \"5XXB-o~\", \"5XXB-p~\", \"5XXU-a~\", \"5XXU-b~\",\n",
    "    \"5XXU-c~\", \"5XXU-f~\", \"5XY3-c~\", \"5XY3-d~\", \"5XY3-e~\", \"5XY3-f~\", \"5XY3-g~\", \"5XY3-h~\",\n",
    "    \"5XY3-i~\", \"5XY3-j~\", \"5XY3-m~\", \"5XY3-o~\", \"5XY3-p~\", \"5XYI-a~\", \"5XYI-b~\", \"5XYI-c~\",\n",
    "    \"5XYI-e~\", \"5YZG-w~\", \"5ZWN-y~\", \"6AZ1-a~\", \"6AZ1-b~\", \"6AZ1-c~\", \"6AZ1-e~\", \"6AZ3-d~\",\n",
    "    \"6AZ3-h~\", \"6DZI-t~\", \"6DZI-z~\", \"6ERI-Az~\", \"6G2J-e~\", \"6G72-b~\", \"6G72-g~\", \"6G72-h~\",\n",
    "    \"6G72-m~\", \"6GAZ-An~\", \"6GAZ-Ao~\", \"6GAZ-Ap~\", \"6GB2-Be~\", \"6GB2-Bw~\", \"6GCS-c~\", \"6GCS-d~\",\n",
    "    \"6GCS-f~\", \"6GCS-h~\", \"6GCS-j~\", \"6GIQ-d~\", \"6GIQ-e~\", \"6GIQ-h~\", \"6HA1-p~\", \"6HA8-d~\",\n",
    "    \"6HHQ-p~\", \"6HIV-Bb~\", \"6HIV-Bc~\", \"6HIX-Av~\", \"6HIX-Bg~\", \"6HIZ-Ci~\", \"6OKK-b~\", \"6OKK-c~\",\n",
    "    \"6QDV-d~\",\n",
    "    \n",
    "    # Additional proteins from percentage list (uppercase, no tilde encoding needed)\n",
    "    \"3H4P-A\", \"3J79-I\", \"3JB9-I\", \"3JB9-R\", \"3JD5-F\", \"3JD5-J\", \"4UDF-1B\", \"4V6U-BL\", \n",
    "    \"4V7E-CE\", \"4V8T-O\", \"5IT7-CC\", \"5IT7-EE\", \"5IT7-HH\", \"5IT7-II\", \"5IT7-RR\", \"5LNK-N\", \n",
    "    \"5NGM-AO\", \"5OOL-W\", \"5OPT-P\", \"5OPT-R\", \"5OQL-U\", \"5OQL-X\", \"5T5H-I\", \"5T5H-L\", \n",
    "    \"5T5H-V\", \"5T5H-W\", \"5V93-F\", \"5XXB-E\", \"5XXB-F\", \"5XXB-G\", \"5XXB-O\", \"5XXU-A\", \n",
    "    \"5XXU-B\", \"5XXU-C\", \"5XXU-F\", \"5XY3-E\", \"5XY3-F\", \"5XY3-G\", \"5XY3-I\", \"5XY3-J\", \n",
    "    \"5XY3-O\", \"5XYI-B\", \"5YZG-W\", \"5ZWN-Y\", \"6AZ1-A\", \"6AZ1-B\", \"6AZ1-C\", \"6DZI-T\", \n",
    "    \"6GAZ-AN\", \"6GAZ-AP\", \"6GB2-BE\", \"6GCS-D\", \"6GCS-F\", \"6GCS-H\", \"6GCS-J\", \"6GIQ-D\", \n",
    "    \"6HIX-AV\", \"6HIX-BG\"\n",
    "}\n",
    "\n",
    "LENGTH_TOLERANCE = 15\n",
    "SIMILARITY_THRESHOLD = 0.0\n",
    "MAX_THREADS = 8\n",
    "\n",
    "# ── SIMILARITY FUNCTION ────────────────────────────────────────────────────────\n",
    "from Bio import pairwise2\n",
    "def similarity(seq1, seq2):\n",
    "    \"\"\"Calculate sequence similarity using Biopython\"\"\"\n",
    "    if not seq1 or not seq2:\n",
    "        return 0.0\n",
    "    score = pairwise2.align.globalxx(seq1, seq2, score_only=True)\n",
    "    return score / max(len(seq1), len(seq2))\n",
    "\n",
    "def similarity_worker(args):\n",
    "    \"\"\"Worker function for parallel similarity calculation\"\"\"\n",
    "    p_seq, f_id, f_seq = args\n",
    "    try:\n",
    "        return f_id, similarity(p_seq, f_seq)\n",
    "    except Exception as e:\n",
    "        return f_id, 0.0\n",
    "\n",
    "print(\"🧬 Mapping Problematic Proteins to OpenProtein Set\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# ── 1. LOAD OPENFOLD FASTA SEQUENCES ───────────────────────────────────────────\n",
    "print(\"📖 Loading OpenProtein Set FASTA sequences...\")\n",
    "\n",
    "fasta_seqs = {}\n",
    "fasta_len = {}\n",
    "\n",
    "with FASTA_FILE.open() as fh:\n",
    "    current_id, sequence_lines = None, []\n",
    "    \n",
    "    for line in tqdm(fh, desc=\"Reading FASTA\", unit=\"lines\"):\n",
    "        line = line.strip()\n",
    "        if line.startswith('>'):\n",
    "            # Save previous sequence\n",
    "            if current_id and sequence_lines:\n",
    "                seq = ''.join(sequence_lines)\n",
    "                fasta_seqs[current_id] = seq\n",
    "                fasta_len[current_id] = len(seq)\n",
    "            \n",
    "            # Start new sequence\n",
    "            current_id = line[1:]  # Remove '>' and keep exact ID\n",
    "            sequence_lines = []\n",
    "        else:\n",
    "            sequence_lines.append(line)\n",
    "    \n",
    "    # Save last sequence\n",
    "    if current_id and sequence_lines:\n",
    "        seq = ''.join(sequence_lines)\n",
    "        fasta_seqs[current_id] = seq\n",
    "        fasta_len[current_id] = len(seq)\n",
    "\n",
    "print(f\"✅ Loaded {len(fasta_seqs):,} OpenProtein Set sequences\")\n",
    "\n",
    "# Build quick lookup indices\n",
    "seq_to_ids = defaultdict(list)\n",
    "len_index = defaultdict(list)\n",
    "\n",
    "for fasta_id, seq in fasta_seqs.items():\n",
    "    seq_to_ids[seq].append(fasta_id)\n",
    "    len_index[len(seq)].append(fasta_id)\n",
    "\n",
    "# ── 2. COLLECT TARGET PROTEINS FROM PDBCH FOLDERS ─────────────────────────────\n",
    "print(f\"\\n🔍 Scanning PDBCH folders for {len(PROBLEMATIC_PROTEINS):,} problematic proteins...\")\n",
    "\n",
    "target_proteins = []  # List of (tilde_encoded_id, protein_folder_path, original_id, sequence)\n",
    "\n",
    "for split_name, split_folder in SPLIT_FOLDERS.items():\n",
    "    if not split_folder.exists():\n",
    "        continue\n",
    "    \n",
    "    split_found = 0\n",
    "    for tilde_encoded_id in PROBLEMATIC_PROTEINS:\n",
    "        protein_folder = split_folder / tilde_encoded_id\n",
    "        \n",
    "        if protein_folder.exists() and protein_folder.is_dir():\n",
    "            # Decode tilde to get original protein ID\n",
    "            original_id = decode_protein_id(tilde_encoded_id)\n",
    "            \n",
    "            # Read sequence from sequence.txt\n",
    "            sequence_file = protein_folder / \"sequence.txt\"\n",
    "            if sequence_file.exists():\n",
    "                try:\n",
    "                    sequence = ''.join(c for c in sequence_file.read_text().strip() if c.isalpha())\n",
    "                    target_proteins.append((tilde_encoded_id, protein_folder, original_id, sequence))\n",
    "                    split_found += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"   ❌ Error reading sequence for {tilde_encoded_id}: {e}\")\n",
    "    \n",
    "    print(f\"   📂 {split_name}: Found {split_found} proteins\")\n",
    "\n",
    "print(f\"✅ Total proteins to map: {len(target_proteins):,}\")\n",
    "\n",
    "# ── 3. HIERARCHICAL MAPPING ────────────────────────────────────────────────────\n",
    "print(f\"\\n🔄 Performing hierarchical mapping...\")\n",
    "\n",
    "results = []\n",
    "stats = Counter(stage1=0, stage2=0, stage3=0, nomatch=0, no_sequence=0)\n",
    "start_time = time.time()\n",
    "\n",
    "for tilde_encoded_id, protein_folder, original_id, sequence in tqdm(target_proteins, desc=\"Mapping proteins\", unit=\"protein\"):\n",
    "    \n",
    "    if not sequence:\n",
    "        results.append([tilde_encoded_id, \"NO_SEQUENCE\", \"NA\", \"No Sequence Available\"])\n",
    "        stats[\"no_sequence\"] += 1\n",
    "        continue\n",
    "    \n",
    "    # STAGE 1: Exact ID match after format conversion\n",
    "    formatted_id = format_protein_id_for_openfold(original_id)\n",
    "    \n",
    "    if formatted_id in fasta_seqs:\n",
    "        results.append([tilde_encoded_id, formatted_id, \"NA\", \"Stage 1 - Exact ID\"])\n",
    "        stats[\"stage1\"] += 1\n",
    "        continue\n",
    "    \n",
    "    # STAGE 2: Exact sequence match\n",
    "    matching_ids = seq_to_ids.get(sequence, [])\n",
    "    if matching_ids:\n",
    "        results.append([tilde_encoded_id, matching_ids[0], \"NA\", \"Stage 2 - Exact Sequence\"])\n",
    "        stats[\"stage2\"] += 1\n",
    "        continue\n",
    "    \n",
    "    # STAGE 3: Similarity search (±15 AA tolerance)\n",
    "    protein_length = len(sequence)\n",
    "    candidates = []\n",
    "    \n",
    "    # Collect candidates within length tolerance\n",
    "    for length in range(max(1, protein_length - LENGTH_TOLERANCE), \n",
    "                       protein_length + LENGTH_TOLERANCE + 1):\n",
    "        candidates.extend(len_index[length])\n",
    "    \n",
    "    best_id, best_similarity = None, SIMILARITY_THRESHOLD\n",
    "    \n",
    "    if candidates:\n",
    "        if MAX_THREADS > 1 and len(candidates) > 10:\n",
    "            # Parallel similarity calculation\n",
    "            args = [(sequence, fasta_id, fasta_seqs[fasta_id]) for fasta_id in candidates]\n",
    "            with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "                for fasta_id, sim_score in executor.map(similarity_worker, args):\n",
    "                    if sim_score > best_similarity:\n",
    "                        best_similarity, best_id = sim_score, fasta_id\n",
    "        else:\n",
    "            # Sequential similarity calculation\n",
    "            for fasta_id in candidates:\n",
    "                sim_score = similarity(sequence, fasta_seqs[fasta_id])\n",
    "                if sim_score > best_similarity:\n",
    "                    best_similarity, best_id = sim_score, fasta_id\n",
    "    \n",
    "    if best_id:\n",
    "        results.append([tilde_encoded_id, best_id, f\"{best_similarity:.4f}\", \"Stage 3 - Similarity\"])\n",
    "        stats[\"stage3\"] += 1\n",
    "    else:\n",
    "        results.append([tilde_encoded_id, \"NO_MATCH\", \"NA\", \"No Match Found\"])\n",
    "        stats[\"nomatch\"] += 1\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "# ── 4. WRITE RESULTS TO TSV ────────────────────────────────────────────────────\n",
    "print(f\"\\n📝 Writing results to TSV...\")\n",
    "\n",
    "with OUTPUT_TSV.open(\"w\", newline=\"\", encoding=\"utf-8\") as tsv_file:\n",
    "    writer = csv.writer(tsv_file, delimiter=\"\\t\")\n",
    "    writer.writerow([\"original_id_tilde\", \"matched_openfold_id\", \"similarity_score\", \"mapping_stage\"])\n",
    "    writer.writerows(results)\n",
    "\n",
    "# ── 5. SUMMARY REPORT ──────────────────────────────────────────────────────────\n",
    "total_processed = sum(stats.values())\n",
    "\n",
    "print(f\"\\n🎉 MAPPING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"📊 Mapping Statistics:\")\n",
    "print(f\"   Stage 1 (Exact ID)      : {stats['stage1']:3d} ({stats['stage1']/total_processed*100:5.1f}%)\")\n",
    "print(f\"   Stage 2 (Exact Sequence): {stats['stage2']:3d} ({stats['stage2']/total_processed*100:5.1f}%)\")\n",
    "print(f\"   Stage 3 (Similarity)    : {stats['stage3']:3d} ({stats['stage3']/total_processed*100:5.1f}%)\")\n",
    "print(f\"   No Sequence Available   : {stats['no_sequence']:3d} ({stats['no_sequence']/total_processed*100:5.1f}%)\")\n",
    "print(f\"   No Match Found          : {stats['nomatch']:3d} ({stats['nomatch']/total_processed*100:5.1f}%)\")\n",
    "\n",
    "successful_matches = stats['stage1'] + stats['stage2'] + stats['stage3']\n",
    "print(f\"\\n✅ Total successful matches: {successful_matches:,} / {total_processed:,} ({successful_matches/total_processed*100:.1f}%)\")\n",
    "print(f\"⏱️  Processing time: {elapsed_time:.1f} seconds\")\n",
    "print(f\"📁 Results saved to: {OUTPUT_TSV}\")\n",
    "\n",
    "# ── 6. SAMPLE RESULTS ──────────────────────────────────────────────────────────\n",
    "print(f\"\\n📋 SAMPLE MAPPING RESULTS:\")\n",
    "print(\"-\" * 70)\n",
    "for i, (tilde_id, matched_id, score, stage) in enumerate(results[:10]):\n",
    "    original_id = decode_protein_id(tilde_id)\n",
    "    formatted_id = format_protein_id_for_openfold(original_id) \n",
    "    print(f\"{original_id:8s} → {tilde_id:10s} → {formatted_id:8s} → {matched_id:15s} | {stage}\")\n",
    "\n",
    "if len(results) > 10:\n",
    "    print(f\"... and {len(results) - 10} more results in TSV file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑️  Deleting A3M Files for Problematic Proteins\n",
      "============================================================\n",
      "\n",
      "🔍 Processing train split...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9be3a7d7deff4c0aa8dc1d2c58079a6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checking train:   0%|          | 0/233 [00:00<?, ?protein/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Deleted: 5IT7-EE/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 3J79-I/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5IT7-HH/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5OPT-P/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5XY3-I/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5OQL-U/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5XXU-A/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5XY3-E/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 6AZ1-C/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 6AZ1-B/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5T5H-V/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 6GAZ-AP/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5XXB-G/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 3JD5-J/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 6HIX-BG/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5OPT-R/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5XXB-E/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5XXU-B/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5XXB-F/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 6AZ1-A/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 4V6U-BL/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5XYI-B/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 6GAZ-AN/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 6DZI-T/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5IT7-RR/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 6HIX-AV/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5T5H-I/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5XY3-F/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5YZG-W/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 3JB9-R/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5XY3-J/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 6GCS-D/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5LNK-N/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 6GCS-H/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5OOL-W/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 6GCS-J/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5XY3-O/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5V93-F/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 3JB9-I/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5OQL-X/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 4V7E-CE/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 6GCS-F/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5XXB-O/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5NGM-AO/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5IT7-CC/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 6GB2-BE/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 3JD5-F/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5XY3-G/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5XXU-C/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5T5H-L/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5T5H-W/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5XXU-F/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5IT7-II/final_filtered_256_stripped.a3m\n",
      "   📊 train: Found 208 proteins, deleted 53 A3M files\n",
      "\n",
      "🔍 Processing val split...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54861a692b6447ca861992bab0ac4dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checking val:   0%|          | 0/233 [00:00<?, ?protein/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Deleted: 4UDF-1B/final_filtered_256_stripped.a3m\n",
      "   📊 val: Found 5 proteins, deleted 1 A3M files\n",
      "\n",
      "🔍 Processing test split...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a08de451e9e2420f953daaa02f9e59da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checking test:   0%|          | 0/233 [00:00<?, ?protein/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Deleted: 3H4P-A/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 4V8T-O/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 5ZWN-Y/final_filtered_256_stripped.a3m\n",
      "   ✅ Deleted: 6GIQ-D/final_filtered_256_stripped.a3m\n",
      "   📊 test: Found 20 proteins, deleted 4 A3M files\n",
      "\n",
      "🎉 DELETION SUMMARY\n",
      "============================================================\n",
      "🔍 Total proteins checked: 233\n",
      "🗑️  Total A3M files deleted: 58\n",
      "\n",
      "✅ Successfully cleared A3M files for 58 problematic proteins\n",
      "   These proteins are now ready for fresh A3M generation\n",
      "\n",
      "Next step: Run the mapping script to identify OpenProtein Set matches\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Delete existing final_filtered_256_stripped.a3m files for specific problematic proteins\n",
    "in the PDBCH folder structure.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ── CONFIGURATION ──────────────────────────────────────────────────────────────\n",
    "PDBCH_ROOT = Path(r\"C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\PDBCH\")\n",
    "\n",
    "SPLIT_FOLDERS = {\n",
    "    \"train\": PDBCH_ROOT / \"train_pdbch\",\n",
    "    \"val\": PDBCH_ROOT / \"val_pdbch\",\n",
    "    \"test\": PDBCH_ROOT / \"test_pdbch\"\n",
    "}\n",
    "\n",
    "TARGET_FILE = \"final_filtered_256_stripped.a3m\"\n",
    "\n",
    "# List of problematic proteins (tilde-encoded folder names + additional from percentage list)\n",
    "PROBLEMATIC_PROTEINS = {\n",
    "    # Original tilde-encoded list\n",
    "    \"3H4P-a~\", \"3J79-f~\", \"3J79-i~\", \"3J7Y-f~\", \"3J92-f~\", \"3J9M-l~\", \"3JB9-g~\",\n",
    "    \"3JB9-i~\", \"3JB9-r~\", \"3JCS-d~\", \"3JCS-e~\", \"3JD5-f~\", \"3JD5-j~\", \"3JD5-n~\",\n",
    "    \"4CE4-u~\", \"4V3P-La~\", \"4V3P-Lb~\", \"4V3P-Le~\", \"4V3P-Lj~\", \"4V4N-Ad~\", \"4V4N-Aj~\",\n",
    "    \"4V6U-Be~\", \"4V6U-Bl~\", \"4V6W-Ab~\", \"4V6W-Af~\", \"4V6W-Ca~\", \"4V6W-Cb~\", \"4V6W-Cc~\",\n",
    "    \"4V6W-Cg~\", \"4V6W-Cj~\", \"4V6W-Ck~\", \"4V6W-Co~\", \"4V6W-Cr~\", \"4V7E-Ba~\", \"4V7E-Bb~\",\n",
    "    \"4V7E-Bf~\", \"4V7E-Bg~\", \"4V7E-Ce~\", \"4V7E-Ci~\", \"4V7E-Co~\", \"4V7E-Cq~\", \"4V7E-Cu~\",\n",
    "    \"4V8M-Bi~\", \"4V8M-Bj~\", \"4V8M-Bk~\", \"4V8M-Bl~\", \"4V8M-Bm~\", \"4V8M-Bp~\", \"4V8M-Br~\",\n",
    "    \"4V8M-Bs~\", \"4V8M-Bt~\", \"4V8M-Bu~\", \"4V8M-Bv~\", \"4V8M-By~\", \"5AJ4-Ak~\", \"5AJ4-Ap~\",\n",
    "    \"5GUP-l~\", \"5GUP-m~\", \"5GUP-v~\", \"5IT7-a~a~\", \"5IT7-c~c~\", \"5IT7-e~e~\", \"5IT7-f~f~\",\n",
    "    \"5IT7-h~h~\", \"5IT7-i~i~\", \"5IT7-j~j~\", \"5IT7-o~o~\", \"5IT7-p~p~\", \"5IT7-r~r~\", \"5KZ5-a~\",\n",
    "    \"5L9W-b~\", \"5LJ5-s~\", \"5LNK-h~\", \"5LNK-l~\", \"5LNK-m~\", \"5LNK-n~\", \"5MMM-i~\", \"5NGM-Ao~\",\n",
    "    \"5O31-m~\", \"5OOL-w~\", \"5OPT-l~\", \"5OPT-p~\", \"5OPT-r~\", \"5OPT-t~\", \"5OQL-e~\", \"5OQL-h~\",\n",
    "    \"5OQL-l~\", \"5OQL-p~\", \"5OQL-u~\", \"5OQL-v~\", \"5OQL-x~\", \"5OQL-y~\", \"5T2A-n~\", \"5T2A-v~\",\n",
    "    \"5T5H-i~\", \"5T5H-l~\", \"5T5H-n~\", \"5T5H-t~\", \"5T5H-v~\", \"5T5H-w~\", \"5V93-e~\", \"5V93-f~\",\n",
    "    \"5V93-n~\", \"5V93-p~\", \"5V93-q~\", \"5VK2-a~\", \"5XXB-b~\", \"5XXB-c~\", \"5XXB-e~\", \"5XXB-f~\",\n",
    "    \"5XXB-g~\", \"5XXB-h~\", \"5XXB-i~\", \"5XXB-n~\", \"5XXB-o~\", \"5XXB-p~\", \"5XXU-a~\", \"5XXU-b~\",\n",
    "    \"5XXU-c~\", \"5XXU-f~\", \"5XY3-c~\", \"5XY3-d~\", \"5XY3-e~\", \"5XY3-f~\", \"5XY3-g~\", \"5XY3-h~\",\n",
    "    \"5XY3-i~\", \"5XY3-j~\", \"5XY3-m~\", \"5XY3-o~\", \"5XY3-p~\", \"5XYI-a~\", \"5XYI-b~\", \"5XYI-c~\",\n",
    "    \"5XYI-e~\", \"5YZG-w~\", \"5ZWN-y~\", \"6AZ1-a~\", \"6AZ1-b~\", \"6AZ1-c~\", \"6AZ1-e~\", \"6AZ3-d~\",\n",
    "    \"6AZ3-h~\", \"6DZI-t~\", \"6DZI-z~\", \"6ERI-Az~\", \"6G2J-e~\", \"6G72-b~\", \"6G72-g~\", \"6G72-h~\",\n",
    "    \"6G72-m~\", \"6GAZ-An~\", \"6GAZ-Ao~\", \"6GAZ-Ap~\", \"6GB2-Be~\", \"6GB2-Bw~\", \"6GCS-c~\", \"6GCS-d~\",\n",
    "    \"6GCS-f~\", \"6GCS-h~\", \"6GCS-j~\", \"6GIQ-d~\", \"6GIQ-e~\", \"6GIQ-h~\", \"6HA1-p~\", \"6HA8-d~\",\n",
    "    \"6HHQ-p~\", \"6HIV-Bb~\", \"6HIV-Bc~\", \"6HIX-Av~\", \"6HIX-Bg~\", \"6HIZ-Ci~\", \"6OKK-b~\", \"6OKK-c~\",\n",
    "    \"6QDV-d~\",\n",
    "    \n",
    "    # Additional proteins from percentage list (uppercase, no tilde encoding needed)\n",
    "    \"3H4P-A\", \"3J79-I\", \"3JB9-I\", \"3JB9-R\", \"3JD5-F\", \"3JD5-J\", \"4UDF-1B\", \"4V6U-BL\", \n",
    "    \"4V7E-CE\", \"4V8T-O\", \"5IT7-CC\", \"5IT7-EE\", \"5IT7-HH\", \"5IT7-II\", \"5IT7-RR\", \"5LNK-N\", \n",
    "    \"5NGM-AO\", \"5OOL-W\", \"5OPT-P\", \"5OPT-R\", \"5OQL-U\", \"5OQL-X\", \"5T5H-I\", \"5T5H-L\", \n",
    "    \"5T5H-V\", \"5T5H-W\", \"5V93-F\", \"5XXB-E\", \"5XXB-F\", \"5XXB-G\", \"5XXB-O\", \"5XXU-A\", \n",
    "    \"5XXU-B\", \"5XXU-C\", \"5XXU-F\", \"5XY3-E\", \"5XY3-F\", \"5XY3-G\", \"5XY3-I\", \"5XY3-J\", \n",
    "    \"5XY3-O\", \"5XYI-B\", \"5YZG-W\", \"5ZWN-Y\", \"6AZ1-A\", \"6AZ1-B\", \"6AZ1-C\", \"6DZI-T\", \n",
    "    \"6GAZ-AN\", \"6GAZ-AP\", \"6GB2-BE\", \"6GCS-D\", \"6GCS-F\", \"6GCS-H\", \"6GCS-J\", \"6GIQ-D\", \n",
    "    \"6HIX-AV\", \"6HIX-BG\"\n",
    "}\n",
    "\n",
    "print(\"🗑️  Deleting A3M Files for Problematic Proteins\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ── SCAN AND DELETE ────────────────────────────────────────────────────────────\n",
    "total_deleted = 0\n",
    "total_checked = 0\n",
    "\n",
    "for split_name, split_folder in SPLIT_FOLDERS.items():\n",
    "    if not split_folder.exists():\n",
    "        print(f\"❌ Split folder not found: {split_folder}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n🔍 Processing {split_name} split...\")\n",
    "    \n",
    "    split_deleted = 0\n",
    "    split_found = 0\n",
    "    \n",
    "    # Check each problematic protein in this split\n",
    "    for protein_id in tqdm(PROBLEMATIC_PROTEINS, desc=f\"Checking {split_name}\", unit=\"protein\"):\n",
    "        total_checked += 1\n",
    "        \n",
    "        # Look for tilde-encoded folder\n",
    "        protein_folder = split_folder / protein_id\n",
    "        \n",
    "        if protein_folder.exists() and protein_folder.is_dir():\n",
    "            split_found += 1\n",
    "            a3m_file = protein_folder / TARGET_FILE\n",
    "            \n",
    "            if a3m_file.exists():\n",
    "                try:\n",
    "                    # Delete the A3M file\n",
    "                    a3m_file.unlink()\n",
    "                    split_deleted += 1\n",
    "                    total_deleted += 1\n",
    "                    print(f\"   ✅ Deleted: {protein_id}/{TARGET_FILE}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   ❌ Error deleting {protein_id}/{TARGET_FILE}: {e}\")\n",
    "            # If A3M doesn't exist, that's fine - we wanted to delete it anyway\n",
    "    \n",
    "    print(f\"   📊 {split_name}: Found {split_found} proteins, deleted {split_deleted} A3M files\")\n",
    "\n",
    "# ── SUMMARY ────────────────────────────────────────────────────────────────────\n",
    "print(f\"\\n🎉 DELETION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"🔍 Total proteins checked: {len(PROBLEMATIC_PROTEINS):,}\")\n",
    "print(f\"🗑️  Total A3M files deleted: {total_deleted:,}\")\n",
    "\n",
    "if total_deleted > 0:\n",
    "    print(f\"\\n✅ Successfully cleared A3M files for {total_deleted} problematic proteins\")\n",
    "    print(\"   These proteins are now ready for fresh A3M generation\")\n",
    "else:\n",
    "    print(f\"\\n📝 No A3M files found to delete - proteins may already be cleared\")\n",
    "\n",
    "print(\"\\nNext step: Run the mapping script to identify OpenProtein Set matches\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧬 PDBCH MSA Fetch and HHFilter Pipeline\n",
      "============================================================\n",
      "🔄 Loading mapping TSV …\n",
      "📋 Loaded 233 protein mappings\n",
      "✅ Total proteins to process: 233\n",
      "⏭️  Already completed: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d75992e83a1b44fe8c12e169d21e8cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing proteins:   0%|          | 0/233 [00:00<?, ?prot/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc43be6b470148ca9d610279b4d9452d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 1 downloads:   0%|          | 0/150 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   📥 Downloaded 150/150 A3M files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c30f87d8f2405ba6da332b4d64ee16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 1 hhfilter:   0%|          | 0/50 [00:00<?, ?prot/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Batch 1: ✅ 50 Success | ⏭️ 0 Skipped | ❌ 0 Failed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c57c6f66eb3454fb11faf3b1aad4493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 2 downloads:   0%|          | 0/150 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   📥 Downloaded 150/150 A3M files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a313e6ffeeb4b9f99e769ebde72d28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 2 hhfilter:   0%|          | 0/50 [00:00<?, ?prot/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Batch 2: ✅ 50 Success | ⏭️ 0 Skipped | ❌ 0 Failed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2b520d879d4584aa23aca3308fbeeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 3 downloads:   0%|          | 0/150 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   📥 Downloaded 148/150 A3M files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7251d0c61471427887b7f52623f3935c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 3 hhfilter:   0%|          | 0/50 [00:00<?, ?prot/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Batch 3: ✅ 50 Success | ⏭️ 0 Skipped | ❌ 0 Failed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b3d7c96879c4a32862e90fa71aeedef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 4 downloads:   0%|          | 0/150 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   📥 Downloaded 150/150 A3M files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3954a2a5c3df42ebbaf604199f411c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 4 hhfilter:   0%|          | 0/50 [00:00<?, ?prot/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Batch 4: ✅ 50 Success | ⏭️ 0 Skipped | ❌ 0 Failed\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd89d78dc214228a2908f7b692daecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 5 downloads:   0%|          | 0/99 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   📥 Downloaded 99/99 A3M files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f3d9b16868437fb8ae08e68dacc498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batch 5 hhfilter:   0%|          | 0/33 [00:00<?, ?prot/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Batch 5: ✅ 33 Success | ⏭️ 0 Skipped | ❌ 0 Failed\n",
      "\n",
      "🎉 Pipeline completed!\n",
      "📁 Results saved to: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\problematic_proteins_openfold_mapping_updated.tsv\n",
      "\n",
      "📊 Final Statistics:\n",
      "   ✅ Success: 233 (100.0%)\n",
      "   ⏭️  Skipped: 0 (0.0%)\n",
      "   ❌ Failed:  0 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Resumable MSA-fetch + hhfilter pipeline for PDBCH problematic proteins\n",
    "====================================================================\n",
    "\n",
    "Input TSV  : problematic_proteins_openfold_mapping.tsv   (original_id_tilde, matched_openfold_id, ...)\n",
    "Output TSV : problematic_proteins_openfold_mapping_updated.tsv  (+ status columns)\n",
    "A3M output : <PDBCH>/<split>/<TILDE_ENCODED_ID>/final_filtered_256_stripped.a3m\n",
    "\n",
    "Concurrency\n",
    "-----------\n",
    "• BATCH_SIZE      – chains processed per outer loop\n",
    "• DL_CONCURRENCY  – parallel S3 downloads (IO-bound)\n",
    "• HH_PARALLEL     – parallel hhfilter+diversity jobs (CPU-bound)\n",
    "\n",
    "The script is *idempotent*; rerun to resume unfinished work.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import csv, os, re, shutil, subprocess, tempfile, itertools, math, time\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import List, Tuple, Dict, Set\n",
    "from collections import defaultdict\n",
    "\n",
    "import boto3\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "from botocore.config import Config\n",
    "from botocore import UNSIGNED\n",
    "\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ─── PATHS & CONSTANTS ──────────────────────────────────────────────────\n",
    "ROOT        = Path(r\"C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\")\n",
    "PDBCH_ROOT  = ROOT / \"data\" / \"PDBCH\"\n",
    "\n",
    "# PDBCH folder structure (tilde-encoded folders)\n",
    "SPLITS = {\n",
    "    \"train\": PDBCH_ROOT / \"train_pdbch\",\n",
    "    \"val\": PDBCH_ROOT / \"val_pdbch\",\n",
    "    \"test\": PDBCH_ROOT / \"test_pdbch\"\n",
    "}\n",
    "\n",
    "MATCH_FILE   = ROOT / \"problematic_proteins_openfold_mapping.tsv\"\n",
    "UPDATED_OUT  = ROOT / \"problematic_proteins_openfold_mapping_updated.tsv\"\n",
    "\n",
    "MAX_ROWS       = 256          # target rows post-hhfilter\n",
    "BATCH_SIZE     = 50           # chains per outer loop batch\n",
    "DL_CONCURRENCY = 14           # parallel S3 downloads\n",
    "HH_PARALLEL    = 10           # parallel hhfilter jobs\n",
    "\n",
    "BUCKET      = \"openfold\"\n",
    "A3M_FILES   = (\"bfd_uniclust_hits.a3m\", \"mgnify_hits.a3m\", \"uniref90_hits.a3m\")\n",
    "\n",
    "# ─── UTILS ──────────────────────────────────────────────────────────────\n",
    "LOWER = ''.join(chr(c) for c in range(97, 123))\n",
    "\n",
    "def parse_chain_id(s: str) -> Tuple[str, str]:\n",
    "    \"\"\"Parse OpenFold chain ID to get PDB and chain parts\"\"\"\n",
    "    s = s.strip().replace('_', '-')\n",
    "    if '-' in s:\n",
    "        pdb, chain = s.split('-', 1)\n",
    "    elif re.fullmatch(r\"[0-9][A-Za-z0-9]{3}[A-Za-z0-9]{1,3}\", s):\n",
    "        pdb, chain = s[:4], s[4:]\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot parse chain ID from '{s}'\")\n",
    "    return pdb.upper(), chain     # pdb upper-case, chain left *as-is*\n",
    "\n",
    "def to_wsl(path: Path) -> str:\n",
    "    \"\"\"Convert Windows path to WSL path\"\"\"\n",
    "    posix = path.resolve().as_posix()\n",
    "    return posix if posix.startswith(\"/mnt/\") else f\"/mnt/{path.drive[0].lower()}{posix[2:]}\"\n",
    "\n",
    "def strip_insertions_a3m(seq: str) -> str:\n",
    "    \"\"\"Remove lowercase insertions and dots from A3M sequence\"\"\"\n",
    "    return seq.translate({ord(c): None for c in LOWER + \".*\"})\n",
    "\n",
    "def load_msa_from_a3m(p: Path):\n",
    "    \"\"\"Load MSA from A3M file\"\"\"\n",
    "    msa, hdr, buf = [], None, []\n",
    "    with p.open() as fh:\n",
    "        for ln in fh:\n",
    "            ln = ln.rstrip()\n",
    "            if ln.startswith('>'):\n",
    "                if hdr is not None:\n",
    "                    msa.append((hdr, ''.join(buf)))\n",
    "                hdr, buf = ln[1:], []\n",
    "            else:\n",
    "                buf.append(ln)\n",
    "        if hdr is not None:\n",
    "            msa.append((hdr, ''.join(buf)))\n",
    "    return msa\n",
    "\n",
    "def diversity_max_subsample(msa, k):\n",
    "    \"\"\"Subsample MSA to k sequences using maximum diversity\"\"\"\n",
    "    if len(msa) <= k:\n",
    "        return msa\n",
    "    seqs = np.array([list(s) for _, s in msa], dtype='U1')\n",
    "    uniq = {aa: i for i, aa in enumerate(sorted({c for row in seqs for c in row}))}\n",
    "    arr  = np.vectorize(uniq.get)(seqs)\n",
    "    dist = squareform(pdist(arr, metric='hamming'))\n",
    "    keep = [0]\n",
    "    sel  = np.zeros(len(msa), bool); sel[0] = True\n",
    "    while sel.sum() < k:\n",
    "        mean = dist[:, sel].mean(1); mean[sel] = -1\n",
    "        idx  = int(mean.argmax())\n",
    "        if mean[idx] <= 0:\n",
    "            break\n",
    "        sel[idx] = True; keep.append(idx)\n",
    "    return [msa[i] for i in keep]\n",
    "\n",
    "# ─── RESUMABILITY ───────────────────────────────────────────────────────\n",
    "def load_existing() -> Set[str]:\n",
    "    \"\"\"Load already processed proteins from output TSV\"\"\"\n",
    "    if not UPDATED_OUT.exists():\n",
    "        return set()\n",
    "    done = set()\n",
    "    with UPDATED_OUT.open(newline='') as fh:\n",
    "        for row in csv.DictReader(fh, delimiter='\\t'):\n",
    "            if row.get('status', '') == 'Success':\n",
    "                done.add(row['original_id_tilde'])\n",
    "    return done\n",
    "\n",
    "def append_results(rows: List[Dict[str,str]], first: bool):\n",
    "    \"\"\"Append results to output TSV\"\"\"\n",
    "    mode = 'w' if first else 'a'\n",
    "    with UPDATED_OUT.open(mode, newline='') as fh:\n",
    "        wr = csv.DictWriter(fh, delimiter='\\t', fieldnames=rows[0].keys())\n",
    "        if first:\n",
    "            wr.writeheader()\n",
    "        wr.writerows(rows)\n",
    "\n",
    "# ─── AWS S3 ─────────────────────────────────────────────────────────────\n",
    "transfer_cfg = TransferConfig(max_concurrency=DL_CONCURRENCY)\n",
    "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "def download_one(key: str, dest: Path) -> bool:\n",
    "    \"\"\"Download single file from S3\"\"\"\n",
    "    try:\n",
    "        dest.parent.mkdir(parents=True, exist_ok=True)\n",
    "        s3.download_file(BUCKET, key, str(dest), Config=transfer_cfg)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# ─── MAIN PER-CHAIN PROCESSOR ───────────────────────────────────────────\n",
    "def process_chain(chain_dir: Path, row: Dict[str,str]) -> Dict[str,str]:\n",
    "    \"\"\"Process single protein chain: download MSAs, run hhfilter, save result\"\"\"\n",
    "    try:\n",
    "        matched_id = row['matched_openfold_id']\n",
    "        \n",
    "        # Skip if no match found\n",
    "        if matched_id in ('NO_MATCH', 'NO_SEQUENCE'):\n",
    "            return {**row, **{\n",
    "                'n_rows_dropped': '', 'hhfilter_rows': '',\n",
    "                'diversity_filtered_rows': '', 'status': f'Skipped: {matched_id}'\n",
    "            }}\n",
    "        \n",
    "        pdb, chain = parse_chain_id(matched_id)\n",
    "        tmp_dir = chain_dir / \"_tmp_a3m\"\n",
    "        tmp_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        # Check if we downloaded A3M files\n",
    "        downloaded = [tmp_dir / f for f in A3M_FILES if (tmp_dir / f).exists()]\n",
    "        if not downloaded:\n",
    "            return {**row, **{\n",
    "                'n_rows_dropped': '', 'hhfilter_rows': '',\n",
    "                'diversity_filtered_rows': '', 'status': 'Failed: download'\n",
    "            }}\n",
    "\n",
    "        # Concatenate raw A3Ms\n",
    "        raw = tmp_dir / \"concat_raw.a3m\"\n",
    "        with raw.open('w') as out:\n",
    "            for f in downloaded:\n",
    "                out.write(f.read_text())\n",
    "\n",
    "        # Run hhfilter\n",
    "        hh_out = tmp_dir / f\"hhfiltered_{MAX_ROWS}.a3m\"\n",
    "        subprocess.run([\n",
    "            \"wsl\", \"hhfilter\", \"-i\", to_wsl(raw), \"-o\", to_wsl(hh_out),\n",
    "            \"-diff\", str(MAX_ROWS)\n",
    "        ], check=True, capture_output=True)\n",
    "\n",
    "        # Load MSA and check if empty\n",
    "        msa = load_msa_from_a3m(hh_out)\n",
    "        hh_rows = len(msa)\n",
    "        if not msa:\n",
    "            return {**row, **{\n",
    "                'n_rows_dropped': '', 'hhfilter_rows': hh_rows,\n",
    "                'diversity_filtered_rows': '', 'status': 'Failed: hhfilter-empty'\n",
    "            }}\n",
    "\n",
    "        # Strip insertions and filter by length\n",
    "        tgt_len = len(strip_insertions_a3m(msa[0][1]))\n",
    "        kept, dropped = [], 0\n",
    "        for hdr, seq in msa:\n",
    "            clean = strip_insertions_a3m(seq)\n",
    "            if len(clean) == tgt_len:\n",
    "                kept.append((hdr, clean))\n",
    "            else:\n",
    "                dropped += 1\n",
    "                \n",
    "        if len(kept) < 2:\n",
    "            return {**row, **{\n",
    "                'n_rows_dropped': dropped, 'hhfilter_rows': hh_rows,\n",
    "                'diversity_filtered_rows': '',\n",
    "                'status': f'Failed: {len(kept)} rows post-strip'\n",
    "            }}\n",
    "\n",
    "        # Apply diversity filtering if needed\n",
    "        div_filtered = 0\n",
    "        if len(kept) > MAX_ROWS:\n",
    "            before = len(kept)\n",
    "            kept = diversity_max_subsample(kept, MAX_ROWS)\n",
    "            div_filtered = before - len(kept)\n",
    "\n",
    "        # Save final A3M file to tilde-encoded folder\n",
    "        final_path = chain_dir / \"final_filtered_256_stripped.a3m\"\n",
    "        with final_path.open('w') as fh:\n",
    "            for h, s in kept:\n",
    "                fh.write(f\">{h}\\n{s}\\n\")\n",
    "\n",
    "        return {**row, **{\n",
    "            'n_rows_dropped': dropped,\n",
    "            'hhfilter_rows': hh_rows,\n",
    "            'diversity_filtered_rows': div_filtered,\n",
    "            'status': 'Success'\n",
    "        }}\n",
    "\n",
    "    except Exception as e:\n",
    "        return {**row, **{\n",
    "            'n_rows_dropped': '', 'hhfilter_rows': '',\n",
    "            'diversity_filtered_rows': '',\n",
    "            'status': f'Failed: {str(e)[:80]}'\n",
    "        }}\n",
    "    finally:\n",
    "        shutil.rmtree(chain_dir / \"_tmp_a3m\", ignore_errors=True)\n",
    "\n",
    "# ─── DRIVER ─────────────────────────────────────────────────────────────\n",
    "def find_protein_folder(tilde_encoded_id: str) -> Path | None:\n",
    "    \"\"\"Find protein folder in PDBCH structure using tilde-encoded ID\"\"\"\n",
    "    for split_folder in SPLITS.values():\n",
    "        protein_folder = split_folder / tilde_encoded_id\n",
    "        if protein_folder.exists() and protein_folder.is_dir():\n",
    "            return protein_folder\n",
    "    return None\n",
    "\n",
    "def grouper(n: int, it):\n",
    "    \"\"\"Group iterator into batches of size n\"\"\"\n",
    "    it = iter(it)\n",
    "    while True:\n",
    "        chunk = list(itertools.islice(it, n))\n",
    "        if not chunk: \n",
    "            return\n",
    "        yield chunk\n",
    "\n",
    "def main():\n",
    "    print(\"🧬 PDBCH MSA Fetch and HHFilter Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(\"🔄 Loading mapping TSV …\")\n",
    "    if not MATCH_FILE.exists():\n",
    "        raise FileNotFoundError(f\"Mapping file not found: {MATCH_FILE}\")\n",
    "    \n",
    "    mapping = {}\n",
    "    with MATCH_FILE.open(newline='') as fh:\n",
    "        reader = csv.DictReader(fh, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            mapping[row['original_id_tilde']] = row\n",
    "\n",
    "    print(f\"📋 Loaded {len(mapping):,} protein mappings\")\n",
    "\n",
    "    # Load already processed proteins\n",
    "    finished = load_existing()\n",
    "    todo = [pid for pid in mapping if pid not in finished]\n",
    "\n",
    "    # Verify hhfilter is available\n",
    "    if subprocess.run([\"wsl\", \"which\", \"hhfilter\"], capture_output=True).returncode:\n",
    "        raise RuntimeError(\"❌ hhfilter not found inside WSL. Please install HH-suite in WSL.\")\n",
    "\n",
    "    print(f\"✅ Total proteins to process: {len(todo):,}\")\n",
    "    print(f\"⏭️  Already completed: {len(finished):,}\")\n",
    "    \n",
    "    if not todo:\n",
    "        print(\"🎉 Nothing to do – all done!\")\n",
    "        return\n",
    "\n",
    "    is_first_batch = (not UPDATED_OUT.exists())\n",
    "    outer = tqdm(total=len(todo), desc=\"Processing proteins\", unit=\"prot\")\n",
    "\n",
    "    for batch_num, batch_pids in enumerate(grouper(BATCH_SIZE, todo), 1):\n",
    "        # Stage 1: Prepare downloads and validate folders\n",
    "        dl_jobs, valid_dirs, batch_results = [], [], []\n",
    "\n",
    "        for tilde_encoded_id in batch_pids:\n",
    "            row = mapping[tilde_encoded_id]\n",
    "            \n",
    "            # Find protein folder in PDBCH structure\n",
    "            protein_folder = find_protein_folder(tilde_encoded_id)\n",
    "            if protein_folder is None:\n",
    "                batch_results.append({**row, **{\n",
    "                    'n_rows_dropped': '', 'hhfilter_rows': '',\n",
    "                    'diversity_filtered_rows': '',\n",
    "                    'status': 'Failed: folder-not-found'\n",
    "                }})\n",
    "                outer.update(1)\n",
    "                continue\n",
    "\n",
    "            # Skip if no valid match\n",
    "            matched_id = row['matched_openfold_id']\n",
    "            if matched_id in ('NO_MATCH', 'NO_SEQUENCE'):\n",
    "                batch_results.append({**row, **{\n",
    "                    'n_rows_dropped': '', 'hhfilter_rows': '',\n",
    "                    'diversity_filtered_rows': '',\n",
    "                    'status': f'Skipped: {matched_id}'\n",
    "                }})\n",
    "                outer.update(1)\n",
    "                continue\n",
    "\n",
    "            # Prepare download jobs\n",
    "            try:\n",
    "                pdb, chain = parse_chain_id(matched_id)\n",
    "                tmp_dir = protein_folder / \"_tmp_a3m\"\n",
    "                for fname in A3M_FILES:\n",
    "                    key = f\"pdb/{pdb.lower()}_{chain}/a3m/{fname}\"\n",
    "                    dl_jobs.append((key, tmp_dir / fname))\n",
    "                valid_dirs.append(protein_folder)\n",
    "            except Exception as e:\n",
    "                batch_results.append({**row, **{\n",
    "                    'n_rows_dropped': '', 'hhfilter_rows': '',\n",
    "                    'diversity_filtered_rows': '',\n",
    "                    'status': f'Failed: parse-id ({str(e)[:50]})'\n",
    "                }})\n",
    "                outer.update(1)\n",
    "                continue\n",
    "\n",
    "        # Stage 2: Download A3M files from S3\n",
    "        if dl_jobs:\n",
    "            with ThreadPoolExecutor(max_workers=DL_CONCURRENCY) as pool:\n",
    "                futs = [pool.submit(download_one, k, d) for k, d in dl_jobs]\n",
    "                successful_downloads = 0\n",
    "                for fut in tqdm(as_completed(futs), total=len(futs),\n",
    "                              desc=f\"Batch {batch_num} downloads\", leave=False, unit=\"file\"):\n",
    "                    if fut.result():\n",
    "                        successful_downloads += 1\n",
    "                \n",
    "                print(f\"   📥 Downloaded {successful_downloads}/{len(dl_jobs)} A3M files\")\n",
    "\n",
    "        # Stage 3: Process with hhfilter and diversity filtering\n",
    "        if valid_dirs:\n",
    "            with ThreadPoolExecutor(max_workers=HH_PARALLEL) as pool:\n",
    "                futs2 = {pool.submit(process_chain, d, mapping[d.name]): d for d in valid_dirs}\n",
    "                for fut in tqdm(as_completed(futs2), total=len(futs2),\n",
    "                                desc=f\"Batch {batch_num} hhfilter\", leave=False, unit=\"prot\"):\n",
    "                    res = fut.result()\n",
    "                    batch_results.append(res)\n",
    "                    outer.update(1)\n",
    "                    \n",
    "                    # Show current status\n",
    "                    status_short = res['status'].split(':')[0] if ':' in res['status'] else res['status']\n",
    "                    outer.set_postfix_str(f\"{res['original_id_tilde'][:12]} ({status_short})\")\n",
    "\n",
    "        # Stage 4: Save batch results\n",
    "        if batch_results:\n",
    "            append_results(batch_results, is_first_batch)\n",
    "            is_first_batch = False\n",
    "            \n",
    "            # Count successes\n",
    "            succ = sum(r['status'] == \"Success\" for r in batch_results)\n",
    "            skipped = sum(r['status'].startswith(\"Skipped\") for r in batch_results)\n",
    "            failed = len(batch_results) - succ - skipped\n",
    "            \n",
    "            print(f\"📝 Batch {batch_num}: ✅ {succ} Success | ⏭️ {skipped} Skipped | ❌ {failed} Failed\")\n",
    "\n",
    "    outer.close()\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n🎉 Pipeline completed!\")\n",
    "    print(f\"📁 Results saved to: {UPDATED_OUT}\")\n",
    "    \n",
    "    # Show final statistics\n",
    "    if UPDATED_OUT.exists():\n",
    "        final_stats = {'Success': 0, 'Skipped': 0, 'Failed': 0}\n",
    "        with UPDATED_OUT.open(newline='') as fh:\n",
    "            for row in csv.DictReader(fh, delimiter='\\t'):\n",
    "                status = row.get('status', 'Unknown')\n",
    "                if status == 'Success':\n",
    "                    final_stats['Success'] += 1\n",
    "                elif status.startswith('Skipped'):\n",
    "                    final_stats['Skipped'] += 1\n",
    "                else:\n",
    "                    final_stats['Failed'] += 1\n",
    "        \n",
    "        total = sum(final_stats.values())\n",
    "        print(f\"\\n📊 Final Statistics:\")\n",
    "        print(f\"   ✅ Success: {final_stats['Success']:,} ({final_stats['Success']/total*100:.1f}%)\")\n",
    "        print(f\"   ⏭️  Skipped: {final_stats['Skipped']:,} ({final_stats['Skipped']/total*100:.1f}%)\")\n",
    "        print(f\"   ❌ Failed:  {final_stats['Failed']:,} ({final_stats['Failed']/total*100:.1f}%)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for protein: 3J92-F\n",
      "--------------------------------------------------\n",
      "Searching in protein_test_pdb...\n",
      "  ✗ Not found in protein_test_pdb\n",
      "Searching in protein_train_pdb...\n",
      "  ✓ FOUND in protein_train_pdb:\n",
      "    C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\3J92-F\n",
      "Searching in protein_val_pdb...\n",
      "  ✗ Not found in protein_val_pdb\n",
      "\n",
      "Search complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Base path to your protein folders\n",
    "base_path = r\"C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\"\n",
    "\n",
    "# Protein folders to search\n",
    "folders = [\"protein_test_pdb\", \"protein_train_pdb\", \"protein_val_pdb\"]\n",
    "\n",
    "# Protein ID to find\n",
    "target_protein = \"3J92-F\"\n",
    "\n",
    "print(f\"Searching for protein: {target_protein}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "found = False\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(base_path, folder)\n",
    "    if os.path.exists(folder_path):\n",
    "        print(f\"Searching in {folder}...\")\n",
    "        \n",
    "        # Search for the protein ID in folder names/files\n",
    "        search_pattern = os.path.join(folder_path, f\"*{target_protein}*\")\n",
    "        matches = glob.glob(search_pattern)\n",
    "        \n",
    "        if matches:\n",
    "            print(f\"  ✓ FOUND in {folder}:\")\n",
    "            for match in matches:\n",
    "                print(f\"    {match}\")\n",
    "            found = True\n",
    "        else:\n",
    "            print(f\"  ✗ Not found in {folder}\")\n",
    "    else:\n",
    "        print(f\"  ! Folder {folder} doesn't exist\")\n",
    "\n",
    "if not found:\n",
    "    print(f\"\\nProtein {target_protein} was not found in any folder.\")\n",
    "else:\n",
    "    print(f\"\\nSearch complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Re-running MSA pipeline for 28 proteins\n",
      "============================================================\n",
      "\n",
      "📊 Status:\n",
      "  - To process: 28/28\n",
      "\n",
      "🧹 Cleaning old A3M files...\n",
      "  🔍 Looking for: 4V3P-LB\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\4V3P-LB\n",
      "  🔍 Looking for: 4V6W-CR\n",
      "    ✅ Found in test: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_test_pdb\\4V6W-CR\n",
      "  🔍 Looking for: 4UDF-1B\n",
      "    ✅ Found in val: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_val_pdb\\4UDF-1B\n",
      "  🔍 Looking for: 5LJ5-S\n",
      "    ✅ Found in test: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_test_pdb\\5LJ5-S\n",
      "  🔍 Looking for: 5AJ4-AK\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\5AJ4-AK\n",
      "  🔍 Looking for: 4V6W-CC\n",
      "    ✅ Found in test: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_test_pdb\\4V6W-CC\n",
      "  🔍 Looking for: 4V8T-O\n",
      "    ✅ Found in test: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_test_pdb\\4V8T-O\n",
      "  🔍 Looking for: 4V3P-LJ\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\4V3P-LJ\n",
      "  🔍 Looking for: 6HIV-BC\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\6HIV-BC\n",
      "  🔍 Looking for: 4V7E-BG\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\4V7E-BG\n",
      "  🔍 Looking for: 4V3P-LA\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\4V3P-LA\n",
      "  🔍 Looking for: 4V6W-CB\n",
      "    ✅ Found in test: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_test_pdb\\4V6W-CB\n",
      "  🔍 Looking for: 3JB9-G\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\3JB9-G\n",
      "  🔍 Looking for: 4V8M-BS\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\4V8M-BS\n",
      "  🔍 Looking for: 6HIV-BB\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\6HIV-BB\n",
      "  🔍 Looking for: 4V8M-BV\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\4V8M-BV\n",
      "  🔍 Looking for: 5AJ4-AP\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\5AJ4-AP\n",
      "  🔍 Looking for: 3J92-F\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\3J92-F\n",
      "  🔍 Looking for: 4V6W-CG\n",
      "    ✅ Found in test: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_test_pdb\\4V6W-CG\n",
      "  🔍 Looking for: 4V3P-LE\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\4V3P-LE\n",
      "  🔍 Looking for: 4V7E-CU\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\4V7E-CU\n",
      "  🔍 Looking for: 4V8M-BU\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\4V8M-BU\n",
      "  🔍 Looking for: 5T2A-V\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\5T2A-V\n",
      "  🔍 Looking for: 4V6W-AF\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\4V6W-AF\n",
      "  🔍 Looking for: 5OPT-L\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\5OPT-L\n",
      "  🔍 Looking for: 5T2A-N\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\5T2A-N\n",
      "  🔍 Looking for: 4V8M-BR\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\4V8M-BR\n",
      "  🔍 Looking for: 4V8M-BK\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\4V8M-BK\n",
      "\n",
      "🚀 Processing 28 proteins...\n",
      "  🔍 Looking for: 4V3P-LB\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\4V3P-LB\n",
      "\n",
      "📋 Processing 4V3P-LB → 4v3p_LB\n",
      "  🔍 Looking for: 4V6W-CR\n",
      "    ✓ Downloaded bfd_uniclust_hits.a3m\n",
      "    ✓ Downloaded mgnify_hits.a3m\n",
      "    ✅ Found in test: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_test_pdb\\4V6W-CR\n",
      "\n",
      "📋 Processing 4V6W-CR → 4v6w_CR\n",
      "  🔍 Looking for: 4UDF-1B\n",
      "    ✓ Downloaded uniref90_hits.a3m\n",
      "    ✓ Downloaded bfd_uniclust_hits.a3m\n",
      "    ✓ Downloaded mgnify_hits.a3m\n",
      "    ✓ Downloaded uniref90_hits.a3m\n",
      "    ✅ Found in val: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_val_pdb\\4UDF-1B\n",
      "\n",
      "📋 Processing 4UDF-1B → 5mqc_B\n",
      "  🔍 Looking for: 5LJ5-S\n",
      "    ✓ Downloaded bfd_uniclust_hits.a3m\n",
      "    ✓ Downloaded mgnify_hits.a3m\n",
      "    ✓ Downloaded uniref90_hits.a3m\n",
      "    📊 hhfilter output: 444 rows\n",
      "    🧹 Stripped: 444 kept, 0 dropped\n",
      "    🎯 Diversity filtered: 444 → 256\n",
      "    ✅ Success! Final MSA: 256 rows\n",
      "    ✅ Found in test: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_test_pdb\\5LJ5-S\n",
      "  🔍 Looking for: 5AJ4-AK\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\5AJ4-AK\n",
      "\n",
      "📋 Processing 5LJ5-S → 5lj5_S\n",
      "\n",
      "📋 Processing 5AJ4-AK → 5aj4_AK\n",
      "  🔍 Looking for: 4V6W-CC\n",
      "    📊 hhfilter output: 442 rows\n",
      "    🧹 Stripped: 442 kept, 0 dropped\n",
      "    🎯 Diversity filtered: 442 → 256\n",
      "    ✅ Success! Final MSA: 256 rows\n",
      "    ✓ Downloaded bfd_uniclust_hits.a3m\n",
      "    ✓ Downloaded mgnify_hits.a3m\n",
      "    ✓ Downloaded bfd_uniclust_hits.a3m\n",
      "    ✓ Downloaded mgnify_hits.a3m\n",
      "    ✓ Downloaded uniref90_hits.a3m\n",
      "    ✓ Downloaded uniref90_hits.a3m\n",
      "    ✅ Found in test: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_test_pdb\\4V6W-CC\n",
      "  🔍 Looking for: 4V8T-O\n",
      "\n",
      "📋 Processing 4V6W-CC → 4v6w_CC\n",
      "    ✓ Downloaded bfd_uniclust_hits.a3m\n",
      "    📊 hhfilter output: 462 rows\n",
      "    🧹 Stripped: 462 kept, 0 dropped\n",
      "    🎯 Diversity filtered: 462 → 256\n",
      "    ✅ Success! Final MSA: 256 rows\n",
      "    ✓ Downloaded mgnify_hits.a3m\n",
      "    ✓ Downloaded uniref90_hits.a3m\n",
      "    📊 hhfilter output: 351 rows\n",
      "    🧹 Stripped: 351 kept, 0 dropped\n",
      "    🎯 Diversity filtered: 351 → 256\n",
      "    ✅ Success! Final MSA: 256 rows\n",
      "    ✅ Found in test: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_test_pdb\\4V8T-O\n",
      "  🔍 Looking for: 4V3P-LJ\n",
      "\n",
      "📋 Processing 4V8T-O → 4v8t_O\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\4V3P-LJ\n",
      "  🔍 Looking for: 6HIV-BC\n",
      "\n",
      "📋 Processing 4V3P-LJ → 4v3p_LJ\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\6HIV-BC\n",
      "\n",
      "📋 Processing 6HIV-BC → 6hiv_BC\n",
      "  🔍 Looking for: 4V7E-BG\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\4V7E-BG\n",
      "\n",
      "📋 Processing 4V7E-BG → 4v7e_BG\n",
      "  🔍 Looking for: 4V3P-LA\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\4V3P-LA\n",
      "\n",
      "📋 Processing 4V3P-LA → 4v3p_LA\n",
      "  🔍 Looking for: 4V6W-CB\n",
      "    ✓ Downloaded bfd_uniclust_hits.a3m    ✓ Downloaded bfd_uniclust_hits.a3m\n",
      "\n",
      "    ✓ Downloaded mgnify_hits.a3m\n",
      "    ✓ Downloaded mgnify_hits.a3m\n",
      "    ✓ Downloaded bfd_uniclust_hits.a3m\n",
      "    ✓ Downloaded uniref90_hits.a3m\n",
      "    ✓ Downloaded uniref90_hits.a3m\n",
      "    ✓ Downloaded bfd_uniclust_hits.a3m\n",
      "    ✓ Downloaded mgnify_hits.a3m\n",
      "    ✓ Downloaded bfd_uniclust_hits.a3m\n",
      "    ✓ Downloaded mgnify_hits.a3m\n",
      "    ✓ Downloaded mgnify_hits.a3m\n",
      "    ✓ Downloaded uniref90_hits.a3m\n",
      "    ✓ Downloaded uniref90_hits.a3m\n",
      "    📊 hhfilter output: 331 rows\n",
      "    🧹 Stripped: 331 kept, 0 dropped\n",
      "    🎯 Diversity filtered: 331 → 256\n",
      "    ✅ Success! Final MSA: 256 rows\n",
      "    ✅ Found in test: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_test_pdb\\4V6W-CB\n",
      "  🔍 Looking for: 3JB9-G\n",
      "\n",
      "📋 Processing 4V6W-CB → 4v6w_CB\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\3JB9-G\n",
      "\n",
      "📋 Processing 3JB9-G → 3jb9_G\n",
      "  🔍 Looking for: 4V8M-BS\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\4V8M-BS\n",
      "  🔍 Looking for: 6HIV-BB\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\6HIV-BB\n",
      "  🔍 Looking for: 4V8M-BV\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\4V8M-BV\n",
      "  🔍 Looking for: 5AJ4-AP\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\5AJ4-AP\n",
      "  🔍 Looking for: 3J92-F\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\3J92-F\n",
      "  🔍 Looking for: 4V6W-CG\n",
      "    ✓ Downloaded uniref90_hits.a3m\n",
      "    📊 hhfilter output: 1099 rows\n",
      "    🧹 Stripped: 1099 kept, 0 dropped\n",
      "    ✓ Downloaded bfd_uniclust_hits.a3m\n",
      "    📊 hhfilter output: 601 rows\n",
      "    🧹 Stripped: 601 kept, 0 dropped\n",
      "    ✓ Downloaded bfd_uniclust_hits.a3m\n",
      "    🎯 Diversity filtered: 601 → 256\n",
      "    ✅ Success! Final MSA: 256 rows\n",
      "\n",
      "📋 Processing 4V8M-BS → 4v8m_BS\n",
      "    ✓ Downloaded mgnify_hits.a3m\n",
      "    🎯 Diversity filtered: 1099 → 256\n",
      "    ✅ Success! Final MSA: 256 rows\n",
      "\n",
      "📋 Processing 6HIV-BB → 6hiv_BB\n",
      "    ✓ Downloaded bfd_uniclust_hits.a3m\n",
      "    ✓ Downloaded uniref90_hits.a3m\n",
      "    ✓ Downloaded mgnify_hits.a3m\n",
      "    ✓ Downloaded bfd_uniclust_hits.a3m\n",
      "    ✓ Downloaded mgnify_hits.a3m\n",
      "    📊 hhfilter output: 627 rows\n",
      "    🧹 Stripped: 627 kept, 0 dropped\n",
      "    📊 hhfilter output: 362 rows\n",
      "    🧹 Stripped: 362 kept, 0 dropped\n",
      "    ✓ Downloaded mgnify_hits.a3m\n",
      "    ✓ Downloaded uniref90_hits.a3m\n",
      "    🎯 Diversity filtered: 362 → 256\n",
      "    ✅ Success! Final MSA: 256 rows\n",
      "\n",
      "📋 Processing 4V8M-BV → 4v8m_BV\n",
      "    🎯 Diversity filtered: 627 → 256\n",
      "    ✅ Success! Final MSA: 256 rows\n",
      "\n",
      "📋 Processing 5AJ4-AP → 5aj4_AP\n",
      "    ✓ Downloaded uniref90_hits.a3m\n",
      "    ✓ Downloaded uniref90_hits.a3m\n",
      "    ✓ Downloaded bfd_uniclust_hits.a3m\n",
      "    ✓ Downloaded bfd_uniclust_hits.a3m\n",
      "    📊 hhfilter output: 1650 rows\n",
      "    🧹 Stripped: 1650 kept, 0 dropped\n",
      "    📊 hhfilter output: 393 rows\n",
      "    🧹 Stripped: 393 kept, 0 dropped\n",
      "    ✓ Downloaded mgnify_hits.a3m\n",
      "    📊 hhfilter output: 439 rows\n",
      "    🧹 Stripped: 439 kept, 0 dropped\n",
      "    ✓ Downloaded mgnify_hits.a3m\n",
      "    🎯 Diversity filtered: 393 → 256\n",
      "    ✅ Success! Final MSA: 256 rows\n",
      "\n",
      "📋 Processing 3J92-F → 3j92_F\n",
      "    🎯 Diversity filtered: 439 → 256\n",
      "    ✅ Success! Final MSA: 256 rows\n",
      "    ✓ Downloaded uniref90_hits.a3m\n",
      "    ✓ Downloaded bfd_uniclust_hits.a3m\n",
      "    ✓ Downloaded uniref90_hits.a3m\n",
      "    ✓ Downloaded mgnify_hits.a3m\n",
      "    🎯 Diversity filtered: 1650 → 256\n",
      "    ✅ Success! Final MSA: 256 rows\n",
      "    📊 hhfilter output: 324 rows\n",
      "    🧹 Stripped: 324 kept, 0 dropped\n",
      "    🎯 Diversity filtered: 324 → 256\n",
      "    ✅ Success! Final MSA: 256 rows\n",
      "    ✅ Found in test: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_test_pdb\\4V6W-CG\n",
      "  🔍 Looking for: 4V3P-LE\n",
      "\n",
      "📋 Processing 4V6W-CG → 4v6w_CG\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\4V3P-LE\n",
      "  🔍 Looking for: 4V7E-CU\n",
      "\n",
      "📋 Processing 4V3P-LE → 4v3p_LE\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\4V7E-CU\n",
      "  🔍 Looking for: 4V8M-BU\n",
      "\n",
      "📋 Processing 4V7E-CU → 4v7e_CU\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\4V8M-BU\n",
      "  🔍 Looking for: 5T2A-V\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\5T2A-V\n",
      "  🔍 Looking for: 4V6W-AF\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\4V6W-AF\n",
      "  🔍 Looking for: 5OPT-L\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\5OPT-L\n",
      "  🔍 Looking for: 5T2A-N\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\5T2A-N\n",
      "  🔍 Looking for: 4V8M-BR\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\4V8M-BR\n",
      "  🔍 Looking for: 4V8M-BK\n",
      "    ✅ Found in train: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\data\\HEAL_PDB\\protein_train_pdb\\4V8M-BK\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94386c4b1c91419ba9f698cf4afd52c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/28 [00:00<?, ?prot/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ✓ Downloaded uniref90_hits.a3m\n",
      "    📊 hhfilter output: 1559 rows\n",
      "    🧹 Stripped: 1559 kept, 0 dropped\n",
      "    ✓ Downloaded bfd_uniclust_hits.a3m\n",
      "    ✓ Downloaded bfd_uniclust_hits.a3m\n",
      "    ✓ Downloaded bfd_uniclust_hits.a3m\n",
      "    ✓ Downloaded mgnify_hits.a3m\n",
      "    ✓ Downloaded mgnify_hits.a3m\n",
      "    ✓ Downloaded mgnify_hits.a3m\n",
      "    🎯 Diversity filtered: 1559 → 256\n",
      "    ✅ Success! Final MSA: 256 rows\n",
      "\n",
      "📋 Processing 4V8M-BU → 4v8m_BU\n",
      "    ✓ Downloaded uniref90_hits.a3m\n",
      "    ✓ Downloaded bfd_uniclust_hits.a3m\n",
      "    ✓ Downloaded uniref90_hits.a3m    ✓ Downloaded uniref90_hits.a3m\n",
      "\n",
      "    ✓ Downloaded mgnify_hits.a3m\n",
      "    📊 hhfilter output: 315 rows\n",
      "    🧹 Stripped: 315 kept, 0 dropped\n",
      "    🎯 Diversity filtered: 315 → 256\n",
      "    ✅ Success! Final MSA: 256 rows\n",
      "\n",
      "📋 Processing 5T2A-V → 5t2a_V\n",
      "    📊 hhfilter output: 780 rows\n",
      "    🧹 Stripped: 780 kept, 0 dropped\n",
      "    ✓ Downloaded uniref90_hits.a3m\n",
      "    ✓ Downloaded bfd_uniclust_hits.a3m\n",
      "    📊 hhfilter output: 321 rows\n",
      "    🧹 Stripped: 321 kept, 0 dropped\n",
      "    🎯 Diversity filtered: 780 → 256\n",
      "    ✅ Success! Final MSA: 256 rows\n",
      "    🎯 Diversity filtered: 321 → 256\n",
      "    ✅ Success! Final MSA: 256 rows\n",
      "\n",
      "📋 Processing 4V6W-AF → 4v6w_AF\n",
      "\n",
      "📋 Processing 5OPT-L → 5opt_L\n",
      "    ✓ Downloaded mgnify_hits.a3m\n",
      "    📊 hhfilter output: 573 rows\n",
      "    🧹 Stripped: 573 kept, 0 dropped\n",
      "    🎯 Diversity filtered: 573 → 256\n",
      "    ✅ Success! Final MSA: 256 rows\n",
      "\n",
      "📋 Processing 5T2A-N → 5t2a_N\n",
      "    ✓ Downloaded bfd_uniclust_hits.a3m\n",
      "    ✓ Downloaded bfd_uniclust_hits.a3m\n",
      "    ✓ Downloaded uniref90_hits.a3m\n",
      "    ✓ Downloaded bfd_uniclust_hits.a3m\n",
      "    ✓ Downloaded mgnify_hits.a3m\n",
      "    ✓ Downloaded mgnify_hits.a3m\n",
      "    ✓ Downloaded mgnify_hits.a3m\n",
      "    📊 hhfilter output: 351 rows\n",
      "    🧹 Stripped: 351 kept, 0 dropped\n",
      "    🎯 Diversity filtered: 351 → 256\n",
      "    ✅ Success! Final MSA: 256 rows\n",
      "\n",
      "📋 Processing 4V8M-BR → 4v8m_BR\n",
      "    ✓ Downloaded uniref90_hits.a3m\n",
      "    ✓ Downloaded uniref90_hits.a3m\n",
      "    ✓ Downloaded bfd_uniclust_hits.a3m\n",
      "    📊 hhfilter output: 1207 rows\n",
      "    🧹 Stripped: 1207 kept, 0 dropped\n",
      "    ✓ Downloaded uniref90_hits.a3m\n",
      "    ✓ Downloaded mgnify_hits.a3m\n",
      "    📊 hhfilter output: 496 rows\n",
      "    🧹 Stripped: 496 kept, 0 dropped\n",
      "    🎯 Diversity filtered: 496 → 256\n",
      "    ✅ Success! Final MSA: 256 rows\n",
      "\n",
      "📋 Processing 4V8M-BK → 4v8m_BK\n",
      "    🎯 Diversity filtered: 1207 → 256\n",
      "    ✅ Success! Final MSA: 256 rows\n",
      "    📊 hhfilter output: 553 rows\n",
      "    🧹 Stripped: 553 kept, 0 dropped\n",
      "    ✓ Downloaded uniref90_hits.a3m\n",
      "    🎯 Diversity filtered: 553 → 256\n",
      "    ✅ Success! Final MSA: 256 rows\n",
      "    ✓ Downloaded bfd_uniclust_hits.a3m\n",
      "    ✓ Downloaded mgnify_hits.a3m\n",
      "    📊 hhfilter output: 467 rows\n",
      "    🧹 Stripped: 467 kept, 0 dropped\n",
      "    ✓ Downloaded uniref90_hits.a3m\n",
      "    🎯 Diversity filtered: 467 → 256\n",
      "    ✅ Success! Final MSA: 256 rows\n",
      "    📊 hhfilter output: 432 rows\n",
      "    🧹 Stripped: 432 kept, 0 dropped\n",
      "    🎯 Diversity filtered: 432 → 256\n",
      "    ✅ Success! Final MSA: 256 rows\n",
      "    📊 hhfilter output: 318 rows\n",
      "    🧹 Stripped: 318 kept, 0 dropped\n",
      "    🎯 Diversity filtered: 318 → 256\n",
      "    ✅ Success! Final MSA: 256 rows\n",
      "    📊 hhfilter output: 615 rows\n",
      "    🧹 Stripped: 615 kept, 0 dropped\n",
      "    🎯 Diversity filtered: 615 → 256\n",
      "    ✅ Success! Final MSA: 256 rows\n",
      "    📊 hhfilter output: 428 rows\n",
      "    🧹 Stripped: 428 kept, 0 dropped\n",
      "    🎯 Diversity filtered: 428 → 256\n",
      "    ✅ Success! Final MSA: 256 rows\n",
      "    📊 hhfilter output: 572 rows\n",
      "    🧹 Stripped: 572 kept, 0 dropped\n",
      "    🎯 Diversity filtered: 572 → 256\n",
      "    ✅ Success! Final MSA: 256 rows\n",
      "\n",
      "💾 Saving updated TSVs...\n",
      "  💾 Created backup: heal_remaining_protein_matches_updated.20250614_113032.bak\n",
      "  💾 Created backup: heal_failed_protein_matches_processed.20250614_113032.bak\n",
      "\n",
      "✅ Complete!\n",
      "📊 Results: 28/28 succeeded\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Re-run MSA pipeline for the last 28 troublesome chains.\n",
    "\n",
    "• Removes any stale final_filtered_256_stripped.a3m\n",
    "• Downloads raw A3Ms from the OpenFold bucket\n",
    "• hhfilter → length strip → diversity (≤256)\n",
    "• Over-writes the row *in whichever TSV the ID lives*.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import csv, re, shutil, subprocess, itertools\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from typing import Dict, Tuple, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "import boto3\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "from botocore.config import Config\n",
    "from botocore import UNSIGNED\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ─── File system layout ────────────────────────────────────────────────\n",
    "ROOT = Path(r\"C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\")\n",
    "HEAL_ROOT = ROOT / \"data\" / \"HEAL_PDB\"\n",
    "SPLITS = {\n",
    "    \"train\": HEAL_ROOT / \"protein_train_pdb\",\n",
    "    \"val\"  : HEAL_ROOT / \"protein_val_pdb\",\n",
    "    \"test\" : HEAL_ROOT / \"protein_test_pdb\",\n",
    "}\n",
    "\n",
    "TSV_REMAINING = ROOT / \"heal_remaining_protein_matches_updated.tsv\"\n",
    "TSV_FAILED    = ROOT / \"heal_failed_protein_matches_processed.tsv\"\n",
    "\n",
    "# ─── Chains to fix ─────────────────────────────────────────────────────\n",
    "TODO = {\n",
    "    \"3H4P-A\", \"3J79-I\", \"3JB9-I\", \"3JB9-R\", \"3JD5-F\", \"3JD5-J\", \"4UDF-1B\",\n",
    "    \"4V6U-BL\", \"4V7E-CE\", \"4V8T-O\", \"5IT7-CC\", \"5IT7-EE\", \"5IT7-HH\", \"5IT7-II\",\n",
    "    \"5IT7-RR\", \"5LNK-N\", \"5NGM-AO\", \"5OOL-W\", \"5OPT-P\", \"5OPT-R\", \"5OQL-U\",\n",
    "    \"5OQL-X\", \"5T5H-I\", \"5T5H-L\", \"5T5H-V\", \"5T5H-W\", \"5V93-F\", \"5XXB-E\",\n",
    "    \"5XXB-F\", \"5XXB-G\", \"5XXB-O\", \"5XXU-A\", \"5XXU-B\", \"5XXU-C\", \"5XXU-F\",\n",
    "    \"5XY3-E\", \"5XY3-F\", \"5XY3-G\", \"5XY3-I\", \"5XY3-J\", \"5XY3-O\", \"5XYI-B\",\n",
    "    \"5YZG-W\", \"5ZWN-Y\", \"6AZ1-A\", \"6AZ1-B\", \"6AZ1-C\", \"6DZI-T\", \"6GAZ-AN\",\n",
    "    \"6GAZ-AP\", \"6GB2-BE\", \"6GCS-D\", \"6GCS-F\", \"6GCS-H\", \"6GCS-J\", \"6GIQ-D\",\n",
    "    \"6HIX-AV\", \"6HIX-BG\"\n",
    "}\n",
    "\n",
    "# ─── OpenFold bucket info ──────────────────────────────────────────────\n",
    "BUCKET       = \"openfold\"\n",
    "A3M_FILES    = (\"bfd_uniclust_hits.a3m\",\"mgnify_hits.a3m\",\"uniref90_hits.a3m\")\n",
    "DL_THREADS   = 14\n",
    "PROC_THREADS = 8\n",
    "MAX_ROWS     = 256\n",
    "TRANSFER_CFG = TransferConfig(max_concurrency=DL_THREADS)\n",
    "S3           = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
    "\n",
    "LOWER = ''.join(map(chr, range(97,123)))\n",
    "\n",
    "# ─── Helper functions ──────────────────────────────────────────────────\n",
    "def to_wsl(path: Path) -> str:\n",
    "    \"\"\"Convert Windows path to WSL path.\"\"\"\n",
    "    posix = path.resolve().as_posix()\n",
    "    return posix if posix.startswith(\"/mnt/\") else f\"/mnt/{path.drive[0].lower()}{posix[2:]}\"\n",
    "\n",
    "def parse_chain_id(pid:str)->Tuple[str,str]:\n",
    "    pid = pid.replace('_','-')\n",
    "    if '-' in pid:\n",
    "        pdb, chain = pid.split('-',1)\n",
    "    elif re.fullmatch(r\"[0-9][A-Za-z0-9]{3}[A-Za-z0-9]{1,3}\", pid):\n",
    "        pdb, chain = pid[:4], pid[4:]\n",
    "    else:\n",
    "        raise ValueError(f\"Bad chain id: {pid}\")\n",
    "    return pdb.lower(), chain            # ← chain case kept!\n",
    "\n",
    "def strip_insertions(seq:str)->str:\n",
    "    return seq.translate({ord(c):None for c in LOWER+\".*\"})\n",
    "\n",
    "def load_a3m(p:Path):\n",
    "    out, hdr, buf = [], None, []\n",
    "    for ln in p.read_text().splitlines():\n",
    "        if ln.startswith('>'):\n",
    "            if hdr: out.append((hdr,''.join(buf)))\n",
    "            hdr, buf = ln[1:], []\n",
    "        else:\n",
    "            buf.append(ln)\n",
    "    if hdr: out.append((hdr,''.join(buf)))\n",
    "    return out\n",
    "\n",
    "def diversity_max(msa:list[Tuple[str,str]], k:int):\n",
    "    if len(msa)<=k: return msa\n",
    "    arr = np.array([list(s) for _,s in msa],dtype='U1')\n",
    "    alpha = sorted({c for row in arr for c in row})\n",
    "    to_i  = {a:i for i,a in enumerate(alpha)}\n",
    "    intarr= np.vectorize(to_i.get)(arr)\n",
    "    dist  = squareform(pdist(intarr,metric='hamming'))\n",
    "    keep  = [0]; sel = np.zeros(len(msa),bool); sel[0]=True\n",
    "    while sel.sum()<k:\n",
    "        mean = dist[:,sel].mean(1); mean[sel]=-1\n",
    "        nxt=int(mean.argmax())\n",
    "        if mean[nxt]<=0: break\n",
    "        sel[nxt]=True; keep.append(nxt)\n",
    "    return [msa[i] for i in keep]\n",
    "\n",
    "def bucket_download(key:str, dest:Path)->bool:\n",
    "    try:\n",
    "        if dest.exists(): return True\n",
    "        dest.parent.mkdir(parents=True,exist_ok=True)\n",
    "        S3.download_file(BUCKET, key, str(dest), Config=TRANSFER_CFG)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️  Download failed for {key}: {e}\")\n",
    "        return False\n",
    "\n",
    "def find_chain_dir(original_id:str)->Optional[Path]:\n",
    "    \"\"\"Find directory with exact case-sensitive matching.\"\"\"\n",
    "    print(f\"  🔍 Looking for: {original_id}\")\n",
    "    \n",
    "    for split_name, split in SPLITS.items():\n",
    "        if not split.exists(): \n",
    "            continue\n",
    "            \n",
    "        # Direct check with exact case\n",
    "        direct_path = split / original_id\n",
    "        if direct_path.exists() and direct_path.is_dir():\n",
    "            print(f\"    ✅ Found in {split_name}: {direct_path}\")\n",
    "            return direct_path\n",
    "            \n",
    "        # Scan all directories for exact match\n",
    "        for d in split.iterdir():\n",
    "            if d.is_dir() and d.name == original_id:\n",
    "                print(f\"    ✅ Found in {split_name}: {d}\")\n",
    "                return d\n",
    "                \n",
    "        # Log case-insensitive matches for debugging\n",
    "        case_insensitive = [d for d in split.iterdir() \n",
    "                           if d.is_dir() and d.name.lower() == original_id.lower()]\n",
    "        if case_insensitive:\n",
    "            print(f\"    ⚠️  Case-insensitive matches in {split_name}: {[d.name for d in case_insensitive]}\")\n",
    "    \n",
    "    print(f\"    ❌ Not found!\")\n",
    "    return None\n",
    "\n",
    "def delete_stale(chain_dir:Path):\n",
    "    (chain_dir/\"final_filtered_256_stripped.a3m\").unlink(missing_ok=True)\n",
    "    shutil.rmtree(chain_dir/\"_tmp_a3m\", ignore_errors=True)\n",
    "\n",
    "def process(chain_dir:Path, matched_id:str):\n",
    "    print(f\"\\n📋 Processing {chain_dir.name} → {matched_id}\")\n",
    "    pdb, chain = parse_chain_id(matched_id)\n",
    "    tmp = chain_dir/\"_tmp_a3m\"; tmp.mkdir(exist_ok=True)\n",
    "\n",
    "    # Download all A3M files\n",
    "    downloaded = []\n",
    "    for f in A3M_FILES:\n",
    "        key  = f\"pdb/{pdb}_{chain}/a3m/{f}\"  # pdb lower, chain as-is\n",
    "        dest = tmp/f\n",
    "        if bucket_download(key, dest):\n",
    "            downloaded.append(f)\n",
    "            print(f\"    ✓ Downloaded {f}\")\n",
    "        else:\n",
    "            print(f\"    ✗ Failed to download {f}\")\n",
    "    \n",
    "    if not downloaded:\n",
    "        shutil.rmtree(tmp,ignore_errors=True)\n",
    "        return 0,0,0,\"Failed: no downloads\"\n",
    "\n",
    "    # Concatenate available files\n",
    "    raw = tmp/\"all_raw.a3m\"\n",
    "    with raw.open('w') as out:\n",
    "        for f in downloaded:\n",
    "            out.write((tmp/f).read_text())\n",
    "\n",
    "    # Run hhfilter with WSL path conversion\n",
    "    hh_out = tmp/f\"all_hhfiltered_{MAX_ROWS}.a3m\"\n",
    "    res = subprocess.run([\n",
    "        \"wsl\",\"hhfilter\",\n",
    "        \"-i\",to_wsl(raw),\n",
    "        \"-o\",to_wsl(hh_out),\n",
    "        \"-diff\",str(MAX_ROWS)\n",
    "    ], capture_output=True, text=True)\n",
    "    \n",
    "    if res.returncode:\n",
    "        print(f\"    ❌ hhfilter failed: {res.stderr}\")\n",
    "        shutil.rmtree(tmp,ignore_errors=True)\n",
    "        return 0,0,0,f\"Failed: hhfilter ({res.returncode})\"\n",
    "\n",
    "    msa = load_a3m(hh_out)\n",
    "    hh_rows = len(msa)\n",
    "    print(f\"    📊 hhfilter output: {hh_rows} rows\")\n",
    "    \n",
    "    if not msa:\n",
    "        shutil.rmtree(tmp,ignore_errors=True)\n",
    "        return 0,hh_rows,0,\"Failed: hh-empty\"\n",
    "\n",
    "    # Strip insertions\n",
    "    tgt_len = len(strip_insertions(msa[0][1]))\n",
    "    kept, dropped = [], 0\n",
    "    for h,s in msa:\n",
    "        clean = strip_insertions(s)\n",
    "        if len(clean) == tgt_len: \n",
    "            kept.append((h,clean))\n",
    "        else: \n",
    "            dropped += 1\n",
    "    \n",
    "    print(f\"    🧹 Stripped: {len(kept)} kept, {dropped} dropped\")\n",
    "    \n",
    "    if len(kept) < 2:\n",
    "        shutil.rmtree(tmp,ignore_errors=True)\n",
    "        return dropped,hh_rows,0,f\"Failed: only {len(kept)} rows\"\n",
    "\n",
    "    # Diversity filtering\n",
    "    div = 0\n",
    "    if len(kept) > MAX_ROWS:\n",
    "        bef = len(kept)\n",
    "        kept = diversity_max(kept, MAX_ROWS)\n",
    "        div = bef - len(kept)\n",
    "        print(f\"    🎯 Diversity filtered: {bef} → {len(kept)}\")\n",
    "\n",
    "    # Write final A3M\n",
    "    final_path = chain_dir/\"final_filtered_256_stripped.a3m\"\n",
    "    with final_path.open('w') as fh:\n",
    "        for h,s in kept: \n",
    "            fh.write(f\">{h}\\n{s}\\n\")\n",
    "    \n",
    "    print(f\"    ✅ Success! Final MSA: {len(kept)} rows\")\n",
    "    shutil.rmtree(tmp,ignore_errors=True)\n",
    "    return dropped,hh_rows,div,\"Success\"\n",
    "\n",
    "# ─── TSV helpers ────────────────────────────────────────────────────────\n",
    "def load_tsv(p:Path): \n",
    "    return list(csv.DictReader(p.open(),delimiter='\\t')) if p.exists() else []\n",
    "\n",
    "def save_tsv(p:Path, rows):\n",
    "    if not rows: return\n",
    "    # Create backup\n",
    "    if p.exists():\n",
    "        backup = p.with_suffix(f'.{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.bak')\n",
    "        shutil.copy2(p, backup)\n",
    "        print(f\"  💾 Created backup: {backup.name}\")\n",
    "    \n",
    "    with p.open('w',newline='') as fh:\n",
    "        wr = csv.DictWriter(fh,delimiter='\\t',fieldnames=rows[0].keys())\n",
    "        wr.writeheader()\n",
    "        wr.writerows(rows)\n",
    "\n",
    "# ─── Main ───────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    print(\"🔧 Re-running MSA pipeline for 28 proteins\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Verify hhfilter\n",
    "    if subprocess.run([\"wsl\",\"which\",\"hhfilter\"],capture_output=True).returncode:\n",
    "        raise RuntimeError(\"hhfilter not found in WSL\")\n",
    "\n",
    "    # Load TSVs\n",
    "    rows_rem = load_tsv(TSV_REMAINING)\n",
    "    rows_fail = load_tsv(TSV_FAILED)\n",
    "    \n",
    "    # Build index\n",
    "    index: Dict[str,Tuple[str,int]] = {}\n",
    "    for i,r in enumerate(rows_rem): \n",
    "        index[r['original_id']] = ('rem',i)\n",
    "    for i,r in enumerate(rows_fail): \n",
    "        index[r['original_id']] = ('fail',i)\n",
    "\n",
    "    # Find proteins in TSVs\n",
    "    todo = [pid for pid in TODO if pid in index]\n",
    "    missing = TODO - set(todo)\n",
    "    \n",
    "    print(f\"\\n📊 Status:\")\n",
    "    print(f\"  - To process: {len(todo)}/{len(TODO)}\")\n",
    "    if missing:\n",
    "        print(f\"  - Not found in TSVs: {missing}\")\n",
    "\n",
    "    # Clean up old files\n",
    "    print(f\"\\n🧹 Cleaning old A3M files...\")\n",
    "    for pid in todo:\n",
    "        d = find_chain_dir(pid)\n",
    "        if d: \n",
    "            delete_stale(d)\n",
    "\n",
    "    # Process proteins\n",
    "    print(f\"\\n🚀 Processing {len(todo)} proteins...\")\n",
    "    with ThreadPoolExecutor(max_workers=PROC_THREADS) as pool:\n",
    "        fut2pid = {}\n",
    "        \n",
    "        for pid in todo:\n",
    "            which, idx = index[pid]\n",
    "            row = rows_rem[idx] if which=='rem' else rows_fail[idx]\n",
    "            d = find_chain_dir(pid)\n",
    "            \n",
    "            if d is None:\n",
    "                print(f\"\\n❌ Directory not found for {pid}\")\n",
    "                # Update status in the appropriate list\n",
    "                if which == 'rem':\n",
    "                    rows_rem[idx]['status'] = \"Failed: dir-missing\"\n",
    "                else:\n",
    "                    rows_fail[idx]['status'] = \"Failed: dir-missing\"\n",
    "                continue\n",
    "                \n",
    "            fut2pid[pool.submit(process, d, row['matched_id'])] = pid\n",
    "\n",
    "        # Process results\n",
    "        for fut in tqdm(as_completed(fut2pid), total=len(fut2pid),\n",
    "                        desc=\"Processing\", unit=\"prot\"):\n",
    "            pid = fut2pid[fut]\n",
    "            which, idx = index[pid]\n",
    "            \n",
    "            try:\n",
    "                dropped, hh, div, status = fut.result()\n",
    "                row = rows_rem[idx] if which=='rem' else rows_fail[idx]\n",
    "                row['n_rows_dropped'] = str(dropped)\n",
    "                row['hhfilter_rows'] = str(hh)\n",
    "                row['diversity_filtered_rows'] = str(div)\n",
    "                row['status'] = status\n",
    "            except Exception as e:\n",
    "                print(f\"\\n❌ Error processing {pid}: {e}\")\n",
    "                row = rows_rem[idx] if which=='rem' else rows_fail[idx]\n",
    "                row['status'] = f\"Failed: {str(e)[:50]}\"\n",
    "\n",
    "    # Save updated TSVs\n",
    "    print(f\"\\n💾 Saving updated TSVs...\")\n",
    "    save_tsv(TSV_REMAINING, rows_rem)\n",
    "    save_tsv(TSV_FAILED, rows_fail)\n",
    "\n",
    "    # Summary\n",
    "    success_count = 0\n",
    "    for pid in todo:\n",
    "        which, idx = index[pid]\n",
    "        row = rows_rem[idx] if which=='rem' else rows_fail[idx]\n",
    "        if row['status'] == \"Success\":\n",
    "            success_count += 1\n",
    "    \n",
    "    print(f\"\\n✅ Complete!\")\n",
    "    print(f\"📊 Results: {success_count}/{len(todo)} succeeded\")\n",
    "    \n",
    "    # Show failures\n",
    "    failures = []\n",
    "    for pid in todo:\n",
    "        which, idx = index[pid]\n",
    "        row = rows_rem[idx] if which=='rem' else rows_fail[idx]\n",
    "        if row['status'] != \"Success\":\n",
    "            failures.append(f\"{pid}: {row['status']}\")\n",
    "    \n",
    "    if failures:\n",
    "        print(f\"\\n❌ Failed proteins:\")\n",
    "        for f in failures[:10]:\n",
    "            print(f\"  - {f}\")\n",
    "        if len(failures) > 10:\n",
    "            print(f\"  ... and {len(failures)-10} more\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total proteins to inspect (after exclusions): 36,622\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37bfbd79608b4f12ba361c424e7cb98c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checking A3M presence:   0%|          | 0/36622 [00:00<?, ?prot/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Every protein has the hhfilter A3M\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1e081e954e4711bfe8c516dc859bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pairwise identity check:   0%|          | 0/36622 [00:00<?, ?prot/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "QA SUMMARY\n",
      "============================================================\n",
      "Proteins checked : 36,622\n",
      "Failed identity  : 2 (< 0.95)\n",
      "\n",
      "⟹ Proteins failing identity threshold:\n",
      "  4UDF-1B      79.2 %\n",
      "  4V8T-O       90.9 %\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\"\"\"\n",
    "QA check for HEAL_PDB A3Ms\n",
    "--------------------------\n",
    "1. Verify every protein (except the exclusions) has final_filtered_256_stripped.a3m\n",
    "2. For each protein, check Biopython pairwise identity between sequence.txt and\n",
    "   the stripped query sequence from that A3M.\n",
    "   • Pass threshold: 95 %\n",
    "If any A3M is missing → raises RuntimeError after printing offenders.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import re, csv, textwrap\n",
    "\n",
    "# ─── paths ──────────────────────────────────────────────────────────────\n",
    "ROOT      = Path(r\"C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\")\n",
    "HEAL_ROOT = ROOT / \"data\" / \"PDBCH\"\n",
    "SPLITS = {\n",
    "    \"train\": HEAL_ROOT / \"train_pdbch\",\n",
    "    \"val\"  : HEAL_ROOT / \"val_pdbch\",\n",
    "    \"test\" : HEAL_ROOT / \"test_pdbch\",\n",
    "}\n",
    "\n",
    "A3M_NAME   = \"final_filtered_256_stripped.a3m\"\n",
    "EXCLUDE = {\n",
    "    \"1W8X-M\", \"2WWX-B\", \"3M7G-A\", \"5GAO-E\", \"5OXE-A\", \"1KVE-A\", \"4BTP-A\",\n",
    "}\n",
    "THRESH     = 0.95\n",
    "\n",
    "# ─── helper: strip insertions from A3M sequence ─────────────────────────\n",
    "LOWER = ''.join(chr(c) for c in range(97,123))  # a-z\n",
    "def strip_a3m(seq: str) -> str:\n",
    "    return seq.translate({ord(c):None for c in LOWER + \".*\"})\n",
    "\n",
    "def first_a3m_sequence(p: Path) -> str:\n",
    "    hdr_seen = False\n",
    "    with p.open() as fh:\n",
    "        for ln in fh:\n",
    "            if ln.startswith('>'):\n",
    "                hdr_seen = True\n",
    "            elif hdr_seen:\n",
    "                return strip_a3m(ln.rstrip())\n",
    "    return \"\"\n",
    "\n",
    "# ─── Biopython similarity (globalxx) ────────────────────────────────────\n",
    "from Bio import pairwise2\n",
    "def similarity(seq1:str, seq2:str)->float:\n",
    "    score = pairwise2.align.globalxx(seq1, seq2, score_only=True)\n",
    "    return score / max(len(seq1), len(seq2)) if max(len(seq1),len(seq2)) else 0.0\n",
    "\n",
    "# ─── collect all protein dirs ───────────────────────────────────────────\n",
    "dirs = []\n",
    "for folder in SPLITS.values():\n",
    "    dirs.extend([d for d in folder.iterdir() if d.is_dir() and d.name not in EXCLUDE])\n",
    "\n",
    "print(f\"Total proteins to inspect (after exclusions): {len(dirs):,}\")\n",
    "\n",
    "# ─── pass 1: ensure A3M exists ──────────────────────────────────────────\n",
    "missing = []\n",
    "for d in tqdm(dirs, desc=\"Checking A3M presence\", unit=\"prot\"):\n",
    "    if not (d / A3M_NAME).exists():\n",
    "        missing.append(d.name)\n",
    "\n",
    "if missing:\n",
    "    print(\"\\n❌ The following proteins are missing the hhfilter A3M:\")\n",
    "    print(textwrap.fill(' '.join(sorted(missing)), width=100))\n",
    "    raise RuntimeError(\"A3M file(s) missing – fix before continuing!\")\n",
    "\n",
    "print(\"✓ Every protein has the hhfilter A3M\\n\")\n",
    "\n",
    "# ─── pass 2: pairwise identity check (multithreaded) ────────────────────\n",
    "def check_identity_worker(d: Path) -> tuple[str, float] | None:\n",
    "    \"\"\"Worker function for identity checking\"\"\"\n",
    "    seq_txt = d / \"sequence.txt\"\n",
    "    a3m     = d / A3M_NAME\n",
    "    try:\n",
    "        seq_query = first_a3m_sequence(a3m)\n",
    "        seq_local = ''.join(c for c in seq_txt.read_text().strip() if c.isalpha())\n",
    "        sim = similarity(seq_query, seq_local)\n",
    "        if sim < THRESH:\n",
    "            return (d.name, sim)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"[ERR] {d.name}: {e}\")\n",
    "        return (d.name, 0.0)\n",
    "\n",
    "fails = []\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    future_to_dir = {executor.submit(check_identity_worker, d): d for d in dirs}\n",
    "    \n",
    "    for future in tqdm(as_completed(future_to_dir), total=len(dirs), \n",
    "                       desc=\"Pairwise identity check\", unit=\"prot\"):\n",
    "        result = future.result()\n",
    "        if result is not None:\n",
    "            fails.append(result)\n",
    "\n",
    "# ─── summary ────────────────────────────────────────────────────────────\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"QA SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Proteins checked : {len(dirs):,}\")\n",
    "print(f\"Failed identity  : {len(fails):,} (< {THRESH:.2f})\")\n",
    "\n",
    "if fails:\n",
    "    print(\"\\n⟹ Proteins failing identity threshold:\")\n",
    "    for pid, s in sorted(fails, key=lambda x: x[0]):\n",
    "        print(f\"  {pid:10s}  {s*100:5.1f} %\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n🎉 All proteins ≥ 95 % identity – good to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "❌ Root directory not found: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\notebooks\\--f=c:\\Users\\rfrjo\\AppData\\Roaming\\jupyter\\runtime\\kernel-v323960d60bfa4c55e8e4d0be3c857aed9127406ae.json",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m ❌ Root directory not found: C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\\notebooks\\--f=c:\\Users\\rfrjo\\AppData\\Roaming\\jupyter\\runtime\\kernel-v323960d60bfa4c55e8e4d0be3c857aed9127406ae.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rfrjo\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Case-Sensitive Sequence Fix Tool\n",
      "============================================================\n",
      "📄 Loading FASTA file with case-sensitive IDs...\n",
      "  ✓ Loaded 36,641 sequences\n",
      "\n",
      "  Sample IDs (showing exact case):\n",
      "    11AS-A\n",
      "    154L-A\n",
      "    155C-A\n",
      "    16PK-A\n",
      "    16VP-A\n",
      "\n",
      "🔍 Checking all protein folders...\n",
      "  Found 29738 proteins in train\n",
      "  Found 3318 proteins in val\n",
      "  Found 3398 proteins in test\n",
      "\n",
      "📊 Total protein folders to check: 36454\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644270753a90426ea8277105e95f1102",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checking sequences:   0%|          | 0/36454 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  🔧 Fixed 3J79-I:\n",
      "     Old length: 104\n",
      "     New length: 221\n",
      "     Old start: MVNVPKTRKTYCSNKCKKHTMHKVSQYKKG...\n",
      "     New start: MTNTSNELKHYNVKGKKKVLVPVNAKKTIN...\n",
      "\n",
      "  🔧 Fixed 3J92-F:\n",
      "     Old length: 110\n",
      "     New length: 250\n",
      "     Old start: MSGRLWSKAIFAGYKRGLRNQREHTALLKI...\n",
      "     New start: MEGADVKEKKKKVPAVPETLKKKRKNFAEL...\n",
      "\n",
      "  🔧 Fixed 3JB9-G:\n",
      "     Old length: 558\n",
      "     New length: 115\n",
      "     Old start: MLVANYSSDSEEQENSQSPNIQPLLHTENL...\n",
      "     New start: MADLVDKPRSELSEIELARLEEYEFSAGPL...\n",
      "\n",
      "============================================================\n",
      "SEQUENCE FIX SUMMARY\n",
      "============================================================\n",
      "Total checked      : 36,454\n",
      "Sequences correct  : 36,372\n",
      "Mismatches found   : 82\n",
      "Successfully fixed : 82\n",
      "Not found in FASTA : 0\n",
      "Errors             : 0\n",
      "\n",
      "❌ All 82 proteins with sequence mismatches:\n",
      "\n",
      "  train (74 proteins):\n",
      "    3J79-I 3J92-F 3JB9-G 3JB9-I 3JB9-R 3JD5-F 3JD5-J 4V3P-LA 4V3P-LB 4V3P-LE\n",
      "    4V3P-LJ 4V6U-BL 4V6W-AF 4V7E-BG 4V7E-CE 4V7E-CU 4V8M-BK 4V8M-BR 4V8M-BS 4V8M-BU\n",
      "    4V8M-BV 5AJ4-AK 5AJ4-AP 5IT7-CC 5IT7-EE 5IT7-HH 5IT7-II 5IT7-RR 5LNK-N 5NGM-AO\n",
      "    5OOL-W 5OPT-L 5OPT-P 5OPT-R 5OQL-U 5OQL-X 5T2A-N 5T2A-V 5T5H-I 5T5H-L\n",
      "    5T5H-V 5T5H-W 5V93-F 5XXB-E 5XXB-F 5XXB-G 5XXB-O 5XXU-A 5XXU-B 5XXU-C\n",
      "    5XXU-F 5XY3-E 5XY3-F 5XY3-G 5XY3-I 5XY3-J 5XY3-O 5XYI-B 5YZG-W 6AZ1-A\n",
      "    6AZ1-B 6AZ1-C 6DZI-T 6GAZ-AN 6GAZ-AP 6GB2-BE 6GCS-D 6GCS-F 6GCS-H 6GCS-J\n",
      "    6HIV-BB 6HIV-BC 6HIX-AV 6HIX-BG\n",
      "\n",
      "  test (8 proteins):\n",
      "    3H4P-A 4V6W-CB 4V6W-CC 4V6W-CG 4V6W-CR 5LJ5-S 5ZWN-Y 6GIQ-D\n",
      "\n",
      "✅ Successfully fixed 82 sequence files!\n",
      "\n",
      "💡 Recommendation: Re-run your QA check script to verify all sequences now pass!\n",
      "\n",
      "✨ Done!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Fix corrupted sequence.txt files by matching with exact case-sensitive IDs\n",
    "==========================================================================\n",
    "\n",
    "This script:\n",
    "1. Reads all protein folders from train/val/test splits\n",
    "2. Loads sequences from nrPDB-GO_2019.06.18_sequences.fasta\n",
    "3. Compares sequence.txt with the correct sequence (case-sensitive match)\n",
    "4. Fixes mismatches by replacing sequence.txt with correct content\n",
    "5. Reports all fixes made\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "import textwrap\n",
    "\n",
    "# ─── PATHS ──────────────────────────────────────────────────────────────\n",
    "ROOT = Path(r\"C:\\Users\\rfrjo\\Documents\\Codebases\\PFP_Testing\")\n",
    "HEAL_ROOT = ROOT / \"data\" / \"HEAL_PDB\"\n",
    "SPLITS = {\n",
    "    \"train\": HEAL_ROOT / \"protein_train_pdb\",\n",
    "    \"val\": HEAL_ROOT / \"protein_val_pdb\",\n",
    "    \"test\": HEAL_ROOT / \"protein_test_pdb\",\n",
    "}\n",
    "\n",
    "FASTA_FILE = ROOT / \"data\" / \"nrPDB-GO_2019.06.18_sequences.fasta\"\n",
    "\n",
    "# ─── LOAD FASTA WITH EXACT CASE ─────────────────────────────────────────\n",
    "def load_fasta_exact_case():\n",
    "    \"\"\"Load FASTA file preserving exact case of IDs.\"\"\"\n",
    "    print(\"📄 Loading FASTA file with case-sensitive IDs...\")\n",
    "    sequences = {}\n",
    "    current_id = None\n",
    "    current_seq = []\n",
    "    \n",
    "    with FASTA_FILE.open() as fh:\n",
    "        for line in fh:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                # Save previous sequence\n",
    "                if current_id:\n",
    "                    sequences[current_id] = ''.join(current_seq)\n",
    "                \n",
    "                # Parse new ID - take everything after '>' up to first space\n",
    "                # This preserves exact case\n",
    "                full_header = line[1:]\n",
    "                current_id = full_header.split()[0] if ' ' in full_header else full_header\n",
    "                current_seq = []\n",
    "            else:\n",
    "                current_seq.append(line)\n",
    "        \n",
    "        # Don't forget the last sequence\n",
    "        if current_id:\n",
    "            sequences[current_id] = ''.join(current_seq)\n",
    "    \n",
    "    print(f\"  ✓ Loaded {len(sequences):,} sequences\")\n",
    "    \n",
    "    # Show some examples to verify case preservation\n",
    "    examples = list(sequences.keys())[:5]\n",
    "    print(\"\\n  Sample IDs (showing exact case):\")\n",
    "    for ex in examples:\n",
    "        print(f\"    {ex}\")\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "# ─── CHECK AND FIX SEQUENCES ────────────────────────────────────────────\n",
    "def check_and_fix_sequences(fasta_sequences):\n",
    "    \"\"\"Check all protein folders and fix sequence mismatches.\"\"\"\n",
    "    print(\"\\n🔍 Checking all protein folders...\")\n",
    "    \n",
    "    # Collect all protein directories\n",
    "    all_dirs = []\n",
    "    for split_name, split_path in SPLITS.items():\n",
    "        if not split_path.exists():\n",
    "            print(f\"  ⚠️  Split path doesn't exist: {split_path}\")\n",
    "            continue\n",
    "        \n",
    "        dirs = [d for d in split_path.iterdir() if d.is_dir()]\n",
    "        print(f\"  Found {len(dirs)} proteins in {split_name}\")\n",
    "        all_dirs.extend([(split_name, d) for d in dirs])\n",
    "    \n",
    "    print(f\"\\n📊 Total protein folders to check: {len(all_dirs)}\")\n",
    "    \n",
    "    # Check each directory\n",
    "    mismatches = []\n",
    "    not_found = []\n",
    "    fixed = []\n",
    "    errors = []\n",
    "    \n",
    "    for split_name, protein_dir in tqdm(all_dirs, desc=\"Checking sequences\"):\n",
    "        protein_id = protein_dir.name  # This is the exact case-sensitive folder name\n",
    "        seq_file = protein_dir / \"sequence.txt\"\n",
    "        \n",
    "        # Check if sequence.txt exists\n",
    "        if not seq_file.exists():\n",
    "            errors.append((protein_id, split_name, \"No sequence.txt\"))\n",
    "            continue\n",
    "        \n",
    "        # Read current sequence\n",
    "        try:\n",
    "            current_seq = ''.join(c for c in seq_file.read_text().strip() if c.isalpha())\n",
    "        except Exception as e:\n",
    "            errors.append((protein_id, split_name, f\"Read error: {e}\"))\n",
    "            continue\n",
    "        \n",
    "        # Look up correct sequence (exact case match)\n",
    "        if protein_id not in fasta_sequences:\n",
    "            not_found.append((protein_id, split_name))\n",
    "            # Try case-insensitive search to help debug\n",
    "            case_insensitive_matches = [k for k in fasta_sequences.keys() \n",
    "                                       if k.lower() == protein_id.lower()]\n",
    "            if case_insensitive_matches:\n",
    "                tqdm.write(f\"  ⚠️  {protein_id} not found, but found case variants: {case_insensitive_matches}\")\n",
    "            continue\n",
    "        \n",
    "        correct_seq = fasta_sequences[protein_id]\n",
    "        \n",
    "        # Compare sequences\n",
    "        if current_seq != correct_seq:\n",
    "            mismatches.append((protein_id, split_name))\n",
    "            \n",
    "            # Fix the sequence\n",
    "            try:\n",
    "                seq_file.write_text(correct_seq + '\\n')\n",
    "                fixed.append((protein_id, split_name))\n",
    "                \n",
    "                # Show details for first few fixes\n",
    "                if len(fixed) <= 3:\n",
    "                    tqdm.write(f\"\\n  🔧 Fixed {protein_id}:\")\n",
    "                    tqdm.write(f\"     Old length: {len(current_seq)}\")\n",
    "                    tqdm.write(f\"     New length: {len(correct_seq)}\")\n",
    "                    if len(current_seq) > 0 and len(correct_seq) > 0:\n",
    "                        # Check if it's completely different or just truncated\n",
    "                        if current_seq[:10] != correct_seq[:10]:\n",
    "                            tqdm.write(f\"     Old start: {current_seq[:30]}...\")\n",
    "                            tqdm.write(f\"     New start: {correct_seq[:30]}...\")\n",
    "                        else:\n",
    "                            tqdm.write(f\"     Sequences had same start but different lengths\")\n",
    "            except Exception as e:\n",
    "                errors.append((protein_id, split_name, f\"Write error: {e}\"))\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SEQUENCE FIX SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total checked      : {len(all_dirs):,}\")\n",
    "    print(f\"Sequences correct  : {len(all_dirs) - len(mismatches) - len(not_found) - len(errors):,}\")\n",
    "    print(f\"Mismatches found   : {len(mismatches):,}\")\n",
    "    print(f\"Successfully fixed : {len(fixed):,}\")\n",
    "    print(f\"Not found in FASTA : {len(not_found):,}\")\n",
    "    print(f\"Errors             : {len(errors):,}\")\n",
    "    \n",
    "    # Show all mismatches\n",
    "    if mismatches:\n",
    "        print(f\"\\n❌ All {len(mismatches)} proteins with sequence mismatches:\")\n",
    "        by_split = defaultdict(list)\n",
    "        for pid, split in mismatches:\n",
    "            by_split[split].append(pid)\n",
    "        \n",
    "        for split in ['train', 'val', 'test']:\n",
    "            if split in by_split:\n",
    "                print(f\"\\n  {split} ({len(by_split[split])} proteins):\")\n",
    "                # Group into lines of ~10 proteins each\n",
    "                proteins = sorted(by_split[split])\n",
    "                for i in range(0, len(proteins), 10):\n",
    "                    batch = proteins[i:i+10]\n",
    "                    print(f\"    {' '.join(batch)}\")\n",
    "    \n",
    "    # Show not found\n",
    "    if not_found:\n",
    "        print(f\"\\n⚠️  {len(not_found)} proteins not found in FASTA (need investigation):\")\n",
    "        for pid, split in not_found[:20]:  # Show first 20\n",
    "            print(f\"    {pid:15s} ({split})\")\n",
    "        if len(not_found) > 20:\n",
    "            print(f\"    ... and {len(not_found)-20} more\")\n",
    "    \n",
    "    # Show errors\n",
    "    if errors:\n",
    "        print(f\"\\n❌ {len(errors)} errors encountered:\")\n",
    "        for pid, split, err in errors[:10]:\n",
    "            print(f\"    {pid:15s} ({split}): {err}\")\n",
    "        if len(errors) > 10:\n",
    "            print(f\"    ... and {len(errors)-10} more\")\n",
    "    \n",
    "    # Final status\n",
    "    if fixed:\n",
    "        print(f\"\\n✅ Successfully fixed {len(fixed)} sequence files!\")\n",
    "        if len(fixed) != len(mismatches):\n",
    "            print(f\"⚠️  Warning: Found {len(mismatches)} mismatches but only fixed {len(fixed)}\")\n",
    "    else:\n",
    "        print(\"\\n✅ No sequence fixes needed - all sequences match!\")\n",
    "    \n",
    "    return mismatches, fixed, not_found, errors\n",
    "\n",
    "# ─── MAIN ───────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    print(\"🔧 Case-Sensitive Sequence Fix Tool\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Load FASTA with exact case preservation\n",
    "    fasta_sequences = load_fasta_exact_case()\n",
    "    \n",
    "    # Check and fix sequences\n",
    "    mismatches, fixed, not_found, errors = check_and_fix_sequences(fasta_sequences)\n",
    "    \n",
    "    # If there were fixes, suggest re-running the QA check\n",
    "    if fixed:\n",
    "        print(\"\\n💡 Recommendation: Re-run your QA check script to verify all sequences now pass!\")\n",
    "    \n",
    "    print(\"\\n✨ Done!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
