{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68a5c51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 16 random A3M files (seed=42).\n",
      "\n",
      "===== GPU | eager FP32 =====\n",
      "final_filtered_256_stripped.a3m       0.6882 s\n",
      "final_filtered_256_stripped.a3m       0.1456 s\n",
      "final_filtered_256_stripped.a3m       0.2433 s\n",
      "final_filtered_256_stripped.a3m       0.1363 s\n",
      "final_filtered_256_stripped.a3m       0.2594 s\n",
      "final_filtered_256_stripped.a3m       0.1704 s\n",
      "final_filtered_256_stripped.a3m       0.1921 s\n",
      "final_filtered_256_stripped.a3m       0.5438 s\n",
      "final_filtered_256_stripped.a3m       0.4511 s\n",
      "final_filtered_256_stripped.a3m       0.2002 s\n",
      "final_filtered_256_stripped.a3m       1.2911 s\n",
      "final_filtered_256_stripped.a3m       0.6022 s\n",
      "final_filtered_256_stripped.a3m       0.3167 s\n",
      "final_filtered_256_stripped.a3m       0.8935 s\n",
      "final_filtered_256_stripped.a3m       0.4789 s\n",
      "final_filtered_256_stripped.a3m       0.4189 s\n",
      "\n",
      "GPU | eager FP32  —  summary (sec)\n",
      "----------------------------------------\n",
      "  mean   :   0.4395\n",
      "  median :   0.3678\n",
      "  stddev :   0.3150\n",
      "  min    :   0.1363\n",
      "  max    :   1.2911\n",
      "\n",
      "===== GPU | eager BF16 (autocast) =====\n",
      "final_filtered_256_stripped.a3m       0.1671 s\n",
      "final_filtered_256_stripped.a3m       0.0575 s\n",
      "final_filtered_256_stripped.a3m       0.0949 s\n",
      "final_filtered_256_stripped.a3m       0.0544 s\n",
      "final_filtered_256_stripped.a3m       0.0964 s\n",
      "final_filtered_256_stripped.a3m       0.0575 s\n",
      "final_filtered_256_stripped.a3m       0.0660 s\n",
      "final_filtered_256_stripped.a3m       0.1693 s\n",
      "final_filtered_256_stripped.a3m       0.1392 s\n",
      "final_filtered_256_stripped.a3m       0.0642 s\n",
      "final_filtered_256_stripped.a3m       0.3761 s\n",
      "final_filtered_256_stripped.a3m       0.1765 s\n",
      "final_filtered_256_stripped.a3m       0.1029 s\n",
      "final_filtered_256_stripped.a3m       0.2853 s\n",
      "final_filtered_256_stripped.a3m       0.1446 s\n",
      "final_filtered_256_stripped.a3m       0.1334 s\n",
      "\n",
      "GPU | eager BF16 (autocast)  —  summary (sec)\n",
      "----------------------------------------\n",
      "  mean   :   0.1366\n",
      "  median :   0.1181\n",
      "  stddev :   0.0884\n",
      "  min    :   0.0544\n",
      "  max    :   0.3761\n",
      "\n",
      "===== GPU | compiled + BF16 =====\n",
      "Compiling model …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_filtered_256_stripped.a3m      38.6286 s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 116\u001b[0m\n\u001b[1;32m    114\u001b[0m run_variant(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU | eager FP32\u001b[39m\u001b[38;5;124m\"\u001b[39m,          gpu, compile_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, use_autocast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    115\u001b[0m run_variant(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU | eager BF16 (autocast)\u001b[39m\u001b[38;5;124m\"\u001b[39m, gpu, compile_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, use_autocast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 116\u001b[0m \u001b[43mrun_variant\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGPU | compiled + BF16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[43mgpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompile_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43muse_autocast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m run_variant(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPU | eager FP32\u001b[39m\u001b[38;5;124m\"\u001b[39m,          cpu, compile_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, use_autocast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[2], line 93\u001b[0m, in \u001b[0;36mrun_variant\u001b[0;34m(label, device, compile_model, use_autocast)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m SAMPLE_FILES:\n\u001b[1;32m     92\u001b[0m     seqs \u001b[38;5;241m=\u001b[39m parse_a3m(p)\n\u001b[0;32m---> 93\u001b[0m     dt \u001b[38;5;241m=\u001b[39m \u001b[43mtimed_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_converter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_autocast\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     times\u001b[38;5;241m.\u001b[39mappend(dt)\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<35\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdt\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m8.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m s\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 59\u001b[0m, in \u001b[0;36mtimed_forward\u001b[0;34m(model, batch_converter, seqs, device, use_autocast)\u001b[0m\n\u001b[1;32m     53\u001b[0m     ctx \u001b[38;5;241m=\u001b[39m autocast(\n\u001b[1;32m     54\u001b[0m         device_type\u001b[38;5;241m=\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype,\n\u001b[1;32m     55\u001b[0m         enabled\u001b[38;5;241m=\u001b[39muse_autocast,\n\u001b[1;32m     56\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16,\n\u001b[1;32m     57\u001b[0m     )\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx:\n\u001b[0;32m---> 59\u001b[0m         _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtok\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepr_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepresentations\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m12\u001b[39m]\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     62\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39msynchronize(device)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:655\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 655\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverbose:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/esm/model/msa_transformer.py:153\u001b[0m, in \u001b[0;36mMSATransformer.forward\u001b[0;34m(self, tokens, repr_layers, need_head_weights, return_contacts)\u001b[0m\n\u001b[1;32m    151\u001b[0m batch_size, num_alignments, seqlen \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    152\u001b[0m padding_mask \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39meq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx)  \u001b[38;5;66;03m# B, R, C\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m padding_mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m    154\u001b[0m     padding_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    156\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens(tokens)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/esm/model/msa_transformer.py:153\u001b[0m, in \u001b[0;36mtorch_dynamo_resume_in_forward_at_153\u001b[0;34m(self, tokens, repr_layers, need_head_weights, return_contacts, batch_size, num_alignments, seqlen)\u001b[0m\n\u001b[1;32m    151\u001b[0m batch_size, num_alignments, seqlen \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m    152\u001b[0m padding_mask \u001b[38;5;241m=\u001b[39m tokens\u001b[38;5;241m.\u001b[39meq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx)  \u001b[38;5;66;03m# B, R, C\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m padding_mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m    154\u001b[0m     padding_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    156\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens(tokens)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:838\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    836\u001b[0m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback))\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    840\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:1201\u001b[0m, in \u001b[0;36maot_module_simplified.<locals>.forward\u001b[0;34m(*runtime_args)\u001b[0m\n\u001b[1;32m   1199\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(params_flat)\n\u001b[1;32m   1200\u001b[0m full_args\u001b[38;5;241m.\u001b[39mextend(runtime_args)\n\u001b[0;32m-> 1201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:328\u001b[0m, in \u001b[0;36m_create_runtime_wrapper.<locals>.runtime_wrapper\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m grad_enabled:\n\u001b[1;32m    327\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_set_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 328\u001b[0m     all_outs \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_at_runtime_with_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompiled_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_amp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteal_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m grad_enabled:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/utils.py:126\u001b[0m, in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 126\u001b[0m         out \u001b[38;5;241m=\u001b[39m normalize_as_list(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m         \u001b[38;5;66;03m# TODO: Please remove soon\u001b[39;00m\n\u001b[1;32m    129\u001b[0m         \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670\u001b[39;00m\n\u001b[1;32m    130\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour compiler for AOTAutograd is returning a function that doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt take boxed arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease wrap it with functorch.compile.make_boxed_func or handle the boxed arguments yourself. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    133\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://github.com/pytorch/pytorch/pull/83137#issuecomment-1211320670 for rationale.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m         )\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:689\u001b[0m, in \u001b[0;36mEffectTokensWrapper.post_compile.<locals>.inner_fn\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    686\u001b[0m     args \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m([\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m num_tokens), \u001b[38;5;241m*\u001b[39margs]\n\u001b[1;32m    687\u001b[0m     old_args\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m--> 689\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;66;03m# Inductor cache DummyModule can return None\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:495\u001b[0m, in \u001b[0;36mFunctionalizedRngRuntimeWrapper.post_compile.<locals>.wrapper\u001b[0;34m(runtime_args)\u001b[0m\n\u001b[1;32m    488\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_functionalized_rng_runtime_epilogue(\n\u001b[1;32m    489\u001b[0m         runtime_metadata,\n\u001b[1;32m    490\u001b[0m         out,\n\u001b[1;32m    491\u001b[0m         \u001b[38;5;66;03m# TODO: this won't be right for the backward when we convert the call_compiled_backward to use the wrapper\u001b[39;00m\n\u001b[1;32m    492\u001b[0m         runtime_metadata\u001b[38;5;241m.\u001b[39mnum_forward_returns,\n\u001b[1;32m    493\u001b[0m     )\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m--> 495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiled_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mruntime_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/output_code.py:460\u001b[0m, in \u001b[0;36mCompiledFxGraph.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_callable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_callable\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     get_runtime_metrics_context()\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/utils.py:2404\u001b[0m, in \u001b[0;36malign_inputs_from_check_idxs.<locals>.run\u001b[0;34m(new_inputs)\u001b[0m\n\u001b[1;32m   2402\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(new_inputs: \u001b[38;5;28mlist\u001b[39m[InputType]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   2403\u001b[0m     copy_misaligned_inputs(new_inputs, inputs_to_check)\n\u001b[0;32m-> 2404\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/tmp/torchinductor_rfrjoon/3n/c3nadkithmp6yx7vdggu5a3movhalqyidjdiov6w64wzmqzged4o.py:3172\u001b[0m, in \u001b[0;36mcall\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m   3170\u001b[0m \u001b[38;5;66;03m# Topologically Sorted Source Nodes: [attn_weights_3], Original ATen: [aten.clone]\u001b[39;00m\n\u001b[1;32m   3171\u001b[0m stream0 \u001b[38;5;241m=\u001b[39m get_raw_stream(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m-> 3172\u001b[0m \u001b[43mtriton_poi_fused_clone_22\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf80\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg25_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuf82\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m14592\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3173\u001b[0m buf83 \u001b[38;5;241m=\u001b[39m empty_strided_cuda((\u001b[38;5;241m228\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m), (\u001b[38;5;241m65536\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m1\u001b[39m), torch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[1;32m   3174\u001b[0m \u001b[38;5;66;03m# Topologically Sorted Source Nodes: [attn_weights_3], Original ATen: [aten.bmm]\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py:909\u001b[0m, in \u001b[0;36mCachingAutotuner.run\u001b[0;34m(self, stream, benchmark_run, *args, **kwargs)\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecompile_time_taken_ns \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlaunchers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 909\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautotune_to_one_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[1;32m    912\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlaunchers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mconfig, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_by_coordesc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    913\u001b[0m ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minductor_meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoordinate_descent_tuning\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlaunchers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    915\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoordinate_descent_tuning(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlaunchers[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    916\u001b[0m     ]\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py:763\u001b[0m, in \u001b[0;36mCachingAutotuner.autotune_to_one_config\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Do the actual autotuning\"\"\"\u001b[39;00m\n\u001b[1;32m    762\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns()\n\u001b[0;32m--> 763\u001b[0m timings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbenchmark_all_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    764\u001b[0m benchmark_time_taken_ns \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlaunchers \u001b[38;5;241m=\u001b[39m [builtins\u001b[38;5;241m.\u001b[39mmin(timings, key\u001b[38;5;241m=\u001b[39mtimings\u001b[38;5;241m.\u001b[39mget)]\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py:737\u001b[0m, in \u001b[0;36mCachingAutotuner.benchmark_all_configs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbenchmark_all_configs\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[1;32m    731\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCachingAutotuner.benchmark_all_configs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    732\u001b[0m         log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[38;5;66;03m# dynamo_compile_runtime_column_us=\"runtime_triton_autotune_time_us\",\u001b[39;00m\n\u001b[1;32m    736\u001b[0m     ):\n\u001b[0;32m--> 737\u001b[0m         timings \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    738\u001b[0m             launcher: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbench(launcher, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    739\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m launcher \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlaunchers\n\u001b[1;32m    740\u001b[0m         }\n\u001b[1;32m    742\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m timings\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    743\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoordesc_tuner\u001b[38;5;241m.\u001b[39mcache_benchmark_result(k\u001b[38;5;241m.\u001b[39mconfig, v)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py:738\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbenchmark_all_configs\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[1;32m    731\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCachingAutotuner.benchmark_all_configs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    732\u001b[0m         log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[38;5;66;03m# dynamo_compile_runtime_column_us=\"runtime_triton_autotune_time_us\",\u001b[39;00m\n\u001b[1;32m    736\u001b[0m     ):\n\u001b[1;32m    737\u001b[0m         timings \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 738\u001b[0m             launcher: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbench\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlauncher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m launcher \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlaunchers\n\u001b[1;32m    740\u001b[0m         }\n\u001b[1;32m    742\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m timings\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    743\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoordesc_tuner\u001b[38;5;241m.\u001b[39mcache_benchmark_result(k\u001b[38;5;241m.\u001b[39mconfig, v)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/runtime/triton_heuristics.py:616\u001b[0m, in \u001b[0;36mCachingAutotuner.bench\u001b[0;34m(self, launcher, with_profiler, *args, **kwargs)\u001b[0m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_props\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m benchmarker\u001b[38;5;241m.\u001b[39mbenchmark_cpu(kernel_call)\n\u001b[0;32m--> 616\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbenchmarker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbenchmark_gpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_call\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/runtime/benchmarking.py:39\u001b[0m, in \u001b[0;36mtime_and_count.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minductor\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenchmarking.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfn_qual_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(fn_qual_name, log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/_inductor/runtime/benchmarking.py:276\u001b[0m, in \u001b[0;36mInductorBenchmarker.benchmark_gpu\u001b[0;34m(self, _callable, estimation_iters, memory_warmup_iters, benchmark_iters, max_benchmark_duration, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m     _callable()\n\u001b[1;32m    275\u001b[0m     end_event\u001b[38;5;241m.\u001b[39mrecord()\n\u001b[0;32m--> 276\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m benchmarked_timing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_event_pairs_min_timing(event_pairs)\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# explicitly delete the buffer, sometimes helps memory\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;66;03m# footprint metrics in OSS Inductor performance benchmarks\u001b[39;00m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/cuda/__init__.py:1040\u001b[0m, in \u001b[0;36msynchronize\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m   1038\u001b[0m _lazy_init()\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[0;32m-> 1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_synchronize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random, time, statistics, sys\n",
    "from typing import List\n",
    "import torch\n",
    "from torch import autocast  # Changed from torch.cuda.amp import autocast\n",
    "# ==============================\n",
    "# CONFIG — adjust as needed\n",
    "# ==============================\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_ROOT = Path(\"/teamspace/studios/this_studio/PFP_Testing/data/PDBCH/train_pdbch\")\n",
    "A3M_NAME  = \"final_filtered_256_stripped.a3m\"\n",
    "N_SAMPLES = 16          # number of random MSAs to time\n",
    "SEED      = 42          # reproducible sampling\n",
    "DEVICE_GPU = \"cuda\"     # change to \"cuda:1\" etc. if needed\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Helper functions\n",
    "# --------------------------------------------------\n",
    "def find_a3m_files(root: Path, name: str) -> List[Path]:\n",
    "    return list(root.rglob(name))\n",
    "\n",
    "def parse_a3m(path: Path) -> List[str]:\n",
    "    \"\"\"Return list of sequences (query first) from an A3M file.\"\"\"\n",
    "    seqs, seq = [], []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as fh:\n",
    "        for line in fh:\n",
    "            line = line.rstrip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\">\"):\n",
    "                if seq:\n",
    "                    seqs.append(\"\".join(seq))\n",
    "                seq = []\n",
    "            else:\n",
    "                seq.append(line)\n",
    "        if seq:\n",
    "            seqs.append(\"\".join(seq))\n",
    "    if not (1 <= len(seqs) <= 256):\n",
    "        raise ValueError(f\"{path}: expected 1–256 sequences, got {len(seqs)}\")\n",
    "    return seqs\n",
    "\n",
    "def timed_forward(model, batch_converter, seqs, device, use_autocast=False):\n",
    "    \"\"\"Return wall-clock seconds for one forward pass (sync'ed).\"\"\"\n",
    "    msa = [(f\"seq{i}\", s) for i, s in enumerate(seqs)]\n",
    "    _, _, tok = batch_converter([msa])\n",
    "    tok = tok.to(device, non_blocking=True)\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize(device)\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ctx = autocast(\n",
    "            device_type=device.type,\n",
    "            enabled=use_autocast,\n",
    "            dtype=torch.bfloat16,\n",
    "        )\n",
    "        with ctx:\n",
    "            _ = model(tok, repr_layers=[12])[\"representations\"][12]\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize(device)\n",
    "    return time.perf_counter() - t0\n",
    "\n",
    "def print_stats(label, times):\n",
    "    mean, med = statistics.mean(times), statistics.median(times)\n",
    "    sd = statistics.stdev(times) if len(times) > 1 else 0.0\n",
    "    print(f\"\\n{label}  —  summary (sec)\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  mean   : {mean:8.4f}\")\n",
    "    print(f\"  median : {med:8.4f}\")\n",
    "    print(f\"  stddev : {sd:8.4f}\")\n",
    "    print(f\"  min    : {min(times):8.4f}\")\n",
    "    print(f\"  max    : {max(times):8.4f}\")\n",
    "\n",
    "def run_variant(label, device, compile_model=False, use_autocast=False):\n",
    "    import esm  # local import keeps notebook cell light\n",
    "    print(f\"\\n===== {label} =====\")\n",
    "    # -------- load / compile (excluded from timing) --------\n",
    "    model, alphabet = esm.pretrained.esm_msa1b_t12_100M_UR50S()\n",
    "    model.eval().to(device)\n",
    "    if compile_model:\n",
    "        print(\"Compiling model …\")\n",
    "        model = torch.compile(model, mode=\"default\", dynamic=True)\n",
    "        # 1 dummy pass to finish compile graph (not timed)\n",
    "        _ = model(torch.randint(0, 20, (1, 1, 16), device=device))\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "    # -------- per-file timing ------------------------------\n",
    "    times = []\n",
    "    for p in SAMPLE_FILES:\n",
    "        seqs = parse_a3m(p)\n",
    "        dt = timed_forward(model, batch_converter, seqs, device, use_autocast)\n",
    "        times.append(dt)\n",
    "        print(f\"{p.name:<35} {dt:8.4f} s\")\n",
    "\n",
    "    print_stats(label, times)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Pick files & run\n",
    "# --------------------------------------------------\n",
    "random.seed(SEED)\n",
    "all_a3m = find_a3m_files(DATA_ROOT, A3M_NAME)\n",
    "if len(all_a3m) < N_SAMPLES:\n",
    "    raise RuntimeError(f\"Found only {len(all_a3m)} A3M files (need {N_SAMPLES}).\")\n",
    "SAMPLE_FILES = random.sample(all_a3m, N_SAMPLES)\n",
    "print(f\"Selected {N_SAMPLES} random A3M files (seed={SEED}).\")\n",
    "\n",
    "# Devices\n",
    "gpu = torch.device(DEVICE_GPU if torch.cuda.is_available() else \"cpu\")\n",
    "cpu = torch.device(\"cpu\")\n",
    "\n",
    "# Run the four variants\n",
    "run_variant(\"GPU | eager FP32\",          gpu, compile_model=False, use_autocast=False)\n",
    "run_variant(\"GPU | eager BF16 (autocast)\", gpu, compile_model=False, use_autocast=True)\n",
    "run_variant(\"GPU | compiled + BF16\",     gpu, compile_model=True,  use_autocast=True)\n",
    "run_variant(\"CPU | eager FP32\",          cpu, compile_model=False, use_autocast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76d5bf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 16 random A3M files for testing (seed=42)\n",
      "Device: cuda\n",
      "============================================================\n",
      "ACCURACY TEST: FP32 vs BF16\n",
      "============================================================\n",
      "\n",
      "File 1/16: final_filtered_256_stripped.a3m\n",
      "  Sequences in MSA: 256\n",
      "  Query length: 183\n",
      "  FP32 time: 0.6552s\n",
      "  BF16 time: 0.1555s\n",
      "  Speedup: 4.21x\n",
      "  Cosine similarity: 1.006351\n",
      "  Pearson correlation: 0.999922\n",
      "  Relative error: 14.5984%\n",
      "  MSE: 1.12e-04\n",
      "\n",
      "File 2/16: final_filtered_256_stripped.a3m\n",
      "  Sequences in MSA: 256\n",
      "  Query length: 82\n",
      "  FP32 time: 0.1456s\n",
      "  BF16 time: 0.0561s\n",
      "  Speedup: 2.60x\n",
      "  Cosine similarity: 1.001910\n",
      "  Pearson correlation: 0.999928\n",
      "  Relative error: 15.8799%\n",
      "  MSE: 1.01e-04\n",
      "\n",
      "File 3/16: final_filtered_256_stripped.a3m\n",
      "  Sequences in MSA: 230\n",
      "  Query length: 159\n",
      "  FP32 time: 0.2435s\n",
      "  BF16 time: 0.0895s\n",
      "  Speedup: 2.72x\n",
      "  Cosine similarity: 1.004292\n",
      "  Pearson correlation: 0.999965\n",
      "  Relative error: 11.7620%\n",
      "  MSE: 5.37e-05\n",
      "\n",
      "File 4/16: final_filtered_256_stripped.a3m\n",
      "  Sequences in MSA: 256\n",
      "  Query length: 75\n",
      "  FP32 time: 0.1364s\n",
      "  BF16 time: 0.0543s\n",
      "  Speedup: 2.51x\n",
      "  Cosine similarity: 1.001876\n",
      "  Pearson correlation: 0.999961\n",
      "  Relative error: 12.7274%\n",
      "  MSE: 5.33e-05\n",
      "\n",
      "File 5/16: final_filtered_256_stripped.a3m\n",
      "  Sequences in MSA: 256\n",
      "  Query length: 152\n",
      "  FP32 time: 0.2596s\n",
      "  BF16 time: 0.4788s\n",
      "  Speedup: 0.54x\n",
      "  Cosine similarity: 1.005147\n",
      "  Pearson correlation: 0.999924\n",
      "  Relative error: 15.7289%\n",
      "  MSE: 1.07e-04\n",
      "\n",
      "File 6/16: final_filtered_256_stripped.a3m\n",
      "  Sequences in MSA: 256\n",
      "  Query length: 96\n",
      "  FP32 time: 0.1704s\n",
      "  BF16 time: 0.0575s\n",
      "  Speedup: 2.96x\n",
      "  Cosine similarity: 1.002608\n",
      "  Pearson correlation: 0.999967\n",
      "  Relative error: 9.7327%\n",
      "  MSE: 4.52e-05\n",
      "\n",
      "File 7/16: final_filtered_256_stripped.a3m\n",
      "  Sequences in MSA: 256\n",
      "  Query length: 108\n",
      "  FP32 time: 0.1923s\n",
      "  BF16 time: 0.0648s\n",
      "  Speedup: 2.97x\n",
      "  Cosine similarity: 1.003345\n",
      "  Pearson correlation: 0.999949\n",
      "  Relative error: 13.3421%\n",
      "  MSE: 7.19e-05\n",
      "\n",
      "File 8/16: final_filtered_256_stripped.a3m\n",
      "  Sequences in MSA: 256\n",
      "  Query length: 323\n",
      "  FP32 time: 0.5440s\n",
      "  BF16 time: 0.1700s\n",
      "  Speedup: 3.20x\n",
      "  Cosine similarity: 1.015102\n",
      "  Pearson correlation: 0.999923\n",
      "  Relative error: 18.5253%\n",
      "  MSE: 1.16e-04\n",
      "\n",
      "File 9/16: final_filtered_256_stripped.a3m\n",
      "  Sequences in MSA: 256\n",
      "  Query length: 265\n",
      "  FP32 time: 0.4513s\n",
      "  BF16 time: 0.1389s\n",
      "  Speedup: 3.25x\n",
      "  Cosine similarity: 1.012332\n",
      "  Pearson correlation: 0.999922\n",
      "  Relative error: 17.8364%\n",
      "  MSE: 1.10e-04\n",
      "\n",
      "File 10/16: final_filtered_256_stripped.a3m\n",
      "  Sequences in MSA: 256\n",
      "  Query length: 110\n",
      "  FP32 time: 0.2005s\n",
      "  BF16 time: 0.0642s\n",
      "  Speedup: 3.12x\n",
      "  Cosine similarity: 1.003395\n",
      "  Pearson correlation: 0.999957\n",
      "  Relative error: 11.8469%\n",
      "  MSE: 6.31e-05\n",
      "\n",
      "File 11/16: final_filtered_256_stripped.a3m\n",
      "  Sequences in MSA: 256\n",
      "  Query length: 741\n",
      "  FP32 time: 1.2913s\n",
      "  BF16 time: 0.3771s\n",
      "  Speedup: 3.42x\n",
      "  Cosine similarity: 1.042223\n",
      "  Pearson correlation: 0.999912\n",
      "  Relative error: 16.7087%\n",
      "  MSE: 1.37e-04\n",
      "\n",
      "File 12/16: final_filtered_256_stripped.a3m\n",
      "  Sequences in MSA: 256\n",
      "  Query length: 359\n",
      "  FP32 time: 0.6026s\n",
      "  BF16 time: 0.1768s\n",
      "  Speedup: 3.41x\n",
      "  Cosine similarity: 1.017891\n",
      "  Pearson correlation: 0.999923\n",
      "  Relative error: 16.5607%\n",
      "  MSE: 1.07e-04\n",
      "\n",
      "File 13/16: final_filtered_256_stripped.a3m\n",
      "  Sequences in MSA: 256\n",
      "  Query length: 190\n",
      "  FP32 time: 0.3169s\n",
      "  BF16 time: 0.1052s\n",
      "  Speedup: 3.01x\n",
      "  Cosine similarity: 1.006948\n",
      "  Pearson correlation: 0.999923\n",
      "  Relative error: 15.8149%\n",
      "  MSE: 1.10e-04\n",
      "\n",
      "File 14/16: final_filtered_256_stripped.a3m\n",
      "  Sequences in MSA: 256\n",
      "  Query length: 518\n",
      "  FP32 time: 0.8940s\n",
      "  BF16 time: 0.2870s\n",
      "  Speedup: 3.12x\n",
      "  Cosine similarity: 1.028600\n",
      "  Pearson correlation: 0.999966\n",
      "  Relative error: 9.4102%\n",
      "  MSE: 5.10e-05\n",
      "\n",
      "File 15/16: final_filtered_256_stripped.a3m\n",
      "  Sequences in MSA: 256\n",
      "  Query length: 285\n",
      "  FP32 time: 0.4790s\n",
      "  BF16 time: 0.1452s\n",
      "  Speedup: 3.30x\n",
      "  Cosine similarity: 1.013186\n",
      "  Pearson correlation: 0.999934\n",
      "  Relative error: 13.5438%\n",
      "  MSE: 9.28e-05\n",
      "\n",
      "File 16/16: final_filtered_256_stripped.a3m\n",
      "  Sequences in MSA: 256\n",
      "  Query length: 250\n",
      "  FP32 time: 0.4193s\n",
      "  BF16 time: 0.1335s\n",
      "  Speedup: 3.14x\n",
      "  Cosine similarity: 1.010935\n",
      "  Pearson correlation: 0.999934\n",
      "  Relative error: 13.8591%\n",
      "  MSE: 1.01e-04\n",
      "\n",
      "========================================\n",
      "ACCURACY SUMMARY\n",
      "========================================\n",
      "Average cosine similarity: 1.011009\n",
      "Average Pearson correlation: 0.999938\n",
      "Average relative error: 14.2424%\n",
      "Average speedup: 2.97x\n",
      "\n",
      "============================================================\n",
      "SPEED TEST: Full MSA vs 32 sequences (BF16)\n",
      "============================================================\n",
      "\n",
      "File 1/16: final_filtered_256_stripped.a3m\n",
      "  Full MSA (256 seqs): 0.0900s\n",
      "  Limited MSA (32 seqs): 0.0351s\n",
      "  Speedup: 2.56x\n",
      "\n",
      "File 2/16: final_filtered_256_stripped.a3m\n",
      "  Full MSA (256 seqs): 0.0485s\n",
      "  Limited MSA (32 seqs): 0.0187s\n",
      "  Speedup: 2.59x\n",
      "\n",
      "File 3/16: final_filtered_256_stripped.a3m\n",
      "  Full MSA (230 seqs): 0.0720s\n",
      "  Limited MSA (32 seqs): 0.0181s\n",
      "  Speedup: 3.98x\n",
      "\n",
      "File 4/16: final_filtered_256_stripped.a3m\n",
      "  Full MSA (256 seqs): 0.0422s\n",
      "  Limited MSA (32 seqs): 0.0184s\n",
      "  Speedup: 2.29x\n",
      "\n",
      "File 5/16: final_filtered_256_stripped.a3m\n",
      "  Full MSA (256 seqs): 0.0820s\n",
      "  Limited MSA (32 seqs): 0.0153s\n",
      "  Speedup: 5.36x\n",
      "\n",
      "File 6/16: final_filtered_256_stripped.a3m\n",
      "  Full MSA (256 seqs): 0.0543s\n",
      "  Limited MSA (32 seqs): 0.0203s\n",
      "  Speedup: 2.68x\n",
      "\n",
      "File 7/16: final_filtered_256_stripped.a3m\n",
      "  Full MSA (256 seqs): 0.0602s\n",
      "  Limited MSA (32 seqs): 0.0168s\n",
      "  Speedup: 3.57x\n",
      "\n",
      "File 8/16: final_filtered_256_stripped.a3m\n",
      "  Full MSA (256 seqs): 0.1671s\n",
      "  Limited MSA (32 seqs): 0.0185s\n",
      "  Speedup: 9.02x\n",
      "\n",
      "File 9/16: final_filtered_256_stripped.a3m\n",
      "  Full MSA (256 seqs): 0.1355s\n",
      "  Limited MSA (32 seqs): 0.0195s\n",
      "  Speedup: 6.96x\n",
      "\n",
      "File 10/16: final_filtered_256_stripped.a3m\n",
      "  Full MSA (256 seqs): 0.0613s\n",
      "  Limited MSA (32 seqs): 0.0181s\n",
      "  Speedup: 3.39x\n",
      "\n",
      "File 11/16: final_filtered_256_stripped.a3m\n",
      "  Full MSA (256 seqs): 0.3766s\n",
      "  Limited MSA (32 seqs): 0.0439s\n",
      "  Speedup: 8.57x\n",
      "\n",
      "File 12/16: final_filtered_256_stripped.a3m\n",
      "  Full MSA (256 seqs): 0.1741s\n",
      "  Limited MSA (32 seqs): 0.0194s\n",
      "  Speedup: 8.96x\n",
      "\n",
      "File 13/16: final_filtered_256_stripped.a3m\n",
      "  Full MSA (256 seqs): 0.1004s\n",
      "  Limited MSA (32 seqs): 0.0171s\n",
      "  Speedup: 5.87x\n",
      "\n",
      "File 14/16: final_filtered_256_stripped.a3m\n",
      "  Full MSA (256 seqs): 0.2830s\n",
      "  Limited MSA (32 seqs): 0.0408s\n",
      "  Speedup: 6.93x\n",
      "\n",
      "File 15/16: final_filtered_256_stripped.a3m\n",
      "  Full MSA (256 seqs): 0.1450s\n",
      "  Limited MSA (32 seqs): 0.0164s\n",
      "  Speedup: 8.82x\n",
      "\n",
      "File 16/16: final_filtered_256_stripped.a3m\n",
      "  Full MSA (256 seqs): 0.1324s\n",
      "  Limited MSA (32 seqs): 0.0150s\n",
      "  Speedup: 8.80x\n",
      "\n",
      "========================================\n",
      "SEQUENCE LIMIT SUMMARY\n",
      "========================================\n",
      "Average full MSA time: 0.1265s\n",
      "Average limited MSA time: 0.0220s\n",
      "Average speedup: 5.76x\n",
      "Memory/compute reduction: ~87.5%\n",
      "\n",
      "============================================================\n",
      "TESTING COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import random, time, statistics\n",
    "import torch\n",
    "from torch import autocast\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ==============================\n",
    "# CONFIG\n",
    "# ==============================\n",
    "DATA_ROOT = Path(\"/teamspace/studios/this_studio/PFP_Testing/data/PDBCH/train_pdbch\")\n",
    "A3M_NAME = \"final_filtered_256_stripped.a3m\"\n",
    "N_TEST_FILES = 16        # number of files for accuracy/speed testing\n",
    "SEED = 42\n",
    "DEVICE_GPU = \"cuda\"\n",
    "MAX_SEQ_LIMIT = 32      # test with this many sequences vs full MSA\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Helper functions\n",
    "# --------------------------------------------------\n",
    "def find_a3m_files(root: Path, name: str) -> List[Path]:\n",
    "    return list(root.rglob(name))\n",
    "\n",
    "def parse_a3m(path: Path, max_sequences: int = None) -> List[str]:\n",
    "    \"\"\"Return list of sequences from A3M file, optionally limited.\"\"\"\n",
    "    seqs, seq = [], []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as fh:\n",
    "        for line in fh:\n",
    "            line = line.rstrip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\">\"):\n",
    "                if seq:\n",
    "                    seqs.append(\"\".join(seq))\n",
    "                    if max_sequences and len(seqs) >= max_sequences:\n",
    "                        break\n",
    "                seq = []\n",
    "            else:\n",
    "                seq.append(line)\n",
    "        if seq and (not max_sequences or len(seqs) < max_sequences):\n",
    "            seqs.append(\"\".join(seq))\n",
    "    \n",
    "    if not seqs:\n",
    "        raise ValueError(f\"{path}: no sequences found\")\n",
    "    \n",
    "    return seqs\n",
    "\n",
    "def get_model_output(model, batch_converter, seqs, device, use_autocast=False):\n",
    "    \"\"\"Get model representations and timing.\"\"\"\n",
    "    msa = [(f\"seq{i}\", s) for i, s in enumerate(seqs)]\n",
    "    _, _, tok = batch_converter([msa])\n",
    "    tok = tok.to(device, non_blocking=True)\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize(device)\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ctx = autocast(\n",
    "            device_type=device.type,\n",
    "            enabled=use_autocast,\n",
    "            dtype=torch.bfloat16,\n",
    "        )\n",
    "        with ctx:\n",
    "            output = model(tok, repr_layers=[12])[\"representations\"][12]\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize(device)\n",
    "    \n",
    "    elapsed_time = time.perf_counter() - t0\n",
    "    return output, elapsed_time\n",
    "\n",
    "def calculate_similarity_metrics(tensor1, tensor2):\n",
    "    \"\"\"Calculate various similarity metrics between two tensors.\"\"\"\n",
    "    # Flatten tensors for easier comparison\n",
    "    flat1 = tensor1.flatten().float()\n",
    "    flat2 = tensor2.flatten().float()\n",
    "    \n",
    "    # Cosine similarity\n",
    "    cos_sim = F.cosine_similarity(flat1.unsqueeze(0), flat2.unsqueeze(0)).item()\n",
    "    \n",
    "    # Mean squared error\n",
    "    mse = F.mse_loss(flat1, flat2).item()\n",
    "    \n",
    "    # Mean absolute error\n",
    "    mae = F.l1_loss(flat1, flat2).item()\n",
    "    \n",
    "    # Relative error (%)\n",
    "    rel_error = (torch.abs(flat1 - flat2) / (torch.abs(flat1) + 1e-8)).mean().item() * 100\n",
    "    \n",
    "    # Pearson correlation\n",
    "    corr = torch.corrcoef(torch.stack([flat1, flat2]))[0, 1].item()\n",
    "    \n",
    "    return {\n",
    "        'cosine_similarity': cos_sim,\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'relative_error_pct': rel_error,\n",
    "        'pearson_correlation': corr\n",
    "    }\n",
    "\n",
    "def test_accuracy_fp32_vs_bf16():\n",
    "    \"\"\"Test accuracy difference between FP32 and BF16.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ACCURACY TEST: FP32 vs BF16\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    import esm\n",
    "    model, alphabet = esm.pretrained.esm_msa1b_t12_100M_UR50S()\n",
    "    model.eval().to(device)\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    \n",
    "    all_metrics = []\n",
    "    \n",
    "    for i, file_path in enumerate(test_files):\n",
    "        print(f\"\\nFile {i+1}/{len(test_files)}: {file_path.name}\")\n",
    "        seqs = parse_a3m(file_path)\n",
    "        print(f\"  Sequences in MSA: {len(seqs)}\")\n",
    "        print(f\"  Query length: {len(seqs[0])}\")\n",
    "        \n",
    "        # FP32 run\n",
    "        fp32_output, fp32_time = get_model_output(model, batch_converter, seqs, device, use_autocast=False)\n",
    "        \n",
    "        # BF16 run  \n",
    "        bf16_output, bf16_time = get_model_output(model, batch_converter, seqs, device, use_autocast=True)\n",
    "        \n",
    "        # Calculate similarity metrics\n",
    "        metrics = calculate_similarity_metrics(fp32_output.cpu(), bf16_output.cpu())\n",
    "        metrics['speedup'] = fp32_time / bf16_time\n",
    "        metrics['fp32_time'] = fp32_time\n",
    "        metrics['bf16_time'] = bf16_time\n",
    "        \n",
    "        all_metrics.append(metrics)\n",
    "        \n",
    "        print(f\"  FP32 time: {fp32_time:.4f}s\")\n",
    "        print(f\"  BF16 time: {bf16_time:.4f}s\") \n",
    "        print(f\"  Speedup: {metrics['speedup']:.2f}x\")\n",
    "        print(f\"  Cosine similarity: {metrics['cosine_similarity']:.6f}\")\n",
    "        print(f\"  Pearson correlation: {metrics['pearson_correlation']:.6f}\")\n",
    "        print(f\"  Relative error: {metrics['relative_error_pct']:.4f}%\")\n",
    "        print(f\"  MSE: {metrics['mse']:.2e}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"ACCURACY SUMMARY\")\n",
    "    print(\"=\" * 40)\n",
    "    avg_cosine = np.mean([m['cosine_similarity'] for m in all_metrics])\n",
    "    avg_corr = np.mean([m['pearson_correlation'] for m in all_metrics])\n",
    "    avg_rel_err = np.mean([m['relative_error_pct'] for m in all_metrics])\n",
    "    avg_speedup = np.mean([m['speedup'] for m in all_metrics])\n",
    "    \n",
    "    print(f\"Average cosine similarity: {avg_cosine:.6f}\")\n",
    "    print(f\"Average Pearson correlation: {avg_corr:.6f}\") \n",
    "    print(f\"Average relative error: {avg_rel_err:.4f}%\")\n",
    "    print(f\"Average speedup: {avg_speedup:.2f}x\")\n",
    "    \n",
    "    return all_metrics\n",
    "\n",
    "def test_sequence_limit_speed():\n",
    "    \"\"\"Test speed difference with limited vs full MSA sequences.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"SPEED TEST: Full MSA vs {MAX_SEQ_LIMIT} sequences (BF16)\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    import esm\n",
    "    model, alphabet = esm.pretrained.esm_msa1b_t12_100M_UR50S()\n",
    "    model.eval().to(device)\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    \n",
    "    full_times = []\n",
    "    limited_times = []\n",
    "    \n",
    "    for i, file_path in enumerate(test_files):\n",
    "        print(f\"\\nFile {i+1}/{len(test_files)}: {file_path.name}\")\n",
    "        \n",
    "        # Full MSA\n",
    "        seqs_full = parse_a3m(file_path)\n",
    "        _, time_full = get_model_output(model, batch_converter, seqs_full, device, use_autocast=True)\n",
    "        \n",
    "        # Limited MSA\n",
    "        seqs_limited = parse_a3m(file_path, max_sequences=MAX_SEQ_LIMIT)\n",
    "        _, time_limited = get_model_output(model, batch_converter, seqs_limited, device, use_autocast=True)\n",
    "        \n",
    "        speedup = time_full / time_limited\n",
    "        \n",
    "        full_times.append(time_full)\n",
    "        limited_times.append(time_limited)\n",
    "        \n",
    "        print(f\"  Full MSA ({len(seqs_full)} seqs): {time_full:.4f}s\")\n",
    "        print(f\"  Limited MSA ({len(seqs_limited)} seqs): {time_limited:.4f}s\")\n",
    "        print(f\"  Speedup: {speedup:.2f}x\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"SEQUENCE LIMIT SUMMARY\")\n",
    "    print(\"=\" * 40)\n",
    "    avg_full = np.mean(full_times)\n",
    "    avg_limited = np.mean(limited_times)\n",
    "    avg_speedup = avg_full / avg_limited\n",
    "    \n",
    "    print(f\"Average full MSA time: {avg_full:.4f}s\")\n",
    "    print(f\"Average limited MSA time: {avg_limited:.4f}s\")\n",
    "    print(f\"Average speedup: {avg_speedup:.2f}x\")\n",
    "    print(f\"Memory/compute reduction: ~{(1 - MAX_SEQ_LIMIT/256)*100:.1f}%\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Main execution\n",
    "# --------------------------------------------------\n",
    "random.seed(SEED)\n",
    "device = torch.device(DEVICE_GPU if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Find test files\n",
    "all_a3m = find_a3m_files(DATA_ROOT, A3M_NAME)\n",
    "if len(all_a3m) < N_TEST_FILES:\n",
    "    raise RuntimeError(f\"Found only {len(all_a3m)} A3M files (need {N_TEST_FILES}).\")\n",
    "\n",
    "test_files = random.sample(all_a3m, N_TEST_FILES)\n",
    "print(f\"Selected {N_TEST_FILES} random A3M files for testing (seed={SEED})\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Run tests\n",
    "accuracy_results = test_accuracy_fp32_vs_bf16()\n",
    "test_sequence_limit_speed()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TESTING COMPLETE\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55047b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 16 random A3M files (seed=42)\n",
      "Device: cuda\n",
      "\n",
      "Initial backend state:\n",
      "    Current backends - Flash: True, MemEff: False, Math: False\n",
      "\n",
      "[01/16] final_filtered_256_stripped.a3m                256 seqs  len=183\n",
      "    Using PyTorch default SDPA settings\n",
      "    Current backends - Flash: True, MemEff: False, Math: False\n",
      "    Explicitly enabled mem_efficient_sdp, disabled math_sdp\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    PyTorch Default    :   0.090s\n",
      "    Mem Efficient Expl :   0.090s   ratio 1.00×\n",
      "\n",
      "[02/16] final_filtered_256_stripped.a3m                256 seqs  len=82\n",
      "    Using PyTorch default SDPA settings\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    Explicitly enabled mem_efficient_sdp, disabled math_sdp\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    PyTorch Default    :   0.048s\n",
      "    Mem Efficient Expl :   0.048s   ratio 1.00×\n",
      "\n",
      "[03/16] final_filtered_256_stripped.a3m                230 seqs  len=159\n",
      "    Using PyTorch default SDPA settings\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    Explicitly enabled mem_efficient_sdp, disabled math_sdp\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    PyTorch Default    :   0.072s\n",
      "    Mem Efficient Expl :   0.072s   ratio 1.00×\n",
      "\n",
      "[04/16] final_filtered_256_stripped.a3m                256 seqs  len=75\n",
      "    Using PyTorch default SDPA settings\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    Explicitly enabled mem_efficient_sdp, disabled math_sdp\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    PyTorch Default    :   0.042s\n",
      "    Mem Efficient Expl :   0.042s   ratio 1.00×\n",
      "\n",
      "[05/16] final_filtered_256_stripped.a3m                256 seqs  len=152\n",
      "    Using PyTorch default SDPA settings\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    Explicitly enabled mem_efficient_sdp, disabled math_sdp\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    PyTorch Default    :   0.082s\n",
      "    Mem Efficient Expl :   0.082s   ratio 1.00×\n",
      "\n",
      "[06/16] final_filtered_256_stripped.a3m                256 seqs  len=96\n",
      "    Using PyTorch default SDPA settings\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    Explicitly enabled mem_efficient_sdp, disabled math_sdp\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    PyTorch Default    :   0.054s\n",
      "    Mem Efficient Expl :   0.054s   ratio 1.00×\n",
      "\n",
      "[07/16] final_filtered_256_stripped.a3m                256 seqs  len=108\n",
      "    Using PyTorch default SDPA settings\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    Explicitly enabled mem_efficient_sdp, disabled math_sdp\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    PyTorch Default    :   0.060s\n",
      "    Mem Efficient Expl :   0.060s   ratio 1.00×\n",
      "\n",
      "[08/16] final_filtered_256_stripped.a3m                256 seqs  len=323\n",
      "    Using PyTorch default SDPA settings\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    Explicitly enabled mem_efficient_sdp, disabled math_sdp\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    PyTorch Default    :   0.167s\n",
      "    Mem Efficient Expl :   0.166s   ratio 1.00×\n",
      "\n",
      "[09/16] final_filtered_256_stripped.a3m                256 seqs  len=265\n",
      "    Using PyTorch default SDPA settings\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    Explicitly enabled mem_efficient_sdp, disabled math_sdp\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    PyTorch Default    :   0.135s\n",
      "    Mem Efficient Expl :   0.135s   ratio 1.00×\n",
      "\n",
      "[10/16] final_filtered_256_stripped.a3m                256 seqs  len=110\n",
      "    Using PyTorch default SDPA settings\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    Explicitly enabled mem_efficient_sdp, disabled math_sdp\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    PyTorch Default    :   0.061s\n",
      "    Mem Efficient Expl :   0.061s   ratio 1.00×\n",
      "\n",
      "[11/16] final_filtered_256_stripped.a3m                256 seqs  len=741\n",
      "    Using PyTorch default SDPA settings\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    Explicitly enabled mem_efficient_sdp, disabled math_sdp\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    PyTorch Default    :   0.375s\n",
      "    Mem Efficient Expl :   0.376s   ratio 1.00×\n",
      "\n",
      "[12/16] final_filtered_256_stripped.a3m                256 seqs  len=359\n",
      "    Using PyTorch default SDPA settings\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    Explicitly enabled mem_efficient_sdp, disabled math_sdp\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    PyTorch Default    :   0.174s\n",
      "    Mem Efficient Expl :   0.174s   ratio 1.00×\n",
      "\n",
      "[13/16] final_filtered_256_stripped.a3m                256 seqs  len=190\n",
      "    Using PyTorch default SDPA settings\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    Explicitly enabled mem_efficient_sdp, disabled math_sdp\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    PyTorch Default    :   0.100s\n",
      "    Mem Efficient Expl :   0.100s   ratio 1.00×\n",
      "\n",
      "[14/16] final_filtered_256_stripped.a3m                256 seqs  len=518\n",
      "    Using PyTorch default SDPA settings\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    Explicitly enabled mem_efficient_sdp, disabled math_sdp\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    PyTorch Default    :   0.282s\n",
      "    Mem Efficient Expl :   0.282s   ratio 1.00×\n",
      "\n",
      "[15/16] final_filtered_256_stripped.a3m                256 seqs  len=285\n",
      "    Using PyTorch default SDPA settings\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    Explicitly enabled mem_efficient_sdp, disabled math_sdp\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    PyTorch Default    :   0.144s\n",
      "    Mem Efficient Expl :   0.144s   ratio 1.00×\n",
      "\n",
      "[16/16] final_filtered_256_stripped.a3m                256 seqs  len=250\n",
      "    Using PyTorch default SDPA settings\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    Explicitly enabled mem_efficient_sdp, disabled math_sdp\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n",
      "    PyTorch Default    :   0.132s\n",
      "    Mem Efficient Expl :   0.132s   ratio 1.00×\n",
      "\n",
      "\n",
      "------------------------------------------------------------\n",
      "Average PyTorch Default    :   0.126s\n",
      "Average Mem Efficient Expl :   0.126s\n",
      "Overall ratio              : 1.00×  (median 1.00×)\n",
      "------------------------------------------------------------\n",
      "\n",
      "Final backend state:\n",
      "    Current backends - Flash: True, MemEff: True, Math: False\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# MEM-EFFICIENT-SDPA SPEED TEST\n",
    "#   – BF16 autocast in both cases\n",
    "#   – compares PyTorch defaults vs explicit mem_efficient_sdp\n",
    "# ================================================================\n",
    "import os, random, time, statistics, torch, numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import random, time, statistics\n",
    "import torch\n",
    "from torch import autocast\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ------------------------------\n",
    "#  CONFIGURATION\n",
    "# ------------------------------\n",
    "DATA_ROOT        = Path(\"/teamspace/studios/this_studio/PFP_Testing/data/PDBCH/train_pdbch\")\n",
    "A3M_NAME         = \"final_filtered_256_stripped.a3m\"\n",
    "N_TEST_FILES     = 16        # ≥ number of *.a3m to draw\n",
    "SEED             = 42\n",
    "DEVICE           = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "assert DEVICE.type == \"cuda\", \"Need GPU for this test\"\n",
    "assert torch.cuda.get_device_capability(DEVICE)[0] >= 8, \"Need Ampere+ SM 80\"\n",
    "\n",
    "def find_a3m_files(root: Path, name: str) -> List[Path]:\n",
    "    return list(root.rglob(name))\n",
    "\n",
    "def parse_a3m(path: Path, max_sequences: int = None) -> List[str]:\n",
    "    \"\"\"Return list of sequences from A3M file, optionally limited.\"\"\"\n",
    "    seqs, seq = [], []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as fh:\n",
    "        for line in fh:\n",
    "            line = line.rstrip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\">\"):\n",
    "                if seq:\n",
    "                    seqs.append(\"\".join(seq))\n",
    "                    if max_sequences and len(seqs) >= max_sequences:\n",
    "                        break\n",
    "                seq = []\n",
    "            else:\n",
    "                seq.append(line)\n",
    "        if seq and (not max_sequences or len(seqs) < max_sequences):\n",
    "            seqs.append(\"\".join(seq))\n",
    "    \n",
    "    if not seqs:\n",
    "        raise ValueError(f\"{path}: no sequences found\")\n",
    "    \n",
    "    return seqs\n",
    "\n",
    "def get_model_output(model, batch_converter, seqs, device, use_autocast=False):\n",
    "    \"\"\"Get model representations and timing.\"\"\"\n",
    "    msa = [(f\"seq{i}\", s) for i, s in enumerate(seqs)]\n",
    "    _, _, tok = batch_converter([msa])\n",
    "    tok = tok.to(device, non_blocking=True)\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize(device)\n",
    "    t0 = time.perf_counter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ctx = autocast(\n",
    "            device_type=device.type,\n",
    "            enabled=use_autocast,\n",
    "            dtype=torch.bfloat16,\n",
    "        )\n",
    "        with ctx:\n",
    "            output = model(tok, repr_layers=[12])[\"representations\"][12]\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize(device)\n",
    "    \n",
    "    elapsed_time = time.perf_counter() - t0\n",
    "    return output, elapsed_time\n",
    "\n",
    "# ------------------------------\n",
    "#  SDPA BACKEND CONTROL\n",
    "# ------------------------------\n",
    "def set_pytorch_defaults():\n",
    "    \"\"\"Leave PyTorch SDPA settings at their defaults (do nothing).\"\"\"\n",
    "    # Don't touch any backends - let PyTorch decide\n",
    "    print(\"    Using PyTorch default SDPA settings\")\n",
    "\n",
    "def set_mem_efficient_explicit():\n",
    "    \"\"\"Explicitly enable mem_efficient_sdp and disable math_sdp.\"\"\"\n",
    "    # Don't touch flash_sdp - leave it as is\n",
    "    torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "    torch.backends.cuda.enable_math_sdp(False)\n",
    "    print(\"    Explicitly enabled mem_efficient_sdp, disabled math_sdp\")\n",
    "\n",
    "def print_current_backends():\n",
    "    \"\"\"Print current state of SDPA backends.\"\"\"\n",
    "    flash_enabled = torch.backends.cuda.flash_sdp_enabled()\n",
    "    mem_enabled = torch.backends.cuda.mem_efficient_sdp_enabled()\n",
    "    math_enabled = torch.backends.cuda.math_sdp_enabled()\n",
    "    print(f\"    Current backends - Flash: {flash_enabled}, MemEff: {mem_enabled}, Math: {math_enabled}\")\n",
    "\n",
    "# ------------------------------\n",
    "#  BENCHMARK DRIVER\n",
    "# ------------------------------\n",
    "def benchmark_sdpa_backends(test_files: List[Path]):\n",
    "    import esm\n",
    "    model, alphabet = esm.pretrained.esm_msa1b_t12_100M_UR50S()\n",
    "    model.eval().to(DEVICE)\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "    times_default, times_mem_efficient = [], []\n",
    "\n",
    "    for i, fp in enumerate(test_files, 1):\n",
    "        seqs = parse_a3m(fp)                 # full MSA\n",
    "        print(f\"[{i:02}/{len(test_files)}] {fp.name:45s}  {len(seqs):3d} seqs  len={len(seqs[0])}\")\n",
    "\n",
    "        # ---------- PyTorch Defaults ----------\n",
    "        set_pytorch_defaults()\n",
    "        print_current_backends()\n",
    "        _, t_default = get_model_output(model, batch_converter, seqs, DEVICE, use_autocast=True)\n",
    "        times_default.append(t_default)\n",
    "\n",
    "        # ---------- Explicit Mem Efficient ----------\n",
    "        set_mem_efficient_explicit()\n",
    "        print_current_backends()\n",
    "        _, t_mem_eff = get_model_output(model, batch_converter, seqs, DEVICE, use_autocast=True)\n",
    "        times_mem_efficient.append(t_mem_eff)\n",
    "\n",
    "        print(f\"    PyTorch Default    : {t_default:7.3f}s\")\n",
    "        print(f\"    Mem Efficient Expl : {t_mem_eff:7.3f}s   ratio {t_default/t_mem_eff:4.2f}×\")\n",
    "        print()\n",
    "\n",
    "    # ---------- SUMMARY ----------\n",
    "    avg_default = statistics.mean(times_default)\n",
    "    avg_mem_eff = statistics.mean(times_mem_efficient)\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(f\"Average PyTorch Default    : {avg_default:7.3f}s\")\n",
    "    print(f\"Average Mem Efficient Expl : {avg_mem_eff:7.3f}s\")\n",
    "    print(f\"Overall ratio              : {avg_default/avg_mem_eff:4.2f}×  \"\n",
    "          f\"(median {statistics.median(times_default)/statistics.median(times_mem_efficient):4.2f}×)\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    # Print final backend state\n",
    "    print(\"\\nFinal backend state:\")\n",
    "    print_current_backends()\n",
    "\n",
    "# ------------------------------\n",
    "#  MAIN EXECUTION\n",
    "# ------------------------------\n",
    "random.seed(SEED)\n",
    "all_a3m = find_a3m_files(DATA_ROOT, A3M_NAME)\n",
    "if len(all_a3m) < N_TEST_FILES:\n",
    "    raise RuntimeError(f\"Need ≥{N_TEST_FILES} files, found {len(all_a3m)}\")\n",
    "\n",
    "test_files = random.sample(all_a3m, N_TEST_FILES)\n",
    "print(f\"Selected {N_TEST_FILES} random A3M files (seed={SEED})\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# Show initial backend state\n",
    "print(\"\\nInitial backend state:\")\n",
    "print_current_backends()\n",
    "print()\n",
    "\n",
    "benchmark_sdpa_backends(test_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
