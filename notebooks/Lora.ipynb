{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38a22524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Environment\n",
      "===========\n",
      "\n",
      "{\n",
      "  \"python\": \"3.10.18 (main, Jun  5 2025, 13:14:17) [GCC 11.2.0]\",\n",
      "  \"platform\": \"Linux-6.8.0-1033-gcp-x86_64-with-glibc2.31\",\n",
      "  \"torch\": \"2.7.1+cu126\",\n",
      "  \"esm\": \"unknown\",\n",
      "  \"cuda_available\": false,\n",
      "  \"cuda_device_count\": 0\n",
      "}\n",
      "\n",
      "Model Class\n",
      "===========\n",
      "\n",
      "<class 'esm.model.msa_transformer.MSATransformer'>\n",
      "\n",
      "Named Modules (tree; truncated)\n",
      "===============================\n",
      "\n",
      "<root> :: MSATransformer\n",
      "embed_tokens :: Embedding\n",
      "dropout_module :: Dropout\n",
      "layers :: ModuleList\n",
      "layers.0 :: AxialTransformerLayer\n",
      "layers.0.row_self_attention :: NormalizedResidualBlock\n",
      "layers.0.row_self_attention.layer :: RowSelfAttention\n",
      "layers.0.row_self_attention.layer.k_proj :: Linear\n",
      "layers.0.row_self_attention.layer.v_proj :: Linear\n",
      "layers.0.row_self_attention.layer.q_proj :: Linear\n",
      "layers.0.row_self_attention.layer.out_proj :: Linear\n",
      "layers.0.row_self_attention.layer.dropout_module :: Dropout\n",
      "layers.0.row_self_attention.dropout_module :: Dropout\n",
      "layers.0.row_self_attention.layer_norm :: LayerNorm\n",
      "layers.0.column_self_attention :: NormalizedResidualBlock\n",
      "layers.0.column_self_attention.layer :: ColumnSelfAttention\n",
      "layers.0.column_self_attention.layer.k_proj :: Linear\n",
      "layers.0.column_self_attention.layer.v_proj :: Linear\n",
      "layers.0.column_self_attention.layer.q_proj :: Linear\n",
      "layers.0.column_self_attention.layer.out_proj :: Linear\n",
      "layers.0.column_self_attention.layer.dropout_module :: Dropout\n",
      "layers.0.column_self_attention.dropout_module :: Dropout\n",
      "layers.0.column_self_attention.layer_norm :: LayerNorm\n",
      "layers.0.feed_forward_layer :: NormalizedResidualBlock\n",
      "layers.0.feed_forward_layer.layer :: FeedForwardNetwork\n",
      "layers.0.feed_forward_layer.layer.activation_fn :: GELU\n",
      "layers.0.feed_forward_layer.layer.activation_dropout_module :: Dropout\n",
      "layers.0.feed_forward_layer.layer.fc1 :: Linear\n",
      "layers.0.feed_forward_layer.layer.fc2 :: Linear\n",
      "layers.0.feed_forward_layer.dropout_module :: Dropout\n",
      "layers.0.feed_forward_layer.layer_norm :: LayerNorm\n",
      "layers.1 :: AxialTransformerLayer\n",
      "layers.1.row_self_attention :: NormalizedResidualBlock\n",
      "layers.1.row_self_attention.layer :: RowSelfAttention\n",
      "layers.1.row_self_attention.layer.k_proj :: Linear\n",
      "layers.1.row_self_attention.layer.v_proj :: Linear\n",
      "layers.1.row_self_attention.layer.q_proj :: Linear\n",
      "layers.1.row_self_attention.layer.out_proj :: Linear\n",
      "layers.1.row_self_attention.layer.dropout_module :: Dropout\n",
      "layers.1.row_self_attention.dropout_module :: Dropout\n",
      "layers.1.row_self_attention.layer_norm :: LayerNorm\n",
      "layers.1.column_self_attention :: NormalizedResidualBlock\n",
      "layers.1.column_self_attention.layer :: ColumnSelfAttention\n",
      "layers.1.column_self_attention.layer.k_proj :: Linear\n",
      "layers.1.column_self_attention.layer.v_proj :: Linear\n",
      "layers.1.column_self_attention.layer.q_proj :: Linear\n",
      "layers.1.column_self_attention.layer.out_proj :: Linear\n",
      "layers.1.column_self_attention.layer.dropout_module :: Dropout\n",
      "layers.1.column_self_attention.dropout_module :: Dropout\n",
      "layers.1.column_self_attention.layer_norm :: LayerNorm\n",
      "layers.1.feed_forward_layer :: NormalizedResidualBlock\n",
      "layers.1.feed_forward_layer.layer :: FeedForwardNetwork\n",
      "layers.1.feed_forward_layer.layer.activation_fn :: GELU\n",
      "layers.1.feed_forward_layer.layer.activation_dropout_module :: Dropout\n",
      "layers.1.feed_forward_layer.layer.fc1 :: Linear\n",
      "layers.1.feed_forward_layer.layer.fc2 :: Linear\n",
      "layers.1.feed_forward_layer.dropout_module :: Dropout\n",
      "layers.1.feed_forward_layer.layer_norm :: LayerNorm\n",
      "layers.2 :: AxialTransformerLayer\n",
      "layers.2.row_self_attention :: NormalizedResidualBlock\n",
      "layers.2.row_self_attention.layer :: RowSelfAttention\n",
      "layers.2.row_self_attention.layer.k_proj :: Linear\n",
      "layers.2.row_self_attention.layer.v_proj :: Linear\n",
      "layers.2.row_self_attention.layer.q_proj :: Linear\n",
      "layers.2.row_self_attention.layer.out_proj :: Linear\n",
      "layers.2.row_self_attention.layer.dropout_module :: Dropout\n",
      "layers.2.row_self_attention.dropout_module :: Dropout\n",
      "layers.2.row_self_attention.layer_norm :: LayerNorm\n",
      "layers.2.column_self_attention :: NormalizedResidualBlock\n",
      "\n",
      "\n",
      "\n",
      "=== Saved full report to: /teamspace/studios/this_studio/PFP_Testing/notebooks/msa_introspection.txt ===\n"
     ]
    }
   ],
   "source": [
    "# === ESM-MSA introspection for LoRA targeting ===\n",
    "import sys, os, platform, re, textwrap, json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# If you have esm installed as `esm` (facebookresearch/esm)\n",
    "import esm\n",
    "\n",
    "OUT_PATH = Path(\"msa_introspection.txt\")\n",
    "\n",
    "def header(title):\n",
    "    line = \"=\" * len(title)\n",
    "    return f\"\\n{title}\\n{line}\\n\"\n",
    "\n",
    "def env_report():\n",
    "    try:\n",
    "        import importlib.metadata as im\n",
    "        esm_ver = im.version(\"esm\")\n",
    "    except Exception:\n",
    "        esm_ver = \"unknown\"\n",
    "    return {\n",
    "        \"python\": sys.version.replace(\"\\n\", \" \"),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"torch\": torch.__version__,\n",
    "        \"esm\": esm_ver,\n",
    "        \"cuda_available\": torch.cuda.is_available(),\n",
    "        \"cuda_device_count\": torch.cuda.device_count(),\n",
    "    }\n",
    "\n",
    "def print_tree(model, max_lines=400):\n",
    "    \"\"\"Render a readable tree of named_modules (truncated).\"\"\"\n",
    "    lines = []\n",
    "    for name, module in model.named_modules():\n",
    "        mod = type(module).__name__\n",
    "        lines.append(f\"{name or '<root>'} :: {mod}\")\n",
    "    truncated = lines[:max_lines]\n",
    "    body = \"\\n\".join(truncated)\n",
    "    if len(lines) > max_lines:\n",
    "        body += f\"\\n... ({len(lines)-max_lines} more lines truncated)\"\n",
    "    return body\n",
    "\n",
    "def find_proj_modules(model):\n",
    "    \"\"\"\n",
    "    Find attention projection linears named q_proj/k_proj/v_proj/out_proj\n",
    "    and group them by their parent block (e.g., row_attn / col_attn).\n",
    "    \"\"\"\n",
    "    hits = []\n",
    "    for name, module in model.named_modules():\n",
    "        for proj in (\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"):\n",
    "            if hasattr(module, proj) and isinstance(getattr(module, proj), nn.Linear):\n",
    "                lin = getattr(module, proj)\n",
    "                hits.append({\n",
    "                    \"parent\": name,\n",
    "                    \"proj_name\": proj,\n",
    "                    \"shape\": tuple(lin.weight.shape),\n",
    "                    \"bias\": getattr(lin, \"bias\", None) is not None,\n",
    "                    \"type\": type(lin).__name__,\n",
    "                })\n",
    "    return hits\n",
    "\n",
    "def group_row_col(hits):\n",
    "    \"\"\"\n",
    "    Try to separate row/col attention based on parent name substrings.\n",
    "    This is robust to common ESM naming like 'layers.0.row_attn' / 'layers.0.col_attn'.\n",
    "    \"\"\"\n",
    "    row, col, other = [], [], []\n",
    "    for h in hits:\n",
    "        parent = h[\"parent\"].lower()\n",
    "        if \"row\" in parent:\n",
    "            row.append(h)\n",
    "        elif \"col\" in parent or \"column\" in parent:\n",
    "            col.append(h)\n",
    "        else:\n",
    "            other.append(h)\n",
    "    return row, col, other\n",
    "\n",
    "def list_all_linears(model):\n",
    "    items = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            items.append({\n",
    "                \"name\": name,\n",
    "                \"shape\": tuple(module.weight.shape),\n",
    "                \"bias\": module.bias is not None\n",
    "            })\n",
    "    return items\n",
    "\n",
    "def dump_named_parameters(model, pattern=None, max_lines=200):\n",
    "    lines = []\n",
    "    for name, p in model.named_parameters():\n",
    "        if (pattern is None) or re.search(pattern, name):\n",
    "            lines.append(f\"{name} :: {tuple(p.shape)} :: requires_grad={p.requires_grad}\")\n",
    "    lines = lines[:max_lines] + ([\"... (truncated)\"] if len(lines) > max_lines else [])\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ---------- Load model ----------\n",
    "msa_model, msa_alphabet = esm.pretrained.esm_msa1b_t12_100M_UR50S()\n",
    "msa_model.eval().requires_grad_(False)\n",
    "\n",
    "# ---------- Collect info ----------\n",
    "env = env_report()\n",
    "tree = print_tree(msa_model, max_lines=800)  # bump if you want more\n",
    "hits = find_proj_modules(msa_model)\n",
    "row_hits, col_hits, other_hits = group_row_col(hits)\n",
    "linears = list_all_linears(msa_model)\n",
    "params_glimpse = dump_named_parameters(msa_model, pattern=r\"(q_proj|k_proj|v_proj|out_proj)\", max_lines=300)\n",
    "\n",
    "# ---------- Build a human-readable report ----------\n",
    "report = []\n",
    "report.append(header(\"Environment\"))\n",
    "report.append(json.dumps(env, indent=2))\n",
    "\n",
    "report.append(header(\"Model Class\"))\n",
    "report.append(repr(type(msa_model)))\n",
    "\n",
    "report.append(header(\"Named Modules (tree; truncated)\"))\n",
    "report.append(tree)\n",
    "\n",
    "report.append(header(\"Attention Projection Linears (q/k/v/out) â€” ALL HITS\"))\n",
    "for h in hits:\n",
    "    report.append(f\"{h['parent']}.{h['proj_name']} :: {h['shape']} :: bias={h['bias']}\")\n",
    "\n",
    "report.append(header(\"Row Attention Projections\"))\n",
    "for h in row_hits:\n",
    "    report.append(f\"{h['parent']}.{h['proj_name']} :: {h['shape']} :: bias={h['bias']}\")\n",
    "\n",
    "report.append(header(\"Column Attention Projections\"))\n",
    "for h in col_hits:\n",
    "    report.append(f\"{h['parent']}.{h['proj_name']} :: {h['shape']} :: bias={h['bias']}\")\n",
    "\n",
    "if other_hits:\n",
    "    report.append(header(\"Other Attention-like Projections (neither row nor col by name)\"))\n",
    "    for h in other_hits:\n",
    "        report.append(f\"{h['parent']}.{h['proj_name']} :: {h['shape']} :: bias={h['bias']}\")\n",
    "\n",
    "report.append(header(\"All nn.Linear layers (for optional LoRA targets; truncated to 500)\"))\n",
    "for item in linears[:500]:\n",
    "    report.append(f\"{item['name']} :: {item['shape']} :: bias={item['bias']}\")\n",
    "if len(linears) > 500:\n",
    "    report.append(f\"... ({len(linears)-500} more linears truncated)\")\n",
    "\n",
    "report.append(header(\"Named parameters glimpse (q/k/v/out only; truncated)\"))\n",
    "report.append(params_glimpse)\n",
    "\n",
    "text = \"\\n\".join(report)\n",
    "\n",
    "# ---------- Save & show ----------\n",
    "OUT_PATH.write_text(text)\n",
    "print(text[:4000])\n",
    "print(f\"\\n\\n=== Saved full report to: {OUT_PATH.resolve()} ===\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
