{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38a22524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Environment\n",
      "===========\n",
      "\n",
      "{\n",
      "  \"python\": \"3.10.18 (main, Jun  5 2025, 13:14:17) [GCC 11.2.0]\",\n",
      "  \"platform\": \"Linux-6.8.0-1033-gcp-x86_64-with-glibc2.31\",\n",
      "  \"torch\": \"2.7.1+cu126\",\n",
      "  \"esm\": \"unknown\",\n",
      "  \"cuda_available\": false,\n",
      "  \"cuda_device_count\": 0\n",
      "}\n",
      "\n",
      "Model Class\n",
      "===========\n",
      "\n",
      "<class 'esm.model.msa_transformer.MSATransformer'>\n",
      "\n",
      "Named Modules (tree; truncated)\n",
      "===============================\n",
      "\n",
      "<root> :: MSATransformer\n",
      "embed_tokens :: Embedding\n",
      "dropout_module :: Dropout\n",
      "layers :: ModuleList\n",
      "layers.0 :: AxialTransformerLayer\n",
      "layers.0.row_self_attention :: NormalizedResidualBlock\n",
      "layers.0.row_self_attention.layer :: RowSelfAttention\n",
      "layers.0.row_self_attention.layer.k_proj :: Linear\n",
      "layers.0.row_self_attention.layer.v_proj :: Linear\n",
      "layers.0.row_self_attention.layer.q_proj :: Linear\n",
      "layers.0.row_self_attention.layer.out_proj :: Linear\n",
      "layers.0.row_self_attention.layer.dropout_module :: Dropout\n",
      "layers.0.row_self_attention.dropout_module :: Dropout\n",
      "layers.0.row_self_attention.layer_norm :: LayerNorm\n",
      "layers.0.column_self_attention :: NormalizedResidualBlock\n",
      "layers.0.column_self_attention.layer :: ColumnSelfAttention\n",
      "layers.0.column_self_attention.layer.k_proj :: Linear\n",
      "layers.0.column_self_attention.layer.v_proj :: Linear\n",
      "layers.0.column_self_attention.layer.q_proj :: Linear\n",
      "layers.0.column_self_attention.layer.out_proj :: Linear\n",
      "layers.0.column_self_attention.layer.dropout_module :: Dropout\n",
      "layers.0.column_self_attention.dropout_module :: Dropout\n",
      "layers.0.column_self_attention.layer_norm :: LayerNorm\n",
      "layers.0.feed_forward_layer :: NormalizedResidualBlock\n",
      "layers.0.feed_forward_layer.layer :: FeedForwardNetwork\n",
      "layers.0.feed_forward_layer.layer.activation_fn :: GELU\n",
      "layers.0.feed_forward_layer.layer.activation_dropout_module :: Dropout\n",
      "layers.0.feed_forward_layer.layer.fc1 :: Linear\n",
      "layers.0.feed_forward_layer.layer.fc2 :: Linear\n",
      "layers.0.feed_forward_layer.dropout_module :: Dropout\n",
      "layers.0.feed_forward_layer.layer_norm :: LayerNorm\n",
      "layers.1 :: AxialTransformerLayer\n",
      "layers.1.row_self_attention :: NormalizedResidualBlock\n",
      "layers.1.row_self_attention.layer :: RowSelfAttention\n",
      "layers.1.row_self_attention.layer.k_proj :: Linear\n",
      "layers.1.row_self_attention.layer.v_proj :: Linear\n",
      "layers.1.row_self_attention.layer.q_proj :: Linear\n",
      "layers.1.row_self_attention.layer.out_proj :: Linear\n",
      "layers.1.row_self_attention.layer.dropout_module :: Dropout\n",
      "layers.1.row_self_attention.dropout_module :: Dropout\n",
      "layers.1.row_self_attention.layer_norm :: LayerNorm\n",
      "layers.1.column_self_attention :: NormalizedResidualBlock\n",
      "layers.1.column_self_attention.layer :: ColumnSelfAttention\n",
      "layers.1.column_self_attention.layer.k_proj :: Linear\n",
      "layers.1.column_self_attention.layer.v_proj :: Linear\n",
      "layers.1.column_self_attention.layer.q_proj :: Linear\n",
      "layers.1.column_self_attention.layer.out_proj :: Linear\n",
      "layers.1.column_self_attention.layer.dropout_module :: Dropout\n",
      "layers.1.column_self_attention.dropout_module :: Dropout\n",
      "layers.1.column_self_attention.layer_norm :: LayerNorm\n",
      "layers.1.feed_forward_layer :: NormalizedResidualBlock\n",
      "layers.1.feed_forward_layer.layer :: FeedForwardNetwork\n",
      "layers.1.feed_forward_layer.layer.activation_fn :: GELU\n",
      "layers.1.feed_forward_layer.layer.activation_dropout_module :: Dropout\n",
      "layers.1.feed_forward_layer.layer.fc1 :: Linear\n",
      "layers.1.feed_forward_layer.layer.fc2 :: Linear\n",
      "layers.1.feed_forward_layer.dropout_module :: Dropout\n",
      "layers.1.feed_forward_layer.layer_norm :: LayerNorm\n",
      "layers.2 :: AxialTransformerLayer\n",
      "layers.2.row_self_attention :: NormalizedResidualBlock\n",
      "layers.2.row_self_attention.layer :: RowSelfAttention\n",
      "layers.2.row_self_attention.layer.k_proj :: Linear\n",
      "layers.2.row_self_attention.layer.v_proj :: Linear\n",
      "layers.2.row_self_attention.layer.q_proj :: Linear\n",
      "layers.2.row_self_attention.layer.out_proj :: Linear\n",
      "layers.2.row_self_attention.layer.dropout_module :: Dropout\n",
      "layers.2.row_self_attention.dropout_module :: Dropout\n",
      "layers.2.row_self_attention.layer_norm :: LayerNorm\n",
      "layers.2.column_self_attention :: NormalizedResidualBlock\n",
      "\n",
      "\n",
      "\n",
      "=== Saved full report to: /teamspace/studios/this_studio/PFP_Testing/notebooks/msa_introspection.txt ===\n"
     ]
    }
   ],
   "source": [
    "# === ESM-MSA introspection for LoRA targeting ===\n",
    "import sys, os, platform, re, textwrap, json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# If you have esm installed as `esm` (facebookresearch/esm)\n",
    "import esm\n",
    "\n",
    "OUT_PATH = Path(\"msa_introspection.txt\")\n",
    "\n",
    "def header(title):\n",
    "    line = \"=\" * len(title)\n",
    "    return f\"\\n{title}\\n{line}\\n\"\n",
    "\n",
    "def env_report():\n",
    "    try:\n",
    "        import importlib.metadata as im\n",
    "        esm_ver = im.version(\"esm\")\n",
    "    except Exception:\n",
    "        esm_ver = \"unknown\"\n",
    "    return {\n",
    "        \"python\": sys.version.replace(\"\\n\", \" \"),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"torch\": torch.__version__,\n",
    "        \"esm\": esm_ver,\n",
    "        \"cuda_available\": torch.cuda.is_available(),\n",
    "        \"cuda_device_count\": torch.cuda.device_count(),\n",
    "    }\n",
    "\n",
    "def print_tree(model, max_lines=400):\n",
    "    \"\"\"Render a readable tree of named_modules (truncated).\"\"\"\n",
    "    lines = []\n",
    "    for name, module in model.named_modules():\n",
    "        mod = type(module).__name__\n",
    "        lines.append(f\"{name or '<root>'} :: {mod}\")\n",
    "    truncated = lines[:max_lines]\n",
    "    body = \"\\n\".join(truncated)\n",
    "    if len(lines) > max_lines:\n",
    "        body += f\"\\n... ({len(lines)-max_lines} more lines truncated)\"\n",
    "    return body\n",
    "\n",
    "def find_proj_modules(model):\n",
    "    \"\"\"\n",
    "    Find attention projection linears named q_proj/k_proj/v_proj/out_proj\n",
    "    and group them by their parent block (e.g., row_attn / col_attn).\n",
    "    \"\"\"\n",
    "    hits = []\n",
    "    for name, module in model.named_modules():\n",
    "        for proj in (\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"):\n",
    "            if hasattr(module, proj) and isinstance(getattr(module, proj), nn.Linear):\n",
    "                lin = getattr(module, proj)\n",
    "                hits.append({\n",
    "                    \"parent\": name,\n",
    "                    \"proj_name\": proj,\n",
    "                    \"shape\": tuple(lin.weight.shape),\n",
    "                    \"bias\": getattr(lin, \"bias\", None) is not None,\n",
    "                    \"type\": type(lin).__name__,\n",
    "                })\n",
    "    return hits\n",
    "\n",
    "def group_row_col(hits):\n",
    "    \"\"\"\n",
    "    Try to separate row/col attention based on parent name substrings.\n",
    "    This is robust to common ESM naming like 'layers.0.row_attn' / 'layers.0.col_attn'.\n",
    "    \"\"\"\n",
    "    row, col, other = [], [], []\n",
    "    for h in hits:\n",
    "        parent = h[\"parent\"].lower()\n",
    "        if \"row\" in parent:\n",
    "            row.append(h)\n",
    "        elif \"col\" in parent or \"column\" in parent:\n",
    "            col.append(h)\n",
    "        else:\n",
    "            other.append(h)\n",
    "    return row, col, other\n",
    "\n",
    "def list_all_linears(model):\n",
    "    items = []\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            items.append({\n",
    "                \"name\": name,\n",
    "                \"shape\": tuple(module.weight.shape),\n",
    "                \"bias\": module.bias is not None\n",
    "            })\n",
    "    return items\n",
    "\n",
    "def dump_named_parameters(model, pattern=None, max_lines=200):\n",
    "    lines = []\n",
    "    for name, p in model.named_parameters():\n",
    "        if (pattern is None) or re.search(pattern, name):\n",
    "            lines.append(f\"{name} :: {tuple(p.shape)} :: requires_grad={p.requires_grad}\")\n",
    "    lines = lines[:max_lines] + ([\"... (truncated)\"] if len(lines) > max_lines else [])\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# ---------- Load model ----------\n",
    "msa_model, msa_alphabet = esm.pretrained.esm_msa1b_t12_100M_UR50S()\n",
    "msa_model.eval().requires_grad_(False)\n",
    "\n",
    "# ---------- Collect info ----------\n",
    "env = env_report()\n",
    "tree = print_tree(msa_model, max_lines=800)  # bump if you want more\n",
    "hits = find_proj_modules(msa_model)\n",
    "row_hits, col_hits, other_hits = group_row_col(hits)\n",
    "linears = list_all_linears(msa_model)\n",
    "params_glimpse = dump_named_parameters(msa_model, pattern=r\"(q_proj|k_proj|v_proj|out_proj)\", max_lines=300)\n",
    "\n",
    "# ---------- Build a human-readable report ----------\n",
    "report = []\n",
    "report.append(header(\"Environment\"))\n",
    "report.append(json.dumps(env, indent=2))\n",
    "\n",
    "report.append(header(\"Model Class\"))\n",
    "report.append(repr(type(msa_model)))\n",
    "\n",
    "report.append(header(\"Named Modules (tree; truncated)\"))\n",
    "report.append(tree)\n",
    "\n",
    "report.append(header(\"Attention Projection Linears (q/k/v/out) — ALL HITS\"))\n",
    "for h in hits:\n",
    "    report.append(f\"{h['parent']}.{h['proj_name']} :: {h['shape']} :: bias={h['bias']}\")\n",
    "\n",
    "report.append(header(\"Row Attention Projections\"))\n",
    "for h in row_hits:\n",
    "    report.append(f\"{h['parent']}.{h['proj_name']} :: {h['shape']} :: bias={h['bias']}\")\n",
    "\n",
    "report.append(header(\"Column Attention Projections\"))\n",
    "for h in col_hits:\n",
    "    report.append(f\"{h['parent']}.{h['proj_name']} :: {h['shape']} :: bias={h['bias']}\")\n",
    "\n",
    "if other_hits:\n",
    "    report.append(header(\"Other Attention-like Projections (neither row nor col by name)\"))\n",
    "    for h in other_hits:\n",
    "        report.append(f\"{h['parent']}.{h['proj_name']} :: {h['shape']} :: bias={h['bias']}\")\n",
    "\n",
    "report.append(header(\"All nn.Linear layers (for optional LoRA targets; truncated to 500)\"))\n",
    "for item in linears[:500]:\n",
    "    report.append(f\"{item['name']} :: {item['shape']} :: bias={item['bias']}\")\n",
    "if len(linears) > 500:\n",
    "    report.append(f\"... ({len(linears)-500} more linears truncated)\")\n",
    "\n",
    "report.append(header(\"Named parameters glimpse (q/k/v/out only; truncated)\"))\n",
    "report.append(params_glimpse)\n",
    "\n",
    "text = \"\\n\".join(report)\n",
    "\n",
    "# ---------- Save & show ----------\n",
    "OUT_PATH.write_text(text)\n",
    "print(text[:4000])\n",
    "print(f\"\\n\\n=== Saved full report to: {OUT_PATH.resolve()} ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f46034c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Loading ESM-C ========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150a21aa376549469b5f0d862809d31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 575,036,992 | Trainable (now): 575,036,992\n",
      "Model class: ESMC\n",
      "Core class:  ESMC\n",
      "\n",
      "======== Transformer Blocks ========\n",
      "Num blocks found: 0\n",
      "\n",
      "======== Attention Linears (separate q/k/v/out) ========\n",
      "transformer.blocks.0.attn.out_proj                            Linear(1152 -> 1152)\n",
      "transformer.blocks.1.attn.out_proj                            Linear(1152 -> 1152)\n",
      "transformer.blocks.2.attn.out_proj                            Linear(1152 -> 1152)\n",
      "transformer.blocks.3.attn.out_proj                            Linear(1152 -> 1152)\n",
      "transformer.blocks.4.attn.out_proj                            Linear(1152 -> 1152)\n",
      "transformer.blocks.5.attn.out_proj                            Linear(1152 -> 1152)\n",
      "transformer.blocks.6.attn.out_proj                            Linear(1152 -> 1152)\n",
      "transformer.blocks.7.attn.out_proj                            Linear(1152 -> 1152)\n",
      "transformer.blocks.8.attn.out_proj                            Linear(1152 -> 1152)\n",
      "transformer.blocks.9.attn.out_proj                            Linear(1152 -> 1152)\n",
      "transformer.blocks.10.attn.out_proj                           Linear(1152 -> 1152)\n",
      "transformer.blocks.11.attn.out_proj                           Linear(1152 -> 1152)\n",
      "transformer.blocks.12.attn.out_proj                           Linear(1152 -> 1152)\n",
      "transformer.blocks.13.attn.out_proj                           Linear(1152 -> 1152)\n",
      "transformer.blocks.14.attn.out_proj                           Linear(1152 -> 1152)\n",
      "transformer.blocks.15.attn.out_proj                           Linear(1152 -> 1152)\n",
      "transformer.blocks.16.attn.out_proj                           Linear(1152 -> 1152)\n",
      "transformer.blocks.17.attn.out_proj                           Linear(1152 -> 1152)\n",
      "transformer.blocks.18.attn.out_proj                           Linear(1152 -> 1152)\n",
      "transformer.blocks.19.attn.out_proj                           Linear(1152 -> 1152)\n",
      "transformer.blocks.20.attn.out_proj                           Linear(1152 -> 1152)\n",
      "transformer.blocks.21.attn.out_proj                           Linear(1152 -> 1152)\n",
      "transformer.blocks.22.attn.out_proj                           Linear(1152 -> 1152)\n",
      "transformer.blocks.23.attn.out_proj                           Linear(1152 -> 1152)\n",
      "transformer.blocks.24.attn.out_proj                           Linear(1152 -> 1152)\n",
      "transformer.blocks.25.attn.out_proj                           Linear(1152 -> 1152)\n",
      "transformer.blocks.26.attn.out_proj                           Linear(1152 -> 1152)\n",
      "transformer.blocks.27.attn.out_proj                           Linear(1152 -> 1152)\n",
      "transformer.blocks.28.attn.out_proj                           Linear(1152 -> 1152)\n",
      "transformer.blocks.29.attn.out_proj                           Linear(1152 -> 1152)\n",
      "transformer.blocks.30.attn.out_proj                           Linear(1152 -> 1152)\n",
      "transformer.blocks.31.attn.out_proj                           Linear(1152 -> 1152)\n",
      "transformer.blocks.32.attn.out_proj                           Linear(1152 -> 1152)\n",
      "transformer.blocks.33.attn.out_proj                           Linear(1152 -> 1152)\n",
      "transformer.blocks.34.attn.out_proj                           Linear(1152 -> 1152)\n",
      "transformer.blocks.35.attn.out_proj                           Linear(1152 -> 1152)\n",
      "\n",
      "======== Potential Fused In-Proj Attentions ========\n",
      "No modules with in_proj_weight/in_proj_bias detected.\n",
      "\n",
      "======== LayerNorm Modules ========\n",
      "Found 146 LayerNorms.\n",
      "transformer.blocks.0.attn.layernorm_qkv.0\n",
      "transformer.blocks.0.attn.q_ln\n",
      "transformer.blocks.0.attn.k_ln\n",
      "transformer.blocks.0.ffn.0\n",
      "transformer.blocks.1.attn.layernorm_qkv.0\n",
      "transformer.blocks.1.attn.q_ln\n",
      "transformer.blocks.1.attn.k_ln\n",
      "transformer.blocks.1.ffn.0\n",
      "transformer.blocks.2.attn.layernorm_qkv.0\n",
      "transformer.blocks.2.attn.q_ln\n",
      "transformer.blocks.2.attn.k_ln\n",
      "transformer.blocks.2.ffn.0\n",
      "transformer.blocks.3.attn.layernorm_qkv.0\n",
      "transformer.blocks.3.attn.q_ln\n",
      "transformer.blocks.3.attn.k_ln\n",
      "transformer.blocks.3.ffn.0\n",
      "transformer.blocks.4.attn.layernorm_qkv.0\n",
      "transformer.blocks.4.attn.q_ln\n",
      "transformer.blocks.4.attn.k_ln\n",
      "transformer.blocks.4.ffn.0\n",
      "transformer.blocks.5.attn.layernorm_qkv.0\n",
      "transformer.blocks.5.attn.q_ln\n",
      "transformer.blocks.5.attn.k_ln\n",
      "transformer.blocks.5.ffn.0\n",
      "transformer.blocks.6.attn.layernorm_qkv.0\n",
      "transformer.blocks.6.attn.q_ln\n",
      "transformer.blocks.6.attn.k_ln\n",
      "transformer.blocks.6.ffn.0\n",
      "transformer.blocks.7.attn.layernorm_qkv.0\n",
      "transformer.blocks.7.attn.q_ln\n",
      "transformer.blocks.7.attn.k_ln\n",
      "transformer.blocks.7.ffn.0\n",
      "transformer.blocks.8.attn.layernorm_qkv.0\n",
      "transformer.blocks.8.attn.q_ln\n",
      "transformer.blocks.8.attn.k_ln\n",
      "transformer.blocks.8.ffn.0\n",
      "transformer.blocks.9.attn.layernorm_qkv.0\n",
      "transformer.blocks.9.attn.q_ln\n",
      "transformer.blocks.9.attn.k_ln\n",
      "transformer.blocks.9.ffn.0\n",
      "... and 106 more\n",
      "\n",
      "======== Per-Block Attention Fields (quick check) ========\n",
      "\n",
      "======== Instructions ========\n",
      "Paste this entire output back to me.\n",
      "- If you see names like '*.self_attn.k_proj' and '*.self_attn.v_proj', we can LoRA those directly.\n",
      "- If instead you see 'in_proj_weight', we'll use a fused-projection LoRA patch.\n",
      "- The block count tells us how many 'last_n' layers we can reasonably target.\n"
     ]
    }
   ],
   "source": [
    "# --- ESM-C Introspection for LoRA Targeting ---\n",
    "import torch, re, sys\n",
    "from typing import List, Tuple\n",
    "from esm.models.esmc import ESMC\n",
    "from torch import nn\n",
    "\n",
    "def is_linear(m): \n",
    "    return isinstance(m, nn.Linear)\n",
    "\n",
    "def find_transformer_blocks(core: nn.Module) -> List[nn.Module]:\n",
    "    \"\"\"\n",
    "    Try to locate the list of transformer blocks.\n",
    "    Works if the model exposes a ModuleList of blocks or\n",
    "    otherwise collects any module that has a 'self_attn' attr.\n",
    "    \"\"\"\n",
    "    # Heuristic 1: a ModuleList whose first item has 'self_attn'\n",
    "    for name, mod in core.named_modules():\n",
    "        if isinstance(mod, nn.ModuleList) and len(mod) > 0:\n",
    "            first = mod[0]\n",
    "            if hasattr(first, \"self_attn\"):\n",
    "                return list(mod)\n",
    "    # Heuristic 2: collect any module that has 'self_attn'\n",
    "    blocks = []\n",
    "    for _, mod in core.named_modules():\n",
    "        if hasattr(mod, \"self_attn\"):\n",
    "            blocks.append(mod)\n",
    "    return blocks\n",
    "\n",
    "def preview_attn_linears(core: nn.Module) -> List[Tuple[str, nn.Linear]]:\n",
    "    \"\"\"\n",
    "    Return (name, module) for attention-related linears commonly used in ESM/ESMC.\n",
    "    \"\"\"\n",
    "    hits = []\n",
    "    for name, mod in core.named_modules():\n",
    "        if is_linear(mod):\n",
    "            if any(key in name for key in (\n",
    "                \"self_attn.q_proj\",\"self_attn.k_proj\",\"self_attn.v_proj\",\"self_attn.out_proj\",\n",
    "                \"attn.q_proj\",\"attn.k_proj\",\"attn.v_proj\",\"attn.out_proj\"\n",
    "            )):\n",
    "                hits.append((name, mod))\n",
    "    return hits\n",
    "\n",
    "def detect_fused_inproj(core: nn.Module) -> List[str]:\n",
    "    \"\"\"\n",
    "    Find modules that look like Fairseq-style fused projection attention\n",
    "    (i.e., have 'in_proj_weight' or 'in_proj_bias' attributes).\n",
    "    \"\"\"\n",
    "    fused = []\n",
    "    for name, mod in core.named_modules():\n",
    "        if hasattr(mod, \"in_proj_weight\") or hasattr(mod, \"in_proj_bias\"):\n",
    "            fused.append(name)\n",
    "    return fused\n",
    "\n",
    "def list_layernorms(core: nn.Module) -> List[str]:\n",
    "    return [name for name, m in core.named_modules() if isinstance(m, nn.LayerNorm)]\n",
    "\n",
    "def fmt(s, char=\"=\"):\n",
    "    return f\"\\n{char*8} {s} {char*8}\"\n",
    "\n",
    "def main():\n",
    "    print(fmt(\"Loading ESM-C\"))\n",
    "    model = ESMC.from_pretrained(\"esmc_600m\")\n",
    "    core = getattr(model, \"model\", model)  # some SDKs wrap the raw torch module here\n",
    "\n",
    "    # Basic model info\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total params: {total:,} | Trainable (now): {trainable:,}\")\n",
    "    print(f\"Model class: {model.__class__.__name__}\")\n",
    "    print(f\"Core class:  {core.__class__.__name__}\")\n",
    "\n",
    "    # Locate transformer blocks\n",
    "    blocks = find_transformer_blocks(core)\n",
    "    print(fmt(\"Transformer Blocks\"))\n",
    "    print(f\"Num blocks found: {len(blocks)}\")\n",
    "    if len(blocks) > 0:\n",
    "        # Print a quick sketch of the first, middle, and last block types\n",
    "        idxs = sorted(set([0, len(blocks)//2, len(blocks)-1]))\n",
    "        for i in idxs:\n",
    "            b = blocks[i]\n",
    "            print(f\"- Block[{i}] type: {b.__class__.__name__}  (attrs: {', '.join(sorted(set(dir(b)) & {'self_attn','fc1','fc2','norm1','norm2'}))})\")\n",
    "\n",
    "    # Find attention linears\n",
    "    print(fmt(\"Attention Linears (separate q/k/v/out)\"))\n",
    "    hits = preview_attn_linears(core)\n",
    "    if hits:\n",
    "        for name, mod in hits[:50]:  # print first 50 for sanity\n",
    "            print(f\"{name:60s}  Linear({mod.in_features} -> {mod.out_features})\")\n",
    "        if len(hits) > 50:\n",
    "            print(f\"... and {len(hits)-50} more\")\n",
    "    else:\n",
    "        print(\"No explicit q_proj/k_proj/v_proj/out_proj linears found.\")\n",
    "\n",
    "    # Fused attention?\n",
    "    print(fmt(\"Potential Fused In-Proj Attentions\"))\n",
    "    fused = detect_fused_inproj(core)\n",
    "    if fused:\n",
    "        for name in fused:\n",
    "            print(f\"{name}\")\n",
    "    else:\n",
    "        print(\"No modules with in_proj_weight/in_proj_bias detected.\")\n",
    "\n",
    "    # LayerNorms\n",
    "    print(fmt(\"LayerNorm Modules\"))\n",
    "    ln_names = list_layernorms(core)\n",
    "    print(f\"Found {len(ln_names)} LayerNorms.\")\n",
    "    for n in ln_names[:40]:\n",
    "        print(n)\n",
    "    if len(ln_names) > 40:\n",
    "        print(f\"... and {len(ln_names)-40} more\")\n",
    "\n",
    "    # Per-block quick look at attention fields\n",
    "    print(fmt(\"Per-Block Attention Fields (quick check)\"))\n",
    "    for i, b in enumerate(blocks[:12]):  # first 12 blocks only to keep output manageable\n",
    "        sa = getattr(b, \"self_attn\", None)\n",
    "        fields = []\n",
    "        if sa is not None:\n",
    "            for attr in (\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\",\"in_proj_weight\",\"in_proj_bias\"):\n",
    "                if hasattr(sa, attr):\n",
    "                    fields.append(attr)\n",
    "        print(f\"Block[{i}]: self_attn fields -> {fields if fields else 'N/A'}\")\n",
    "\n",
    "    print(fmt(\"Instructions\"))\n",
    "    print(\n",
    "        \"Paste this entire output back to me.\\n\"\n",
    "        \"- If you see names like '*.self_attn.k_proj' and '*.self_attn.v_proj', we can LoRA those directly.\\n\"\n",
    "        \"- If instead you see 'in_proj_weight', we'll use a fused-projection LoRA patch.\\n\"\n",
    "        \"- The block count tells us how many 'last_n' layers we can reasonably target.\"\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cloudspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
